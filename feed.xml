<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://arampacha.github.io/thoughtsamples/feed.xml" rel="self" type="application/atom+xml" /><link href="https://arampacha.github.io/thoughtsamples/" rel="alternate" type="text/html" /><updated>2021-05-07T08:31:22-05:00</updated><id>https://arampacha.github.io/thoughtsamples/feed.xml</id><title type="html">thoughtsamples</title><subtitle>trying to sample non-random stuff from my neural net</subtitle><entry><title type="html">Finetuning Transformers on GLUE benchmark</title><link href="https://arampacha.github.io/thoughtsamples/2021/05/07/glue-benchmark.html" rel="alternate" type="text/html" title="Finetuning Transformers on GLUE benchmark" /><published>2021-05-07T00:00:00-05:00</published><updated>2021-05-07T00:00:00-05:00</updated><id>https://arampacha.github.io/thoughtsamples/2021/05/07/glue-benchmark</id><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">A Transformer based Language Model from scratch</title><link href="https://arampacha.github.io/thoughtsamples/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html" rel="alternate" type="text/html" title="A Transformer based Language Model from scratch" /><published>2021-01-02T00:00:00-06:00</published><updated>2021-01-02T00:00:00-06:00</updated><id>https://arampacha.github.io/thoughtsamples/fastai/pytorch/2021/01/02/transformer-lm-from-scratch</id><author><name>Arto</name></author><category term="fastai" /><category term="pytorch" /><summary type="html"></summary></entry></feed>