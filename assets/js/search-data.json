{
  
    
        "post0": {
            "title": "Finetuning Transformers on GLUE benchmark",
            "content": "from transformers import AutoModelForSequenceClassification from fastai.text.all import * from fastai.callback.wandb import * from fasthugs.learner import TransLearner from fasthugs.data import TransformersTextBlock, TextGetter, get_splits from datasets import load_dataset, concatenate_datasets import wandb import gc . . Introduction . In this blogpost we will look at how to conbine the power of HuggingFace with great flexibility of fastai. For this purpose we will be finetuning distilroberta-base on The General Language Understanding Evaluation(GLUE) benchmark tasks. To make integration of HF models with fastai training loop smooth I&#39;m using lightweight wrapper library fasthugs. It is currently under development and we&#39;ll be inroducing more features together with NLP related blogposts like this one. . Fun fact:GLUE benchmark was introduced in this paper in 2018 as tough to beat benchmark to chellange NLP systems and in just about a year new SuperGLUE benchmark was introduced because original GLUE has become too easy for the models. To give you a grasp on what are we dealing with, here is a brief summary of GLUE tasks: . Name Task description Size Metrics . cola Corpus of Linguistic Acceptability | Determine whether it is a grammatical sentence | 8.5k | matthews_corrcoef | . sst2 Stanford Sentiment Treebank | Predict the sentiment of a givensentence | 67k | accuracy | . mrpc Microsoft Research Paraphrase Corpus | Determine whether the sentences in the pair are semantically equivalent | 3.7k | f1/accuracy | . stsb Semantic Textual Similarity Benchmark | Determine similarity score for 2 sentences | 7k | pearsonr/spearmanr | . qqp Quora question pair | Determine if 2 questions are the same (paraphrase) | 364k | f1/accuracy | . mnli Mulit-Genre Natural Language Inference | Predict whether the premise entails, contradicts or is neutral to the hypothesis | 393k | accuracy | . qnli Stanford Question Answering Dataset | Determine whether the context sentence containsthe answer to the question | 105k | accuracy | . rte Recognize Textual Entailment | Determine whether one sentece entails another | 2.5k | accuracy | . wnli Winograd Schema Challenge | Predict if the sentence with the pronoun substituted is entailed by the original sentence | 634 | accuracy | . As you can see some datasets are really small here. And we&#39;ll look at how one can adress. . Setup . Let&#39;s define main settings for the run in one place: . ds_name = &#39;glue&#39; model_name = &quot;distilroberta-base&quot; max_len = 512 bs = 32 val_bs = bs*2 n_epoch = 4 lr = 2e-5 wd = 0. opt_func = Adam diff_lr_decay_factor = 0 . To make switching between datasets smooth I&#39;ll define couple of dictionaries containing per-task information. We&#39;ll need metrics, text fields to retrieve data and number of outputs for the model. . GLUE_TASKS = [&quot;cola&quot;, &quot;mnli&quot;, &quot;mrpc&quot;, &quot;qnli&quot;, &quot;qqp&quot;, &quot;rte&quot;, &quot;sst2&quot;, &quot;stsb&quot;, &quot;wnli&quot;] def validate_task(): assert task in GLUE_TASKS . glue_metrics = { &#39;cola&#39;:[MatthewsCorrCoef()], &#39;sst2&#39;:[accuracy], &#39;mrpc&#39;:[F1Score(), accuracy], &#39;stsb&#39;:[PearsonCorrCoef(), SpearmanCorrCoef()], &#39;qqp&#39;: [F1Score(), accuracy], &#39;mnli&#39;:[accuracy], &#39;qnli&#39;:[accuracy], &#39;rte&#39;: [accuracy], &#39;wnli&#39;:[accuracy], } glue_textfields = { &#39;cola&#39;:[&#39;sentence&#39;, None], &#39;sst2&#39;:[&#39;sentence&#39;, None], &#39;mrpc&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;stsb&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;qqp&#39;: [&#39;question1&#39;, &#39;question2&#39;], &#39;mnli&#39;:[&#39;premise&#39;, &#39;hypothesis&#39;], &#39;qnli&#39;:[&#39;question&#39;, &#39;sentence&#39;], &#39;rte&#39;: [&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;wnli&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], } glue_num_labels = {&#39;mnli&#39;:3, &#39;stsb&#39;:1} . . def layerwise_splitter(model): emb = L(model.base_model.embeddings) layers = L(model.base_model.encoder.layer.children()) clf = L(m for m in list(model.children())[1:] if params(m)) groups = emb + layers + clf return groups.map(params) . . Running a GLUE task . task = &#39;sst2&#39;; validate_task() ds = load_dataset(ds_name, task) . valid_ = &#39;validation-matched&#39; if task==&#39;mnli&#39; else &#39;validation&#39; len(ds[&#39;train&#39;]), len(ds[valid_]) . (67349, 872) . train_idx, valid_idx = get_splits(ds, valid=valid_) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[valid_]]) train_ds[0] . {&#39;idx&#39;: 0, &#39;label&#39;: 0, &#39;sentence&#39;: &#39;hide new secretions from the parental units &#39;} . Here I use number of characters a proxy for length of tokenized text to speed up dls creation. . lens = train_ds.map(lambda s: {&#39;len&#39;: sum([len(s[i]) for i in glue_textfields[task] if i])}, remove_columns=train_ds.column_names, num_proc=2, keep_in_memory=True) train_lens = lens.select(train_idx)[&#39;len&#39;] valid_lens = lens.select(valid_idx)[&#39;len&#39;] . blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock() if task==&#39;stsb&#39; else CategoryBlock()] dblock = DataBlock(blocks = blocks, get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . dl_kwargs=[{&#39;res&#39;:train_lens}, {&#39;val_res&#39;:valid_lens}] dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs, dl_kwargs=dl_kwargs) dls.show_batch(max_n=4) . text category . 0 ... spiced with humor (&#39;i speak fluent flatula,&#39;advises denlopp after a rather, er, bubbly exchange with an alien deckhand ) and witty updatings ( silver&#39;s parrot has been replaced with morph, a cute alien creature who mimics everyone and everything around ) | 1 | . 1 stopped thinking about how good it all was, and started doing nothing but reacting to it - feeling a part of its grand locations, thinking urgently as the protagonists struggled, feeling at the mercy of its inventiveness, gasping at its visual delights | 1 | . 2 there aren&#39;t too many films that can be as simultaneously funny, offbeat and heartwarming ( without a thick shmear of the goo, at least ), but `` elling &#39;&#39; manages to do all three quite well, making it one of the year&#39;s most enjoyable releases | 1 | . 3 hatfield and hicks make the oddest of couples, and in this sense the movie becomes a study of the gambles of the publishing world, offering a case study that exists apart from all the movie&#39;s political ramifications. | 1 | . Single run . The GLUE benchmark contains 8 tasks and it might be cumbersome to systematize the results. To make the analysis simpler and much more powerful I will be using Weights&amp;Biases tracking platform. And even better thanks to Morgan McGuire (@morg) we have an open W&amp;B project. You just need to log your runs under glue-benchmark project and set entity=&quot;fastai_community&quot; and your results will be added to the pull for further investigation of hyperparameters. The fastest way to start participating would be to fork this notebook as it is set up to run any of the GLUE tasks with minimal changes. There is a lot to try: gradual unfreezing strategy is reported not to be helpful when finetuning Transformer-based models (for example see a discussion here); differential learning rates are used in NLP [1, 2] but are not common practice, do we need to use weight decay, if yes - how much and where, what suggestions from LR-finder work best? These are only few of many open questions and there are so much more. And even more interesting one how do this scale with dataset and model size? . Deep Learning as of now is highly empirical field and experiments require both some engeniering and compute. This post is aimed to fuel comunity effort towards finding empirical truth by joining small forces together. Even if you&#39;re new to NLP do not hasitate to participate and run couple of experiments while learning along the way! . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; if diff_lr_decay_factor: GROUP += f&quot;diff_lr_{diff_lr_decay_factor}&quot; NOTES = f&#39;finetuning {model_name} with {opt_func.__name__} lr={lr:.0e}&#39; TAGS =[model_name, ds_name, opt_func.__name__] . wandb.init(reinit=True, project=&quot;glue-benchmark&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func, splitter=layerwise_splitter) . if diff_lr_decay_factor != 0: k = len(layerwise_splitter(model)) lr = slice(lr*diff_lr_decay_factor**k,lr) metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(4, lr, wd=wd, cbs=cbs) . Could not gather input dimensions . epoch train_loss valid_loss accuracy time . 0 | 0.229984 | 0.264627 | 0.900229 | 02:02 | . 1 | 0.157474 | 0.251536 | 0.912844 | 02:02 | . 2 | 0.105107 | 0.252113 | 0.916284 | 02:03 | . 3 | 0.070137 | 0.278783 | 0.925459 | 02:03 | . Better model found at epoch 0 with accuracy value: 0.9002293348312378. Better model found at epoch 1 with accuracy value: 0.9128440618515015. Better model found at epoch 2 with accuracy value: 0.9162843823432922. Better model found at epoch 3 with accuracy value: 0.9254587292671204. . learn.show_results() . text category category_ . 0 the movie has an infectious exuberance that will engage anyone with a passing interest in the skate/surf culture, the l.a. beach scene and the imaginative ( and sometimes illegal ) ways kids can make a playground out of the refuse of adults. | 1 | 1 | . 1 what really makes it special is that it pulls us into its world, gives us a hero whose suffering and triumphs we can share, surrounds him with interesting characters and sends us out of the theater feeling we&#39;ve shared a great adventure. | 1 | 1 | . 2 this is a train wreck of an action film -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs. | 0 | 0 | . 3 it&#39;s one of those baseball pictures where the hero is stoic, the wife is patient, the kids are as cute as all get-out and the odds against success are long enough to intimidate, but short enough to make a dream seem possible. | 1 | 1 | . 4 though perry and hurley make inspiring efforts to breathe life into the disjointed, haphazard script by jay scherick and david ronn, neither the actors nor director reginald hudlin can make it more than fitfully entertaining. | 0 | 1 | . 5 may be far from the best of the series, but it&#39;s assured, wonderfully respectful of its past and thrilling enough to make it abundantly clear that this movie phenomenon has once again reinvented itself for a new generation. | 1 | 1 | . 6 despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults. | 0 | 0 | . 7 it&#39;s inoffensive, cheerful, built to inspire the young people, set to an unending soundtrack of beach party pop numbers and aside from its remarkable camerawork and awesome scenery, it&#39;s about as exciting as a sunburn. | 0 | 1 | . 8 but the power of these ( subjects ) is obscured by the majority of the film that shows a stationary camera on a subject that could be mistaken for giving a public oration, rather than contributing to a film&#39;s narrative. | 0 | 0 | . Sweeps . Finding the perfect learning rate for a task isn&#39;t easy. Add weight decay, different optimizers, differential learning rates and various scheduler to the mix and search for best hyperparameters becomes a really big task. For that reason there exist automated tools for hyperparameter search. Here we&#39;ll look at sweeps functionality provided by W&amp;B. It not only facilitates hyperparameter finetuning but also enables great visualization of the results, which might help for further analysis. Check out documentaion for more details. . def train(): with wandb.init() as run: cfg = run.config model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(task, 2)) metrics = glue_metrics[task] k = len(layerwise_splitter(model)) if cfg.diff_lr_decay_factor: lr = slice(cfg.lr*cfg.diff_lr_decay_factor**k,cfg.lr) learn = TransLearner(dls, model, metrics=metrics, opt_func=Adam, splitter=layerwise_splitter) learn.fit_one_cycle(n_epoch, cfg.lr, wd=cfg.wd, cbs=[WandbCallback(log_preds=False, log_model=False)]) del learn gc.collect() torch.cuda.empty_cache() torch.cuda.synchronize() . metrics = glue_metrics[task] metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ sweep_name = f&quot;glue-{task}-sweep&quot; sweep_config = { &quot;project&quot;:&quot;glue-benchmark&quot;, &quot;entity&quot;:&quot;fastai_cimmunity&quot;, &quot;name&quot;: sweep_name, &quot;method&quot;: &quot;random&quot;, &quot;parameters&quot;: { &quot;lr&quot;: {&quot;values&quot;:[1e-5,2e-5,3e-5,5e-5, 1e-4, 3e-4]}, &quot;wd&quot;: {&quot;values&quot;:[0.,1e-2,5e-2]}, &quot;diff_lr_decay_factor&quot;:{&quot;values&quot;:[0., 0.9, 0.8, 0.7, 0.6]} }, &quot;metric&quot;:{&quot;goal&quot;: &quot;maximise&quot;, &quot;name&quot;: metric_to_monitor}, &quot;early_terminate&quot;: {&quot;type&quot;: &quot;hyperband&quot;, &quot;s&quot;: 2, &quot;eta&quot;: 3, &quot;max_iter&quot;: 40} } . sweep_id = wandb.sweep(sweep_config) wandb.agent(sweep_id, function=train) . As a result we get a nice chart which helps to relate hyperparameter combinations to model performance. . . The sweep can be explored interactively by this link https://wandb.ai/fastai_community/glue-benchmark/sweeps/hc8ytty4. . Another task example: MNLI . MNLI task is interesting for a couple of reasons. It has the largest training set in the benchmark, for the results of training for MNLI might be useful for smaller tasks as we will consider in the next section. Unlike most of the GLUE tasks, which ar formulated as binary classification problem, this one has three categories: entailment, neutral and contradiction. One can argue that solving such kind of problem should envolve more &quot;understanding&quot; of the meaning of text. . task = &#39;mnli&#39;; validate_task() ds = load_dataset(ds_name, task) train_idx, valid_idx = get_splits(ds, valid=&#39;validation_matched&#39;) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[&#39;validation_matched&#39;]]) . Each sample contains premise and hypothesis, the task is to determine whether the hypothesis entails, contradicts or is neutral to the premise. Let&#39;s check out an example: . train_ds[0] . {&#39;hypothesis&#39;: &#39;Product and geography are what make cream skimming work. &#39;, &#39;idx&#39;: 0, &#39;label&#39;: 1, &#39;premise&#39;: &#39;Conceptually cream skimming has two basic dimensions - product and geography.&#39;} . The data preparation and dataloaders construction do not differ much from those for previous task: . lens = train_ds.map(lambda s: {&#39;len&#39;: len(s[&#39;premise&#39;])+len(s[&#39;hypothesis&#39;])}, remove_columns=train_ds.column_names, num_proc=4, keep_in_memory=True) train_lens = lens.select(train_idx)[&#39;len&#39;] valid_lens = lens.select(valid_idx)[&#39;len&#39;] blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock() if task==&#39;stsb&#39; else CategoryBlock()] dblock = DataBlock(blocks = blocks, get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . dl_kwargs=[{&#39;res&#39;:train_lens}, {&#39;val_res&#39;:valid_lens}] dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs, dl_kwargs=dl_kwargs, num_workers=4) dls.show_batch(max_n=4) . text text_ category . 0 well uh that&#39;s kind of obvious i mean they&#39;re even carrying it to to where now uh that they advertise on TV you know if your if you uh you know have done this or if you need this uh uh we&#39;ll sue for you and you don&#39;t have to pay us unless you but then what they don&#39;t tell you is that if you if they win you give them at least a third of the of the thing that they win so i don&#39;t know it is uh it&#39;s getting to be more business now rather than uh actually uh dealing with the crime than with uh um the uh punishment they the the lawyers are just in it for the money i&#39;m i&#39;m convinced i know i i agree with you i think you&#39;re real you&#39;re very right that the politicians should i think they | I think that there should be an equal representation of backgrounds in our politicians. | 0 | . 1 um-hum still have a problem with uh you know i haven&#39;t come to an absolute conclusion on my opinion on this but and i know other Christians would disagree with me my husband and i are kind of not even in agreement on this but we don&#39;t fight over it or anything but you know how can you know the Bible says bless your enemies and bless those that curse you and it&#39;s like be gentle unto all men apt to teach patient kind so it&#39;s like how can you i don&#39;t know for me i don&#39;t know you know i can&#39;t say that i agree with Vietnam because how can you be gentle unto all men and and then shoot them | As a Christian I believe that Vietnam is a necessary war. | 2 | . 2 These 1) approving both changes in existing services and the establishment of new servicesthose are known as classification cases; 2) adjudicating complaints from anyone who believes the Postal Service is not providing rates or services as required by law; 3) issuing advisory opinions when the Postal Service proposes a substantially nationwide change in the nature of its services; and, 4) our mostly recently assigned task, providing Congress with annual reports about the costs and revenues of international mail. | The Postal Service undergoing a review across all of its activities. | 1 | . 3 After all, in this piece the car-home-and-fire salesman turned global strategist describes the Gulf War as if it were a model of Clausewitzian clarity concerning ultimate goals and acceptable means, forgetting in the process that at the end of that war, the Bush/Powell/Schwarzkopf axis internally disagreed about war issues that had never been articulated for the American Should the U.S. destroy the Iraqi military, invade Baghdad, or topple Hussein even after Iraq was repulsed from Kuwait? | Bush, Powell and Schwarzkopf were in full agreement about war issues. | 2 | . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; NOTES = f&#39;finetuning {model_name} with Adam lr={lr:.0e}&#39; TAGS =[model_name, ds_name, &#39;adam&#39;, task] wandb.init(reinit=True, project=&quot;glue-benchmark&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . Training procedure is also very similar: . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics) . metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(4, lr, wd=wd, cbs=cbs) . Could not gather input dimensions . epoch train_loss valid_loss accuracy time . 0 | 0.532420 | 0.497427 | 0.801936 | 22:54 | . 1 | 0.447823 | 0.431625 | 0.835660 | 23:02 | . 2 | 0.384313 | 0.431362 | 0.841161 | 22:58 | . 3 | 0.297241 | 0.459461 | 0.843709 | 23:11 | . Better model found at epoch 0 with accuracy value: 0.8019357919692993. Better model found at epoch 1 with accuracy value: 0.8356596827507019. Better model found at epoch 2 with accuracy value: 0.8411614894866943. Better model found at epoch 3 with accuracy value: 0.8437086343765259. . learn.show_results() . text text_ category category_ . 0 yes they would they just wouldn&#39;t be able to own the kind of automobiles that they think they deserve to own or the kind of homes that we think we deserve to own we might have to you know just be able to i think if we a generation went without debt then the next generation like if if our our generation my husband and i we&#39;re twenty eight if we lived our lives and didn&#39;t become you know indebted like you know our generation before us that um the budget would balance and that we became accustomed to living with what we could afford which we wouldn&#39;t be destitute i mean we wouldn&#39;t be living on the street by any means but just compared to how spoiled we are we would be in our own minds but i feel like the generation after us would oh man it it | Society would be perfect and there would be no more war if we could just rid ourselves of our debt. | 1 | 2 | . 1 and i look back on that and i bought shoes i went shopping i did not need that money i did not need it i didn&#39;t need it i shouldn&#39;t have even qualified to get it i didn&#39;t need it and it would have been a little rough i might have eaten some bologna instead of roast beef out of the deli but i did not need it and as i look back now now we&#39;re paying that back i told my son if you have to live in the ghetto to go to college do it but don&#39;t take out ten thousand dollars in loans don&#39;t do it and i don&#39;t i hope don&#39;t think he&#39;ll have to do that but i just so like we might if we didn&#39;t have those loans we could have saved in the last five years the money for that and i believe | My friends should look towards me as a model of saving money. | 1 | 1 | . 2 and i look back on that and i bought shoes i went shopping i did not need that money i did not need it i didn&#39;t need it i shouldn&#39;t have even qualified to get it i didn&#39;t need it and it would have been a little rough i might have eaten some bologna instead of roast beef out of the deli but i did not need it and as i look back now now we&#39;re paying that back i told my son if you have to live in the ghetto to go to college do it but don&#39;t take out ten thousand dollars in loans don&#39;t do it and i don&#39;t i hope don&#39;t think he&#39;ll have to do that but i just so like we might if we didn&#39;t have those loans we could have saved in the last five years the money for that and i believe | I regret taking out loans. | 0 | 1 | . 3 well the first thing for me is i wonder i see a couple of different ways of talking about what privacy is um if privacy is something that disturbs your private state i mean an invasion of privacy is something that disturbs your private state that&#39;s one thing and if privacy is something that comes into your private state and extracts information from it in other words finds something out about you that&#39;s another and the first kind of invasion of the first type of privacy seems invaded to me in very much everyday in this country but in the second type at least overtly uh where someone comes in and uh finds out information about you that should be private uh does not seem uh um obviously everyday | Talking about privacy is a complicated topic, there are a couple different ways of talking about it, for example privacy is something that disturbs your private state... | 0 | 1 | . 4 The rule prohibits the sale of nicotine-containing cigarettes and smokeless tobacco to individuals under the age of 18; requires manufacturers, distributors, and retailers to comply with various conditions regarding the sale and distribution of these products; requires retailers to verify a purchaser&#39;s age by photographic identification; prohibits all free samples; limits the distribution of these products through vending machines and self-service displays by permitting such methods of sale only in facilities where access by individuals under 18 is prohibited; limits the advertising and labeling to which children and adolescents are exposed; prohibits promotional, non-tobacco items such as hats and tee shirts; prohibits sponsorship of | This rule will make the sale of tobacco products to people under 18 years old legal in every state and Mexico. | 2 | 2 | . 5 yeah the the i mean people like that are crazy i did a study on it though when i was in high school it was one of these things we had to pick a topic to to investigate and at that time i don&#39;t think it&#39;s like that any more but at that time uh it was very unfair capital punishment was a lot more common and if you tended and it tended to be that if you were ignorant or if you were a foreigner or if you were black or any minority for that matter the chances your chances of of uh getting the death penalty were you know like hundreds of times greater than if you could just communicate well i mean you didn&#39;t have to be um you didn&#39;t even necessarily have to be white but if you could just communicate and you could come | It was something I performed research on during high school. | 0 | 0 | . 6 yeah because you look at the statistics now and i&#39;m sure it&#39;s in your your newspapers just like it is in ours that every major city now the increase of crime is is escalating i mean there are more look at the look at the people there are being shot now i mean every day there&#39;s there&#39;s dozens of dozens of people across the nation they just get blown away for no reason you know stray bullets or California they were going out there and they were shooting and they get these guys and they don&#39;t do anything with them so i kind of i kind of agree with you i&#39;m kind of you still in the in the uh prison system | &quot;Crime is escalating now in every major city, however there are plans in place now.&quot; | 1 | 1 | . 7 i know that you know the further we go from Adam the worse the food is for you but God still somehow makes us all be able to still live i think it&#39;s a miracle we&#39;re all still alive after so many generations well the last couple of processed foods you know i mean but i don&#39;t know i like to i like to my i like to be able to eat really healthy you know what am saying and i guess i&#39;m going to have to wait for the millennium i think though because i do don&#39;t think we&#39;re going to restore the earth to you know i think Jesus is the only one that can make this earth be restored to what it should be | It is miraculous God still provides for us to this day. | 0 | 0 | . 8 i know because i think i&#39;ve been reading i read this ten years ago that they were having these big uh um rallies and people would be in the streets flashing signs statehood yes and other people would statehood down the statehood it&#39;s it down there if you&#39;re um familiar with their politics they uh it&#39;s very uh i i don&#39;t know it&#39;s called Latino there they have loudspeakers on their cars and they run down the neighborhood saying vote for you know Pierre he&#39;s or uh Pedro uh Pedro he&#39;s the best it&#39;s it&#39;s really kind of comical | Ten years ago, they rallies on streets with flashing signs and loudspeakers advertising voting candidates. | 0 | 0 | . MNLI task has another missmatched validation set. matched set contains in-domain data and the missmatched is a cross-domain. . valid_mm_dl = dls.test_dl(ds[&#39;validation_mismatched&#39;], with_labels=True) learn.validate(dl=valid_mm_dl) . Notice that there are similar datasets available (e.g. snli dataset). Those might be used to improve the performance. But for these post I&#39;ll limit the scope to GLUE data only and leave the experiments with extra data for upcoming posts. . Low resource tasks . Some daatsets are rather small, RTE has only 2.5k samples in the training set. This is not much at all for untrivial language task like this one. But we can try to use a small trick for that. The MNLI task is quite similar and has much more training data. Let&#39;s reuse model trained on it for improving RTE score. This trick is common practice and has been employed in original RoBERTa paper when reporting GLUE score. . task = &#39;rte&#39;; validate_task() ds = load_dataset(ds_name, task) valid_ = &#39;validation-matched&#39; if task==&#39;mnli&#39; else &#39;validation&#39; len(ds[&#39;train&#39;]), len(ds[valid_]) train_idx, valid_idx = get_splits(ds, valid=valid_) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[valid_]]) . train_ds[0] . {&#39;idx&#39;: 0, &#39;label&#39;: 1, &#39;sentence1&#39;: &#39;No Weapons of Mass Destruction Found in Iraq Yet.&#39;, &#39;sentence2&#39;: &#39;Weapons of Mass Destruction Found in Iraq.&#39;} . blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock() if task==&#39;stsb&#39; else CategoryBlock()] dblock = DataBlock(blocks = blocks, get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs) dls.show_batch(max_n=4) . text text_ category . 0 No Weapons of Mass Destruction Found in Iraq Yet. | Weapons of Mass Destruction Found in Iraq. | 1 | . 1 The most recent poll carried out by NOP market research in January revealed that 61% of Britons are opposed to joining the euro. | The introduction of the euro has been opposed. | 0 | . 2 The disappearance of York University chef Claudia Lawrence is now being treated as suspected murder, North Yorkshire Police said. However detectives said they had not found any proof that the 35-year-old, who went missing on 18 March, was dead. Her father Peter Lawrence made a direct appeal to his daughter to contact him five weeks after she disappeared. His plea came at a news conference held shortly after a £10,000 reward was offered to help find Miss Lawrence. Crimestoppers said the sum they were offering was &quot;significantly higher&quot; than usual because of public interest in the case. | Claudia Lawrence is 35 years old. | 0 | . 3 A Continental Connection flight from Newark to Buffalo crashed into a house about four to six miles from Buffalo Niagara International Airport on Thursday night, killing 50 people, officials said. Continental Airlines Flight 3407 is a daily commuter flight from Newark Liberty International Airport in Newark, New Jersey to Buffalo, New York, operated under the Continental Connection brand by Virginia-based regional airline Colgan Air. | A daily commuter flight crashed in New York. | 0 | . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; if diff_lr_decay_factor: GROUP += f&quot;diff_lr_{diff_lr_decay_factor}&quot; NOTES = f&#39;finetuning {model_name} with {opt_func.__name__} lr={lr:.0e}&#39; TAGS =[model_name, ds_name, opt_func.__name__] wandb.init(reinit=True, project=&quot;fasthugs&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func) . try: learn.load(&#39;distilroberta-base-mnli&#39;, with_opt=False, strict=False) except RuntimeError as e: print(e) . Error(s) in loading state_dict for RobertaForSequenceClassification: size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]). size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]). . if diff_lr_decay_factor != 0: k = len(layerwise_splitter(model)) lr = slice(lr*diff_lr_decay_factor**k,lr) metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(10, lr, wd=wd, cbs=cbs, pct_start=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.569979 | 0.565890 | 0.693141 | 00:30 | . 1 | 0.511280 | 0.529077 | 0.736462 | 00:31 | . 2 | 0.409093 | 0.601690 | 0.743682 | 00:31 | . 3 | 0.265996 | 0.763166 | 0.736462 | 00:31 | . 4 | 0.171846 | 0.770063 | 0.754513 | 00:32 | . 5 | 0.098103 | 0.922156 | 0.768953 | 00:32 | . 6 | 0.067698 | 1.030401 | 0.761733 | 00:31 | . 7 | 0.048222 | 1.007513 | 0.772563 | 00:31 | . 8 | 0.034855 | 1.056370 | 0.765343 | 00:32 | . 9 | 0.021131 | 1.069907 | 0.761733 | 00:32 | . As one can see by using this simple trick we&#39;ve improved the result reported at HuggingFace model card by some 10%. Pretty nice, ha? . Just to be sure that improvement is due to using model finetuned on mnli let&#39;s do another run starting from vanilla distilroberta: . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func) . learn.fit_one_cycle(10, lr, wd=wd, cbs=cbs, pct_start=0.1) . Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;, &#39;roberta.pooler.dense.weight&#39;, &#39;roberta.pooler.dense.bias&#39;] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . epoch train_loss valid_loss accuracy time . 0 | 0.695126 | 0.691306 | 0.527076 | 00:31 | . 1 | 0.692349 | 0.692152 | 0.480144 | 00:31 | . 2 | 0.678994 | 0.641740 | 0.624549 | 00:31 | . 3 | 0.602276 | 0.600447 | 0.671480 | 00:31 | . 4 | 0.488653 | 0.662074 | 0.678700 | 00:31 | . 5 | 0.377430 | 0.683057 | 0.678700 | 00:31 | . 6 | 0.269494 | 0.967499 | 0.657040 | 00:31 | . 7 | 0.182777 | 1.016970 | 0.685921 | 00:32 | . 8 | 0.140067 | 1.038462 | 0.696751 | 00:31 | . 9 | 0.113930 | 1.068865 | 0.682310 | 00:32 | . The same is applicable for STSB taks, which has 7k training samples. Performance gain for STSB is not so prominent but it&#39;s still there. You can compare the results for cold and warm starts in this W&amp;B report. . Concluding thoughts . With this we have an simple easy to use framework for quick experimentation with LM finetuning. HuggingFace provides us with huge variety of state of the art Transformers and fastai facilitates configurable training loop with gret API. You are wellcomed to share your comments in dedicated fastai forums topic, try out fasthugs (I&#39;m happy to here your opinions and accept feature requests) and finally open this notebook on Colab, select your task and try to set new best for the model. .",
            "url": "https://arampacha.github.io/thoughtsamples/fastai/huggingface/transformers/2021/05/07/glue-benchmark.html",
            "relUrl": "/fastai/huggingface/transformers/2021/05/07/glue-benchmark.html",
            "date": " • May 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A Transformer based Language Model from scratch",
            "content": "In this notebook i&#39;m going to construct transformer based language model from scratch starting with the simplest building blocks. This is inspired by Chapter 12 of Deep Learning for Coders book in which it&#39;s demonstrated how to create a Recurrent Neural Network. It provides a strong intuition of how RNNs relate to regular feed-forward neural nets and why certain design choices were made. Here we aim to aquire similar kind of intuition about Transfomer based architectures. . But as always we should start with the data to be modeled, &#39;cause without data any model makes no particular sense. . Data . Similar to authors of the book I&#39;ll use simple Human numbers dataset which is specifically designed to prototyping model fast and straightforward. For more details on the data one can refer to the aforemantioned book chapter which is also available for free as a notebook (isn&#39;t that awesome?!) . from fastai.text.all import * path = untar_data(URLs.HUMAN_NUMBERS) Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;train.txt&#39;),Path(&#39;valid.txt&#39;)] . The data consists of consecutive numbers from 1 to 9999 inclusive spelled as words. . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . text = &#39; . &#39;.join([l.strip() for l in lines]) tokens = text.split(&#39; &#39;) tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . vocab = L(*tokens).unique() vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . The task will be to predict subsequent token given preceding three. This kind of tasks when the goal is to predict next token from previous ones is called autoregresive language modeling. . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . bs = 64 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . x, y = dls.one_batch() x.shape, y.shape . (torch.Size([64, 3]), torch.Size([64])) . Dot product attention . . The core idea behind Transformers is Attention. Since the release of famous paper Attention is All You Need transformers has become most popular architecture for language modelling. . There are a lot of great resourses explaining transformers architecture. I&#39;ll list some of those I found useful and comprehensive: . The Annotated Transformer completes the original paper with code | Encoder-Decoder Model notebook by huggingface gives mathemetically grounded explanation of how transformer encoder-decoder models work | The Illustrated GPT-2 one of the great blogposts by Jay Alammar visualizing generative language modelling on exaple of GPT-2 | minGPT cool repo by A. Karpathy providing clear minimal implementation of GPT model | There exist multiple attention mechanisms. The particular one used in the original transformer paper is Scaled Dot Product attention. Given query vector for particular token we will compare it with a key vector for each token in a sequence and decide how much value vectors of those will effect resulting representetion of the token of interest. One way to view this from a linguistic prospective is: a key is a question each word respondes to, value is information that word represent and a query is related to what every word was looking to combine with. . Mathemetically we can compute attention for all q, k, v in a matrix form: . $$ textbf {Attention}(Q,K,V) = textbf {softmax}({QK^T over sqrt d_k})V $$ . Note that dot product $QK^T$ results in matrix of shape (seq_len x seq_len). Then it is devided by $ sqrt d_k$ to compensate the fact, that longer sequences will have larger dot product. $ textbf{softmax}$ is applied to rescale the attention matrix to be betwin 0 and 1. When multiplied by $V$ it produces a matrix of the same shape as $V$ (seq_len x dv). . So where those q, k, v come from. Well that&#39;s fairly straitforward queries are culculated from the embeddings of tokens we want to find representation for by simple linear projection. Keys and values are calculated from the embeddings of context tokens. In case of self attention all of them come from the original sequence. . class SelfAttention(Module): def __init__(self, d_in, d_qk, d_v=None): d_v = ifnone(d_v, d_qk) self.iq = nn.Linear(d_in, d_qk) self.ik = nn.Linear(d_in, d_qk) self.iv = nn.Linear(d_in, d_v) self.out = nn.Linear(d_v, d_in) self.scale = d_qk**-0.5 def forward(self, x): q, k, v = self.iq(x), self.ik(x), self.iv(x) q *= self.scale return self.out(F.softmax(q@k.transpose(-2,-1), -1)@v) . Even though self attention mechanism is extremely useful it posseses limited expressive power. Essentially we are computing weighted some of the input modified by single affine transformation, shared across the whole sequence. To add more computational power to the model we can introduce fully connected feedforward network on top of the SelfAttention layer. . Curious reader can find detailed formal analysis of the roles of SelfAttention and FeedForward layers in transformer architecture in this paper by C. Yun et al. In brief the authors state that SelfAttention layers compute precise contextual maps and FeedForward layers then assign the results of these contextual maps to the desired output values. . class FeedForward(Module): def __init__(self, d_in, d_ff): self.lin1 = nn.Linear(d_in, d_ff) self.lin2 = nn.Linear(d_ff, d_in) self.act = nn.ReLU() def forward(self, x): out = self.lin2(self.act(self.lin1(x))) return out . The output would be of shape (bs, seq_len, d) which then may be mapped to (bs, seq_len, vocab_sz) using linear layer. But we have only one target. To adress this issue we can simply do average pooling over seq_len dimention. . The resulting model is fairly simple: . class Model1(Module): def __init__(self, vocab_sz, d_model, d_qk, d_ff): self.emb = Embedding(vocab_sz, d_model) self.attn = SelfAttention(d_model, d_qk) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) x = x.mean(1) return self.out(x) . model = Model1(len(vocab), 64, 64, 128) out = model(x) out.shape . torch.Size([64, 30]) . learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.lr_find() . SuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.019054606556892395) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.023451 | 2.183568 | 0.416211 | 00:03 | . 1 | 1.563715 | 2.401872 | 0.361540 | 00:03 | . 2 | 1.540635 | 1.874314 | 0.452817 | 00:03 | . 3 | 1.594812 | 1.739459 | 0.456145 | 00:03 | . 4 | 1.614958 | 1.713703 | 0.468743 | 00:03 | . To evaluete the model performance we need to compare it to some baseline. Let&#39;s see what would be the accuracy if of the model which would always predict most common token. . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . As you can see, always predicting &quot;thousand&quot; which turn out to be the most common token in the dataset would result in ~15% accuracy. Our simple transformer does much better then that. It feels promising, so let&#39;s try to improve the architecture and check if we can get better results. . Multihead attention . A structured sequence may comprise multiple distinctive kinds of relationships. Our model is forced to learn only one way in which queries, keys and values are constructed from the original token embedding. To remove this limitation we can modify attention layer include multiple heads which would correspond to extracting different kinds of relationships between tokens. The MultiHeadAttention layer consits of several heads each of those is similar to SelfAttention layer we made before. To keep computational cost of the multi-head layer we set $d_k = d_v = d_{model}/n_h$, where $n_h$ is number of heads. . class SelfAttention(Module): def __init__(self, d_in, d_qk, d_v=None): d_v = ifnone(d_v, d_qk) self.iq = nn.Linear(d_in, d_qk) self.ik = nn.Linear(d_in, d_qk) self.iv = nn.Linear(d_in, d_v) self.scale = d_qk**-0.5 def forward(self, x): q, k, v = self.iq(x), self.ik(x), self.iv(x) return F.softmax(q@k.transpose(-2,-1)*self.scale, -1)@v . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads, d_qk=None, d_v=None): d_qk = ifnone(d_qk, d_model//n_heads) d_v = ifnone(d_v, d_qk) self.heads = nn.ModuleList([SelfAttention(d_model, d_qk) for _ in range(n_heads)]) self.out = nn.Linear(d_v*n_heads, d_model) def forward(self, x): out = [m(x) for m in self.heads] return self.out(torch.cat(out, -1)) . inp = torch.randn(8, 10, 64) mha = MultiHeadAttention(64, 8) out = mha(inp) out.shape . torch.Size([8, 10, 64]) . class Model2(Module): def __init__(self, vocab_sz, d_model=64, n_heads=4, d_ff=64*4): self.emb = nn.Embedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) x = x.mean(1) return self.out(x) . learn = Learner(dls, Model2(len(vocab)), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 5e-4) . epoch train_loss valid_loss accuracy time . 0 | 2.177783 | 2.223564 | 0.332303 | 00:04 | . 1 | 1.629889 | 1.867587 | 0.445210 | 00:04 | . 2 | 1.607201 | 1.738342 | 0.464464 | 00:04 | . 3 | 1.606301 | 1.711135 | 0.467316 | 00:04 | . 4 | 1.592446 | 1.708671 | 0.467554 | 00:04 | . MultiHead Attention Refactor . Python for loops are slow, therefore it is better to refactor the MultiHeadAttention module to compute Q, K, V for all heads in batch. . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads): assert d_model%n_heads == 0 self.n_heads = n_heads #d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.out = nn.Linear(d_model, d_model, bias=False) self.scale = d_model//n_heads def forward(self, x): bs, seq_len, d = x.size() # (bs,sl,d) -&gt; (bs,sl,nh,dh) -&gt; (bs,nh,sl,dh) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q*= self.scale att = F.softmax(q@k.transpose(-2,-1), -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return self.out(out) . learn = Learner(dls, Model2(len(vocab)), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.945255 | 2.148961 | 0.362729 | 00:03 | . 1 | 1.576766 | 2.055920 | 0.426432 | 00:03 | . 2 | 1.599834 | 1.934431 | 0.443784 | 00:03 | . 3 | 1.624774 | 1.742481 | 0.460185 | 00:03 | . 4 | 1.615320 | 1.773680 | 0.452817 | 00:03 | . Note that some speedup is observed even on such a tiny dataset and small model. . More signal . Similarly to the RNN case considered in the book, we can take the next step and create more signal for the model to learn from. To adapt to the modified objective we need to make couple of steps. First let&#39;s rearrange data to proper input-target pairs for the new task. . Arranging data . Unlike RNN the tranformer is not a stateful model. This means it treats each sequence indepently and can only attend within fixed length context. This limitation was adressed by authors of Transformer-XL paper where adding a segment-level recurrence mechanism and a novel positional encoding scheme were proposed to enable capturing long-term dependencies. I will not go into details of TransformerXL architecture here. As will shell see stateless transformer can also learn a lot about the structure of our data. . One thing to note in this case is that we don&#39;t need to maintain the structure of the data outside of the sequences, so we can shuffle the sequences randomly in the dataloader. . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=bs, drop_last=True, shuffle=True) xb, yb = dls.one_batch() xb.shape, yb.shape . (torch.Size([64, 16]), torch.Size([64, 16])) . [L(vocab[o] for o in s) for s in seqs[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . Positional encoding . Before we did average pooling over seq_len dimention. Our model didn&#39;t care about the order of the tokens at all. But actually order of the tokens in a sentence matter a lot. In our case one hundred two and two hundred one are pretty different and hundred one two doesn&#39;t make sense. . To encorporate positional information into the model authors of the transformer architecture proposed to use positional encodings in addition to regular token embeddings. Positional encodings may be learned, but it&#39;s also possible to use hardcoded encodings. For instance encodings may be composed of sin and cos. In this way each position in a sequence will get unique vector associated with it. . class PositionalEncoding(Module): def __init__(self, d): self.register_buffer(&#39;freq&#39;, 1/(10000 ** (torch.arange(0., d, 2.)/d))) self.scale = d**0.5 def forward(self, x): device = x.device pos_enc = torch.cat([torch.sin(torch.outer(torch.arange(x.size(1), device=device), self.freq)), torch.cos(torch.outer(torch.arange(x.size(1), device=device), self.freq))], axis=-1) return x*self.scale + pos_enc . x = torch.zeros(1, 16, 64) encs = PositionalEncoding(64)(x) plt.matshow(encs.squeeze()) plt.xlabel(&#39;Embedding size&#39;) plt.ylabel(&#39;Sequence length&#39;) plt.show() . . class TransformerEmbedding(Module): def __init__(self, emb_sz, d_model): self.emb = nn.Embedding(emb_sz, d_model) self.pos_enc = PositionalEncoding(d_model) def forward(self, x): return self.pos_enc(self.emb(x)) . class Model3(Module): def __init__(self, vocab_sz, d_model=64, n_heads=4, d_ff=64*4): self.emb = TransformerEmbedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) return self.out(x) . model = Model3(len(vocab)) out = model(xb) out.shape . torch.Size([64, 16, 30]) . def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . learn = Learner(dls, Model3(len(vocab)), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.059098 | 1.966106 | 0.244059 | 00:01 | . 1 | 0.996265 | 0.151726 | 0.956055 | 00:01 | . 2 | 0.427699 | 0.078305 | 0.977865 | 00:01 | . 3 | 0.207902 | 0.066726 | 0.976481 | 00:01 | . 4 | 0.116920 | 0.074462 | 0.974935 | 00:01 | . Wow! Thats a great accuracy! So the problem is solved and we only needed one attention layer and 2 layer deep feed-forward block? Don&#39;t you feel somewhat skeptical about this result? . Well, you should be! Think about what we did here: the goal was to predict a target sequence, say [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;] from an input [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;]. These two sequences intersect on all positions except the first and the last one. So models needs to learn simply to copy input tokens starting from the second one to the outputs. In our case this will result in 15 correct predictions of total 16 positions, thats almost 94% accuracy. This makes the task very simple but not very useful to learn. To train proper autoregressive language model, as we did with RNNs, a concept of masking is to be introduced. . Causal Masking . So we want to allow the model for each token to attend only to itself and those prior to it. To acomplish this we can set all the values of attention matrix above the main diagonal to $- infty$. After softmax this values will effectively turn to 0 thus disabling attention to the &quot;future&quot;. . def get_subsequent_mask(x): sz = x.size(1) mask = (torch.triu(torch.ones(sz, sz, device=x.device)) == 1).transpose(0, 1) mask = mask.float().masked_fill(mask == 0, float(&#39;-inf&#39;)).masked_fill(mask == 1, float(0.0)) return mask . inp = torch.randn(8, 10, 64) mask = get_subsequent_mask(inp) plt.matshow(mask); . q, k = torch.rand(1,10,32), torch.randn(1,10,32) att_ = F.softmax((q@k.permute(0,2,1)+mask), -1) plt.matshow(att_[0].detach()); . We should also modify the attention layer to except mask: . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads): assert d_model%n_heads == 0 self.n_heads = n_heads d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.scale = d_qk**-0.5 self.out = nn.Linear(d_model, d_model, bias=False) def forward(self, x, mask=None): bs, seq_len, d = x.size() mask = ifnone(mask, 0) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q*= self.scale att = F.softmax(q@k.transpose(-2,-1) + mask, -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return self.out(out) . class Model4(Module): def __init__(self, vocab_sz, d_model=64, n_heads=8, d_ff=64*4): self.emb = TransformerEmbedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) mask = get_subsequent_mask(x) x = self.ff(self.attn(x, mask)) return self.out(x) . learn = Learner(dls, Model4(len(vocab)), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.399806 | 2.321602 | 0.262533 | 00:01 | . 1 | 1.804210 | 2.251197 | 0.251709 | 00:01 | . 2 | 1.559652 | 2.320621 | 0.282878 | 00:01 | . 3 | 1.426687 | 2.365385 | 0.281006 | 00:01 | . 4 | 1.355069 | 2.430914 | 0.301025 | 00:01 | . Now we get somewhat lower accuracy, which is expected given that the task has become more difficult. Also training loss is significantly lower than validation loss, which means the model is overfitting. Let&#39;s see if the same approaches as was taken to RNNs can help. . Multilayer transformer . To solve a more difficult task we ussualy need a deeper model. For convenience let&#39;s make a TransformerLayer which will combine self-attention and feed-forward blocks. . class TransformerLayer(Module): def __init__(self, d_model, n_heads=8, d_ff=None, causal=True): d_ff = ifnone(d_ff, 4*d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.causal = causal def forward(self, x, mask=None): if self.causal: mask = get_subsequent_mask(x) return self.ff(self.attn(x, mask)) . class Model5(Module): def __init__(self, vocab_sz, d_model=64, n_layer=4, n_heads=8): self.emb = TransformerEmbedding(vocab_sz, d_model) self.encoder = nn.Sequential(*[TransformerLayer(d_model, n_heads) for _ in range(n_layer)]) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.encoder(x) return self.out(x) . learn = Learner(dls, Model5(len(vocab), n_layer=4), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.896548 | 2.794600 | 0.151611 | 00:03 | . 1 | 2.790987 | 2.813897 | 0.151774 | 00:03 | . 2 | 2.769421 | 2.791088 | 0.151774 | 00:04 | . 3 | 2.756661 | 2.790368 | 0.151367 | 00:04 | . 4 | 2.748009 | 2.799597 | 0.150960 | 00:04 | . That&#39;s not good! 4 layer deep Transformer strugles to learn anything. But there are good news, this problem has been already resolved in the original transformer. . Residual connections and Regularization . If you are familiar with ResNets the proposed solution will not surprise you much. The idea is simple yet very effective. Instead of returning modified output $f(x)$ each transformer sublayer will return $x + f(x)$. Thishow to produce allows the original input to propagate freely through the model. So the model learns not an entirely new representation of $x$ but how to modify $x$ to add some useful information to the original representation. . As we modify layers to include the residual connections let&#39;s also add some regularization by inserting Dropout layers. . class TransformerEmbedding(Module): def __init__(self, emb_sz, d_model, p=0.1): self.emb = Embedding(emb_sz, d_model) nn.init.trunc_normal_(self.emb.weight, std=d_model**-0.5) self.pos_enc = PositionalEncoding(d_model) self.drop = nn.Dropout(p) def forward(self, x): return self.drop(self.pos_enc(self.emb(x))) . Another modification is to add layer normalization which is intended to improve learning dynamics of the network by reparametrising data statistics and is generally used in transformer based architectures. . class FeedForward(Module): def __init__(self, d_model, d_ff, p=0.2): self.lin1 = nn.Linear(d_model, d_ff) self.lin2 = nn.Linear(d_ff, d_model) self.act = nn.ReLU() self.norm = nn.LayerNorm(d_model) self.drop = nn.Dropout(p) def forward(self, x): x = self.norm(x) out = self.act(self.lin1(x)) out = self.lin2(out) return x + self.drop(out) . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads, p=0.1): assert d_model%n_heads == 0 self.n_heads = n_heads d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.scale = d_qk**0.5 self.out = nn.Linear(d_model, d_model, bias=False) self.norm = nn.LayerNorm(d_model) self.drop = nn.Dropout(p) def forward(self, x, mask=None): bs, seq_len, d = x.size() mask = ifnone(mask, 0) x = self.norm(x) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) att = F.softmax(q@k.transpose(-2,-1)/self.scale + mask, -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return x + self.drop(self.out(out)) . class TransformerLayer(Module): def __init__(self, d_model, n_heads=8, d_ff=None, causal=True, p_att=0.1, p_ff=0.1): d_ff = ifnone(d_ff, 4*d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff, p=p_ff) self.causal = causal self._init() def forward(self, x, mask=None): if self.causal: mask = get_subsequent_mask(x) return self.ff(self.attn(x, mask)) def _init(self): for p in self.parameters(): if p.dim()&gt;1: nn.init.xavier_uniform_(p) . class Model6(Module): def __init__(self, vocab_sz, d_model=64, n_layer=4, n_heads=8, p_emb=0.1, p_att=0.1, p_ff=0.2, tie_weights=True): self.emb = TransformerEmbedding(vocab_sz, d_model, p=p_emb) self.encoder = nn.Sequential(*[TransformerLayer(d_model, n_heads, p_att=p_att, p_ff=p_ff) for _ in range(n_layer)], nn.LayerNorm(d_model)) self.out = nn.Linear(d_model, vocab_sz) if tie_weights: self.out.weight = self.emb.emb.weight def forward(self, x): x = self.emb(x) x = self.encoder(x) return self.out(x) . learn = Learner(dls, Model6(len(vocab), n_layer=2), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(8, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.635322 | 2.077494 | 0.193929 | 00:02 | . 1 | 1.751456 | 1.042653 | 0.667806 | 00:02 | . 2 | 1.120504 | 0.743249 | 0.767822 | 00:02 | . 3 | 0.825433 | 0.797066 | 0.733643 | 00:02 | . 4 | 0.687212 | 0.747418 | 0.751058 | 00:02 | . 5 | 0.615355 | 0.815827 | 0.747233 | 00:02 | . 6 | 0.576437 | 0.809159 | 0.751302 | 00:02 | . 7 | 0.557015 | 0.823612 | 0.744466 | 00:02 | . Bonus - Generation example . Learning to predict numbers is great, but let&#39;s try something more fun. We can train a langyage model generate some texts. For example let&#39;s try to generate some text in style of Lewis Carroll. For this we&#39;ll fit a language model on &quot;Alice in Wonderland&quot; and &quot;Through the looking glass&quot;. . def parse_txt(fns): txts = [] for fn in fns: with open(fn) as f: tmp = &#39;&#39; for line in f.readlines(): line = line.strip(&#39; n&#39;) if line: tmp += &#39; &#39; + line elif tmp: txts.append(tmp.strip()) tmp = &#39;&#39; return txts . . texts = parse_txt([path/&#39;11-0.txt&#39;, path/&#39;12-0.txt&#39;]) . len(texts) . 1779 . texts[0:2] . [&#39; ufeffCHAPTER I. Down the Rabbit-Hole&#39;, &#39;Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?”&#39;] . class CharTokenizer(Transform): &quot;Simple charecter level tokenizer&quot; def __init__(self, vocab=None): self.vocab = ifnone(vocab, [&#39;&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;] + list(string.printable)) self.c2i = defaultdict(int, [(c,i) for i, c in enumerate(self.vocab)]) def encodes(self, s, add_bos=False, add_eos=False): strt = [self.c2i[&#39;xxbos&#39;]] if add_bos else [] end = [self.c2i[&#39;xxeos&#39;]] if add_eos else [] return LMTensorText(strt + [self.c2i[c] for c in s] + end) def decodes(self, s, remove_special=False): return TitledStr(&#39;&#39;.join([self.decode_one(i) for i in s])) def decode_one(self, i): if i == 2: return &#39; n&#39; elif i == 1: return &#39;&#39; else: return self.vocab[i] @property def vocab_sz(self): return len(self.vocab) . . tok = CharTokenizer() . def add_bos_eos(x:list, bos_id=1, eos_id=2): return [bos_id] + x + [eos_id] . nums = [add_bos_eos(tok(t.lower()).tolist()) for t in texts] . len(nums) . 1779 . all_nums = [] for n in nums: all_nums.extend(n) . all_nums[:15] . [1, 0, 15, 20, 13, 28, 32, 17, 30, 97, 21, 78, 97, 16, 27] . print(tok.decode(all_nums[:100])) . chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on . sl = 512 seqs = L((tensor(all_nums[i:i+sl]), tensor(all_nums[i+1:i+sl+1])) for i in range(0,len(all_nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], device=&#39;cuda&#39;, bs=8, drop_last=True, shuffle=True) xb, yb = dls.one_batch() xb.shape, yb.shape . (torch.Size([8, 512]), torch.Size([8, 512])) . model = Model6(tok.vocab_sz, 512, 6, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() learn.lr_find() . SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.6309573650360107) . learn.fit_one_cycle(50, 5e-4, cbs=EarlyStoppingCallback(patience=5)) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.207689 | 3.014779 | 0.187012 | 20.384583 | 00:06 | . 1 | 2.957462 | 2.648416 | 0.258105 | 14.131640 | 00:06 | . 2 | 2.645952 | 2.427977 | 0.287435 | 11.335924 | 00:07 | . 3 | 2.499318 | 2.395460 | 0.292887 | 10.973244 | 00:07 | . 4 | 2.436253 | 2.394616 | 0.288965 | 10.963985 | 00:07 | . 5 | 2.402833 | 2.364455 | 0.295703 | 10.638234 | 00:07 | . 6 | 2.387243 | 2.367871 | 0.290381 | 10.674637 | 00:07 | . 7 | 2.375951 | 2.384834 | 0.285010 | 10.857258 | 00:07 | . 8 | 2.372440 | 2.380138 | 0.276270 | 10.806398 | 00:07 | . 9 | 2.355436 | 2.329239 | 0.305892 | 10.270120 | 00:07 | . 10 | 2.314718 | 2.250078 | 0.331901 | 9.488480 | 00:07 | . 11 | 2.231658 | 2.096768 | 0.385319 | 8.139816 | 00:07 | . 12 | 2.145322 | 1.989301 | 0.413477 | 7.310425 | 00:07 | . 13 | 1.998541 | 1.862125 | 0.444971 | 6.437402 | 00:07 | . 14 | 1.880289 | 1.772380 | 0.465283 | 5.884840 | 00:07 | . 15 | 1.777548 | 1.735309 | 0.482080 | 5.670681 | 00:07 | . 16 | 1.692429 | 1.649408 | 0.501937 | 5.203897 | 00:07 | . 17 | 1.616076 | 1.616357 | 0.513688 | 5.034715 | 00:07 | . 18 | 1.548247 | 1.586346 | 0.522575 | 4.885864 | 00:07 | . 19 | 1.484927 | 1.550523 | 0.532633 | 4.713936 | 00:07 | . 20 | 1.430124 | 1.512773 | 0.543213 | 4.539303 | 00:07 | . 21 | 1.375972 | 1.500666 | 0.545199 | 4.484674 | 00:07 | . 22 | 1.324876 | 1.491262 | 0.552637 | 4.442699 | 00:07 | . 23 | 1.276637 | 1.469852 | 0.557975 | 4.348590 | 00:07 | . 24 | 1.229700 | 1.477631 | 0.560189 | 4.382551 | 00:07 | . 25 | 1.186633 | 1.459370 | 0.562956 | 4.303250 | 00:07 | . 26 | 1.139820 | 1.467342 | 0.564225 | 4.337692 | 00:07 | . 27 | 1.092420 | 1.476885 | 0.566960 | 4.379283 | 00:07 | . 28 | 1.050866 | 1.486061 | 0.567350 | 4.419652 | 00:07 | . 29 | 1.006091 | 1.505293 | 0.568506 | 4.505473 | 00:07 | . 30 | 0.957875 | 1.528569 | 0.567106 | 4.611572 | 00:07 | . No improvement since epoch 25: early stopping . Text generation . def expand_dim1(x): if len(x.shape) == 1: return x[None, :] else: return x def top_p_filter(logits, top_p=0.9): sorted_logits, sorted_indices = torch.sort(logits, descending=True) cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) sorted_indices_to_remove = cum_probs &gt; top_p sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove) logits[indices_to_remove] = float(&#39;-inf&#39;) return logits @torch.no_grad() def generate(model, inp, max_len=50, temperature=1., top_k = 20, top_p = 0.9, early_stopping=False, #need eos_idx to work eos_idx=None): model.to(inp.device) #TODO test for potential problems model.eval() thresh = top_p inp = expand_dim1(inp) b, t = inp.shape out = inp for _ in range(max_len): x = out logits = model(x)[:, -1, :] filtered_logits = top_p_filter(logits) probs = F.softmax(filtered_logits / temperature, dim=-1) sample = torch.multinomial(probs, 1) out = torch.cat((out, sample), dim=-1) if early_stopping and (sample == eos_idx).all(): break return out . . out = generate(learn.model, tok(&#39;Alice said &#39;), max_len=200, early_stopping=True, eos_idx=tok.c2i[&#39;xxeos&#39;]) . print(tok.decode(out[0])) . Alice said in a minute turn, only purind his with it migut at in musible. i cant elp out my why yested it to like thought: i know i did it wish indeed it hope? . Pretraining on larger dataset . dataset = load_dataset(&quot;bookcorpus&quot;, split=&#39;train&#39;) . Downloading and preparing dataset bookcorpus/plain_text (download: 1.10 GiB, generated: 4.52 GiB, post-processed: Unknown size, total: 5.62 GiB) to /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/af844be26c089fb64810e9f2cd841954fd8bd596d6ddd26326e4c70e2b8c96fc... Dataset bookcorpus downloaded and prepared to /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/af844be26c089fb64810e9f2cd841954fd8bd596d6ddd26326e4c70e2b8c96fc. Subsequent calls will reuse this data. . df = pd.DataFrame(dataset[:10_000_000]) df.head() . text . 0 the half-ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved . | . 1 isbn : 1492913731 isbn-13 : 978-1492913733 for my family , who encouraged me to never stop fighting for my dreams chapter 1 summer vacations supposed to be fun , right ? | . 2 i wish i had a better answer to that question . | . 3 starlings , new york is not the place youd expect much to happen . | . 4 its a small quiet town , the kind where everyone knows your name . | . df[&#39;len&#39;] = df[&#39;text&#39;].str.len() . cut = int(len(df)*0.8) splits = range_of(df)[:cut], range_of(df[cut:]) tfms = Pipeline([ColReader(&#39;text&#39;), tok]) dsets = Datasets(df, tfms=tfms, dl_type=LMDataLoader, splits=splits) . @patch def create_item(self:LMDataLoader, seq): if seq&gt;=self.n: raise IndexError sl = self.last_len if seq//self.bs==self.n_batches-1 else self.seq_len st = (seq%self.bs)*self.bl + (seq//self.bs)*self.seq_len txt = self.chunks[st : st+sl+1] return LMTensorText(txt[:-1]),txt[1:] . %%time dl_kwargs = [{&#39;lens&#39;:df[&#39;len&#39;].values[splits[0]]}, {&#39;val_lens&#39;:df[&#39;len&#39;].values[splits[1]]}] dls = dsets.dataloaders(bs=32, seq_len=512, dl_kwargs=dl_kwargs, shuffle_train=True, num_workers=2) . CPU times: user 13min 29s, sys: 13 s, total: 13min 42s Wall time: 13min 35s . dls.show_batch(max_n=2) . text text_ . 0 im sorry .`` ah , this is katrice , the rowan queen , coming toward us . &#39;&#39;i said ill tell you !but she still did n&#39;t understand why he was a slave here .the sprites had come to believe by now that they were a forgotten people .thats what happened .well have to take him with us for a ways and then let him go .nick stifled the fear in him that was trying to take over .crouching down behind the stone balusters , with every nerve tingling , valeria glared down at the stealthy figure .there did seem to be shado | m sorry .`` ah , this is katrice , the rowan queen , coming toward us . &#39;&#39;i said ill tell you !but she still did n&#39;t understand why he was a slave here .the sprites had come to believe by now that they were a forgotten people .thats what happened .well have to take him with us for a ways and then let him go .nick stifled the fear in him that was trying to take over .crouching down behind the stone balusters , with every nerve tingling , valeria glared down at the stealthy figure .there did seem to be shadow | . 1 habitants were genuine , hard working , proud and tough .sergeant colon &#39;s view of the world was certainly changing .&#39;we did n&#39;t want any part of it , &#39; the grandfather continued , heedless of his company , &#39;but maybe that &#39;s just how the rhega are destined to die ... not by our own hands , our own fights .she looked enquiring , but no one took any notice of her .more land would n&#39;t save his people-they needed something else .the other got off a single shot of his .45 caliber pistol before he was clawed acr | abitants were genuine , hard working , proud and tough .sergeant colon &#39;s view of the world was certainly changing .&#39;we did n&#39;t want any part of it , &#39; the grandfather continued , heedless of his company , &#39;but maybe that &#39;s just how the rhega are destined to die ... not by our own hands , our own fights .she looked enquiring , but no one took any notice of her .more land would n&#39;t save his people-they needed something else .the other got off a single shot of his .45 caliber pistol before he was clawed acro | . model = Model6(tok.vocab_sz, 512, 8, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() learn.lr_find() . SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.5248074531555176) . learn = learn.load(path/&#39;char_bookcorpus_10m&#39;) . &lt;fastai.learner.Learner at 0x7fa5a807e0f0&gt; . learn.fit_one_cycle(1, 1e-4) . epoch train_loss valid_loss accuracy perplexity time . 0 | 1.065701 | 1.064358 | 0.667121 | 2.898976 | 4:23:52 | . learn.save(path/&#39;char_bookcorpus_10m&#39;) . Path(&#39;/content/drive/MyDrive/char_model/char_bookcorpus_10m.pth&#39;) . Finetune on Carrolls&#39; books . sl = 512 seqs = L((tensor(all_nums[i:i+sl]), tensor(all_nums[i+1:i+sl+1])) for i in range(0,len(all_nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], device=&#39;cuda&#39;, bs=16, drop_last=True, shuffle=True) . model = Model6(tok.vocab_sz, 512, 8, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() . learn = learn.load(path/&#39;char_bookcorpus_10m&#39;) learn.lr_find() . SuggestedLRs(lr_min=0.03019951581954956, lr_steep=0.25118863582611084) . learn.fit_one_cycle(10, 1e-4) . epoch train_loss valid_loss accuracy perplexity time . 0 | 1.744969 | 1.472124 | 0.618443 | 4.358481 | 00:08 | . 1 | 1.442924 | 1.128739 | 0.658796 | 3.091756 | 00:08 | . 2 | 1.248169 | 1.068535 | 0.670881 | 2.911111 | 00:08 | . 3 | 1.134621 | 1.048366 | 0.675764 | 2.852986 | 00:08 | . 4 | 1.058302 | 1.044972 | 0.678554 | 2.843319 | 00:08 | . 5 | 1.004934 | 1.036241 | 0.680333 | 2.818603 | 00:08 | . 6 | 0.966509 | 1.042076 | 0.679461 | 2.835096 | 00:08 | . 7 | 0.938987 | 1.040590 | 0.680507 | 2.830886 | 00:08 | . 8 | 0.920266 | 1.035050 | 0.681728 | 2.815248 | 00:08 | . 9 | 0.907935 | 1.037062 | 0.681257 | 2.820917 | 00:08 | . As you see pretraining model on large corpus followed by finetuning helped to reduce validation loss from arount 1.53 to 1.037 and improve accuracy in predicting next character to 68% (compared to 56.7% before). Let&#39;s see how it effects sampled text: . out = generate(learn.model, tok(&#39;Alice said &#39;), max_len=200, early_stopping=True, eos_idx=tok.c2i[&#39;xxeos&#39;]) . print(tok.decode(out[0])) . . Alice said what you want is, why, if you cant see a little rather from people to bed their moment, when birds began drinking from behind, and offering the cart to say something, and dripping off a strange mou .",
            "url": "https://arampacha.github.io/thoughtsamples/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html",
            "relUrl": "/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html",
            "date": " • Jan 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://arampacha.github.io/thoughtsamples/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://arampacha.github.io/thoughtsamples/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}