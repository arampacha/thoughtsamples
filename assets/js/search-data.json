{
  
    
        "post0": {
            "title": "Self-training for ASR with HuggingFace",
            "content": "Art by dalle-mini . Performance of automatic speech recognition (ASR) models had increased substantially in past few years. One of the driving factors is availability of pre-trained large-scale speech representation models. Recently XLS-R model was released. It is pre-trained on large cross-lingual dataset which covers 128 different languages. When fine-tuned on medium-size labeled dataset model provides good performance on speech recognition task. Unfortunately the performance on low resource languages might still be mediocre. To improve the performance in low data regime self-training is often employed. Noizy-student[ref] is popular self-training method. Iterative pseudo-labeling is particularly well suited for ASR. One can fuse audio model with language model to improve the speech recognition performance. Language model not only provides a mean to improve quality of pseudo-labels but also the LM fusion scores can be used to filter pseudo labeled data to select most confident samples. In this notebook I employ simple heuristic for filtering the data introduced in Improved Noisy Student Training for Automatic Speech Recognition. . The procedure can be summarized as follows: . Train Wav2Vec2-xls-r model on available labeled data | Train language model | Tune hyperparameters for filtering on dev data | Generate pseudo-labels for unlabeled data | Filter generated labels using LM scores | Re-train model using mixture of labeled and pseudo-labeled data | Steps 3-6 can be repeated multiple times. The filtering threshold can be relaxed for each iteration. . In this notebook I&#39;ll explore the application of the Noizy Student training procedure for speech recognition on Armenian language. Mozilla Common Voice dataset provides limited amount of high quality audio-transcript pairs. VoxLingua107 dataset has fair amount of untranscribed audio data for 107 languages including 66 hours of Armenian speech. . I&#39;m using pyctcdecode library for integrating LM to ASR system. For detailed introduction to training KenLM and using it with HuggingFace refer to this exelent blogpost. . import os import glob import numpy as np import matplotlib.pyplot as plt import torch import torch.nn.functional as F from torch.utils.data import DataLoader from tqdm.auto import tqdm from transformers import ( Wav2Vec2ProcessorWithLM, Wav2Vec2Processor, Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC, Trainer, TrainingArguments, ) from datasets import ( load_dataset, load_metric, Audio, concatenate_datasets, DatasetDict, Dataset, load_from_disk ) import bitsandbytes as bnb . . I&#39;m using WandB for experiment tracking. Let&#39;s set some environment variables to prepare the experiment. . import wandb %env WANDB_ENTITY = arampacha wandb_entity = os.environ[&quot;WANDB_ENTITY&quot;] %env WANDB_PROJECT = xlsr-hy wandb_project = os.environ[&quot;WANDB_PROJECT&quot;] %env WANDB_LOG_MODEL = false %env WANDB_WATCH = false . . env: WANDB_ENTITY=arampacha env: WANDB_PROJECT=xlsr-hy env: WANDB_LOG_MODEL=false env: WANDB_WATCH=false . import torch from dataclasses import dataclass, field from typing import Any, Dict, List, Optional, Union @dataclass class DataCollatorCTCWithPadding: &quot;&quot;&quot; Data collator that will dynamically pad the inputs received. Args: processor (:class:`~transformers.Wav2Vec2Processor`) The processor used for proccessing the data. padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`): Select a strategy to pad the returned sequences (according to the model&#39;s padding side and padding index) among: * :obj:`True` or :obj:`&#39;longest&#39;`: Pad to the longest sequence in the batch (or no padding if only a single sequence if provided). * :obj:`&#39;max_length&#39;`: Pad to a maximum length specified with the argument :obj:`max_length` or to the maximum acceptable input length for the model if that argument is not provided. * :obj:`False` or :obj:`&#39;do_not_pad&#39;` (default): No padding (i.e., can output a batch with sequences of different lengths). &quot;&quot;&quot; processor: Wav2Vec2Processor padding: Union[bool, str] = True def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -&gt; Dict[str, torch.Tensor]: # split inputs and labels since they have to be of different lenghts and need # different padding methods input_features = [{&quot;input_values&quot;: feature[&quot;input_values&quot;]} for feature in features] batch = self.processor.pad( input_features, padding=self.padding, return_tensors=&quot;pt&quot;, ) if &quot;labels&quot; in features[0].keys(): label_features = [{&quot;input_ids&quot;: feature[&quot;labels&quot;]} for feature in features] with self.processor.as_target_processor(): labels_batch = self.processor.pad( label_features, padding=self.padding, return_tensors=&quot;pt&quot;, ) # replace padding with -100 to ignore loss correctly labels = labels_batch[&quot;input_ids&quot;].masked_fill(labels_batch.attention_mask.ne(1), -100) batch[&quot;labels&quot;] = labels return batch . . 1. Train model on labeled data . The first we need a model trained on labeled data. Common Voice dataset has 738 training samples for Armenian and I haven&#39;t found any additional labeled data publicly available. The model pre-trained on Common Voice is available here. It achieves 22.6 word error rate (WER) with LM boosted decoding. . Notice that large ASR models like those of Wav2Vec2-XLS-R family can learn grammar in some extent. At some point during training the model can start to overfit to train set vocab. The valid loss stops decreasing while validation WER still improves. In my experience validation loss is better indicator of performance after LM fusion. . model_dir = &quot;wav2vec2-xls-r-300m-hy-ns&quot; lang_id = &quot;hy-AM&quot; repo_name = &quot;wav2vec2-xls-r-300m-hy-ns&quot; . @torch.no_grad() def predict(model, dataset, bs=32, device=&quot;cpu&quot;): model.eval() model.to(device) loader = DataLoader( dataset, batch_size=bs, collate_fn=data_collator, shuffle=False, drop_last=False, num_workers=4 ) all_logits = [] for batch in tqdm(loader): batch = {k:v.to(device) for k,v in batch.items()} logits = model(**batch).logits.cpu() all_logits.append(logits) lens = [logits.shape[1] for logits in all_logits] max_len = max(lens) all_logits = [F.pad(logits, (0, 0, 0, max_len-l), value=-100.) for logits, l in zip(all_logits, lens)] return torch.cat(all_logits) . . common_voice_train = load_dataset(&quot;mozilla-foundation/common_voice_8_0&quot;, lang_id, split=&quot;train+validation&quot;, use_auth_token=True) common_voice_test = load_dataset(&quot;mozilla-foundation/common_voice_8_0&quot;, lang_id, split=&quot;test&quot;, use_auth_token=True) common_voice_train = common_voice_train.remove_columns([&quot;accent&quot;, &quot;age&quot;, &quot;gender&quot;, &quot;client_id&quot;, &quot;down_votes&quot;, &quot;locale&quot;, &quot;segment&quot;, &quot;up_votes&quot;]) common_voice_test = common_voice_test.remove_columns([&quot;accent&quot;, &quot;age&quot;, &quot;gender&quot;, &quot;client_id&quot;, &quot;down_votes&quot;, &quot;locale&quot;, &quot;segment&quot;, &quot;up_votes&quot;]) common_voice_train = common_voice_train.cast_column(&quot;audio&quot;, Audio(sampling_rate=16_000)) common_voice_test = common_voice_test.cast_column(&quot;audio&quot;, Audio(sampling_rate=16_000)) print(len(common_voice_train), len(common_voice_test)) . def extract_all_chars(batch): all_text = &quot; &quot;.join(batch[&quot;sentence&quot;]) vocab = list(set(all_text)) return {&quot;vocab&quot;: [vocab], &quot;all_text&quot;: [all_text]} . . import re chars_to_remove_regex = &#39;[ , ? . ! - ; : &quot; ‚Äú % ‚Äò ‚Äù ÔøΩ &#39;¬´¬ª ( )÷â’ù’û’õ’ö]&#39; def normalize_text(batch): text = batch[&quot;sentence&quot;] text = re.sub(chars_to_remove_regex, &#39;&#39;, text.lower())+&quot; &quot; return {&quot;sentence&quot;:text} . common_voice_train = common_voice_train.map(normalize_text) common_voice_test = common_voice_test.map(normalize_text) . Now the transcripts should be ready. Let&#39;s check out some samples and verify train and test split vocabularies match. . test_transcripts = common_voice_test[&quot;sentence&quot;] test_transcripts[:5] . [&#39;’æ’°’ø’´’Ø’°’∂’´ ’©’°’∂’£’°÷Ä’°’∂’∂’•÷Ä’® ’∞’°’¥’°÷Ä’æ’∏÷Ç’¥ ’•’∂ ’°’∑’≠’°÷Ä’∞’∏÷Ç’¥ ’°’¥’•’∂’°’¥’•’Æ ’©’°’∂’£’°÷Ä’°’∂’∂’•÷Ä’´÷Å ’¥’•’Ø’® &#39;, &#39;’¨’∏’µ’°’∂’® ’π’´’∂’°’Ø’°’∂ ’∞’∂’°’£’∏÷Ç’µ’∂ ’¢’∂’°’Ø’°’æ’°’µ÷Ä’•÷Ä’´÷Å ’ß &#39;, &#39;’Ø’°’∂ ’∂’°÷á ’Ω’°’∂÷Ä’•÷Ä ’Ø’•’∂’§’°’∂’´’∂’•÷Ä’´ ’¢÷Ä’§’´ ’∞’°’¥’°÷Ä &#39;, &#39;’°’µ’§’∫’•’Ω ’Ø’°’µ’°÷Å’•’¨ ’ß ’∂÷Ä’°’∂÷Å ’Æ’°’∂’∏’©’∏÷Ç’©’µ’∏÷Ç’∂’® &#39;, &#39;’∏÷Ä’∏’∑’∏÷Ç’¥’® ’•’∂’©’°÷Ä’Ø’æ’•’¨ ’ß ÷Ñ’∂’∂’°’§’°’ø’∏÷Ç’©’µ’°’∂ &#39;] . vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names) vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names) all_chars_train = sorted(vocab_train[&quot;vocab&quot;][0]) all_chars_test = sorted(vocab_test[&quot;vocab&quot;][0]) print(&quot;&quot;.join(all_chars_train)) print(&quot;&quot;.join(all_chars_test)) . . ’°’¢’£’§’•’¶’ß’®’©’™’´’¨’≠’Æ’Ø’∞’±’≤’≥’¥’µ’∂’∑’∏’π’∫’ª’º’Ω’æ’ø÷Ä÷Å÷Ç÷É÷Ñ÷Ö÷Ü÷á ’°’¢’£’§’•’¶’ß’®’©’™’´’¨’≠’Æ’Ø’∞’±’≤’≥’¥’µ’∂’∑’∏’π’∫’ª’º’Ω’æ’ø÷Ä÷Å÷Ç÷É÷Ñ÷Ö÷Ü÷á . Armenian alphabet consists of 39 distinct characters. The vocabulary also includes whitespace and two special tokens - [PAD] and [UNK]. . vocab_dict = {v: k for k, v in enumerate(sorted(vocab_train[&quot;vocab&quot;][0]))} vocab_dict[&quot;|&quot;] = vocab_dict[&quot; &quot;] del vocab_dict[&quot; &quot;] vocab_dict[&quot;[UNK]&quot;] = len(vocab_dict) vocab_dict[&quot;[PAD]&quot;] = len(vocab_dict) len(vocab_dict) . 42 . if not os.path.isfile(f&quot;{model_dir}/vocab.json&quot;): import json with open(f&quot;{model_dir}/vocab.json&quot;, &quot;w&quot;) as f: json.dump(vocab_dict, f) . . tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_dir, unk_token=&quot;[UNK]&quot;, pad_token=&quot;[PAD]&quot;, word_delimiter_token=&quot;|&quot;) feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True) processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer) . file wav2vec2-xls-r-300m-hy-ns/config.json not found Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. . def prepare_dataset(batch): audio = batch[&quot;audio&quot;] # batched output is &quot;un-batched&quot; batch[&quot;input_values&quot;] = processor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_values[0] batch[&quot;length&quot;] = len(batch[&quot;input_values&quot;]) with processor.as_target_processor(): batch[&quot;labels&quot;] = processor(batch[&quot;sentence&quot;]).input_ids return batch . common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_test.column_names) common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names) . For measuring the model performance word error rate (WER) and character error rate (CER) metrics are used. Both metrics are available through datasets library. . wer_metric = load_metric(&quot;wer&quot;) cer_metric = load_metric(&quot;cer&quot;) def compute_metrics(pred): pred_logits = pred.predictions pred_ids = np.argmax(pred_logits, axis=-1) pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id pred_str = processor.batch_decode(pred_ids) label_str = processor.batch_decode(pred.label_ids, group_tokens=False) wer = wer_metric.compute(predictions=pred_str, references=label_str) cer = cer_metric.compute(predictions=pred_str, references=label_str) return {&quot;wer&quot;: wer, &quot;cer&quot;:cer} . . data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True) . We start from Wav2Vec2-XLS-R-300m pretrained checkpoint by Meta AI. . model = Wav2Vec2ForCTC.from_pretrained( &quot;facebook/wav2vec2-xls-r-300m&quot;, attention_dropout=0.0, hidden_dropout=0.1, feat_proj_dropout=0.0, mask_time_prob=0.75, mask_feature_prob=0.25, mask_feature_length=64, layerdrop=0.05, ctc_loss_reduction=&quot;mean&quot;, pad_token_id=processor.tokenizer.pad_token_id, vocab_size=len(processor.tokenizer), ) model.freeze_feature_encoder(); . For training I&#39;m using bitsandbytes 8bit optimizer. It&#39;s designed to reduce memory usage and allows to fit with larger batch size. Tri-stage LR schedule was used for fine-tuning by the authors of the original paper. I train for total of 1600 steps, which is enough for validation loss to reach its minimum for dataset of this size. . from torch.optim.lr_scheduler import LambdaLR def get_tri_stage_schedule( optimizer, num_training_steps, ratios=[0.1, 0.4, 0.5], num_warmup_steps=None, num_hold_steps=None, start_ratio=0.01, end_ratio=0.05 ): assert (num_warmup_steps is None) == (num_hold_steps is None) if num_warmup_steps is None: num_warmup_steps = int(ratios[0]*num_training_steps) num_hold_steps = int(ratios[1]*num_training_steps) start_decay_step = num_warmup_steps + num_hold_steps a_w, b_w = (1-start_ratio)/num_warmup_steps, start_ratio num_decay_steps = num_training_steps - start_decay_step a_d, b_d = (end_ratio-1)/num_decay_steps, 1. def lr_lambda(current_step): if current_step &lt; num_warmup_steps: return a_w * float(current_step) + b_w if current_step &lt; start_decay_step: return 1. return max(end_ratio, a_d * float(current_step - start_decay_step) + b_d ) return LambdaLR(optimizer, lr_lambda) . . num_training_steps = 1600 optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=8e-5, betas=(0.9, 0.98), eps=1e-8, weight_decay=0.) scheduler = get_tri_stage_schedule(optimizer, num_training_steps) . training_args = TrainingArguments( output_dir=model_dir, group_by_length=True, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=4, dataloader_num_workers=8, evaluation_strategy=&quot;steps&quot;, max_steps=num_training_steps, gradient_checkpointing=True, fp16=True, save_steps=200, eval_steps=200, logging_steps=200, learning_rate=8e-5, save_total_limit=4, push_to_hub=False, run_name=&quot;xlsr-300m-hy-demo-1&quot;, report_to=&quot;wandb&quot;, load_best_model_at_end=True, ) . trainer = Trainer( model=model, data_collator=data_collator, args=training_args, compute_metrics=compute_metrics, train_dataset=common_voice_train, eval_dataset=common_voice_test, tokenizer=processor.feature_extractor, optimizers=(optimizer, scheduler) ) . output = trainer.train() . The following columns in the training set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running training ***** Num examples = 728 Num Epochs = 320 Instantaneous batch size per device = 32 Total train batch size (w. parallel, distributed &amp; accumulation) = 128 Gradient Accumulation steps = 4 Total optimization steps = 1600 Automatic Weights &amp; Biases logging enabled, to disable set os.environ[&#34;WANDB_DISABLED&#34;] = &#34;true&#34; wandb: Currently logged in as: arampacha (use `wandb login --relogin` to force relogin) wandb: wandb version 0.12.10 is available! To upgrade, please run: wandb: $ pip install wandb --upgrade . Syncing run xlsr-300m-hy-demo-1 to Weights &amp; Biases (docs). . [1600/1600 2:25:42, Epoch 319/320] Step Training Loss Validation Loss Wer Cer . 200 | 7.868500 | 3.194159 | 1.000000 | 1.000000 | . 400 | 3.544700 | 2.603772 | 1.000000 | 0.974001 | . 600 | 2.442200 | 0.868212 | 0.820453 | 0.214062 | . 800 | 1.605500 | 0.610635 | 0.697112 | 0.158928 | . 1000 | 1.327300 | 0.550194 | 0.654957 | 0.143247 | . 1200 | 1.204300 | 0.556630 | 0.623341 | 0.135508 | . 1400 | 1.109700 | 0.552339 | 0.606557 | 0.133434 | . 1600 | 1.067400 | 0.545073 | 0.597580 | 0.130653 | . &lt;/div&gt; &lt;/div&gt; The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-200 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-200/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-200/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-200/preprocessor_config.json The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-400 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-400/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-400/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-400/preprocessor_config.json The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-600 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-600/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-600/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-600/preprocessor_config.json The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-800 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-800/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-800/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-800/preprocessor_config.json The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-1000 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1000/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1000/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1000/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-200] due to args.save_total_limit The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-1200 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1200/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1200/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1200/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-400] due to args.save_total_limit The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-1400 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1400/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1400/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1400/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-600] due to args.save_total_limit The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-1600 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1600/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1600/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1600/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-800] due to args.save_total_limit Training completed. Do not forget to share your model on huggingface.co/models =) Loading best model from wav2vec2-xls-r-300m-hy-ns/checkpoint-1600 (score: 0.545072615146637). . TrainOutput(global_step=1600, training_loss=2.521186532974243, metrics={&#39;train_runtime&#39;: 8754.7736, &#39;train_samples_per_second&#39;: 23.393, &#39;train_steps_per_second&#39;: 0.183, &#39;total_flos&#39;: 5.069586580567806e+19, &#39;train_loss&#39;: 2.521186532974243, &#39;epoch&#39;: 319.87}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; trainer.save_model() wandb.finish() . 2. LM boosted inference . You can find a tutorial on how to train KenLM and use it for boosting ASR system in this blogpost. Here I&#39;ll use pre-trained 5-gram KenLM for Armenian which I created before. . processor_lm = Wav2Vec2ProcessorWithLM.from_pretrained(&quot;/workspace/output/hy/models/wav2vec2-xls-r-1b-hy/&quot;) . 3. Tune LM hyperparameters on dev data . preds = trainer.predict(common_voice_test) logits = preds.predictions . The following columns in the test set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Prediction ***** Num examples = 335 Batch size = 32 . . [11/11 1:35:23] logits.shape . (335, 492, 44) . Let&#39;s run greed-search for LM fusion hyperparameters. . lm_params = {&quot;alpha&quot;:0.5, &quot;beta&quot;:1.5} best_wer = 1. print(f&quot;alpha t| beta t| wer t t| cer&quot;) for alpha in [0.5, 0.6, 0.7]: for beta in [1.5, 2., 2.5, 3.,]: processor_lm.decoder.reset_params(alpha=alpha, beta=beta) decoded_preds = processor_lm.batch_decode(logits, beam_width=100, num_processes=8) wer = wer_metric.compute(predictions=decoded_preds.text, references=test_transcripts) cer = cer_metric.compute(predictions=decoded_preds.text, references=test_transcripts) if wer &lt; best_wer: lm_params[&quot;alpha&quot;] = alpha lm_params[&quot;beta&quot;] = beta best_wer = wer best_decoded_preds = decoded_preds print(f&quot;{alpha} t| {beta} t| {wer:.4f} t| {cer:.4f}&quot;) print(&quot; nBest LM parameters: &quot;, lm_params) . alpha | beta | wer | cer 0.5 | 1.5 | 0.2920 | 0.0711 0.5 | 2.0 | 0.2923 | 0.0712 0.5 | 2.5 | 0.2939 | 0.0716 0.5 | 3.0 | 0.2966 | 0.0715 0.6 | 1.5 | 0.2873 | 0.0720 0.6 | 2.0 | 0.2884 | 0.0712 0.6 | 2.5 | 0.2881 | 0.0706 0.6 | 3.0 | 0.2916 | 0.0708 0.7 | 1.5 | 0.2877 | 0.0732 0.7 | 2.0 | 0.2842 | 0.0723 0.7 | 2.5 | 0.2845 | 0.0719 0.7 | 3.0 | 0.2842 | 0.0714 Best LM parameters: {&#39;alpha&#39;: 0.7, &#39;beta&#39;: 2.0} . processor_lm.decoder.reset_params(**lm_params) decoded_preds = best_decoded_preds . import random ids = random.sample(range(len(common_voice_test)), k=5) for i in ids: print(f&quot;Logit score: {decoded_preds.logit_score[i]:.4f}, LM score: {decoded_preds.lm_score[i]:.4f}&quot;) print(decoded_preds.text[i]) print(test_transcripts[i]) . -15.00629891494811 -36.10539751911238 ’æ’•÷Ä’ª’´’∂ ’°’∂’£’°’¥ ’¢’°’¶’¥’°’©’´’æ ’±’•’º’°’£÷Ä’•÷Ä ’∏’π’∂’π’°÷Å’°’∂ ’°’º’°’ª’´’∂ ’∞’°’¥’°’∑’≠’°÷Ä’∞’°’µ’´’∂ ’∫’°’ø’•÷Ä’°’¶’¥’´ ’ø’°÷Ä’´’∂’•÷Ä’´’∂ ’æ’•÷Ä’ª’´’∂ ’°’∂’£’°’¥ ’¢’°’¶’¥’°’©’´’æ ’±’•’º’°’£÷Ä’•÷Ä ’∏’π’∂’π’°÷Å’°’∂ ’°’º’°’ª’´’∂ ’∞’°’¥’°’∑’≠’°÷Ä’∞’°’µ’´’∂ ’∫’°’ø’•÷Ä’°’¶’¥’´ ’ø’°÷Ä’´’∂’•÷Ä’´’∂ -18.88247006201864 -21.816377967257626 ’¥’ø÷Ñ’´ ’≥’Ø’∏÷Ç’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’Ω’ø’•’≤’Æ’°’£’∏÷Ä’Æ’°’Ø’°’∂ ’®’∂’§’∏÷Ç’∂’°’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’°’º’Ø’°’µ’∏÷Ç’©’µ’°’∂ ’§÷Ä’Ω÷á’∏÷Ä’∏÷Ç’¥’∂’•÷Ä’´÷Å ’¥’•’Ø’∂ ’ß ’¥’ø÷Ñ’´ ’≥’Ø’∏÷Ç’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’Ω’ø’•’≤’Æ’°’£’∏÷Ä’Æ’°’Ø’°’∂ ’®’∂’§’∏÷Ç’∂’°’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ’°’º’Ø’°’µ’∏÷Ç’©’µ’°’∂ ’§÷Ä’Ω÷á’∏÷Ä’∏÷Ç’¥’∂’•÷Ä’´÷Å ’¥’•’Ø’∂ ’ß -20.694962102139808 -71.93717848657455 ’£’•’¨’® ’´÷Ä ’Ω’ø’•’≤’Æ’°’Æ ’Ø’•÷Ä’∫’°÷Ä’´ ’∂’°’∂’æ’°’∂’∏÷Ç’¥ ’ß ’π’°÷Ä’°’≥’≥’´ ÷Ö’∫’•÷Ä’°’µ’´’∂ ’§’•’∂’§’´ ÷Ñ’∏÷Ç’•’µ’¨’® ’´÷Ä ’Ω’ø’•’≤’Æ’°’Æ ’Ø’•÷Ä’∫’°÷Ä’´’∂ ’°’∂’æ’°’∂’∏÷Ç’¥ ’ß ’π’°÷Ä’°’≥’≥’´ ÷Ö’∫’•÷Ä’°’µ’´’∂ ’§’•’∂’§’´ -21.81080996648263 -34.990664297362855 ÷Ñ’∂’∂’°÷Ä’Ø’•’∂÷Ñ ’´’∂’π’∏÷Ç ’ß ’≥’´’∑’ø ’æ’•÷Ä’ª’´’∂ ’∞’°’ø’Ø’∏÷Ç’©’µ’∏÷Ç’∂’® ÷Ñ’∂’∂’°÷Ä’Ø’•’∂÷Ñ ’©’• ’´’∂’π’∏÷Ç ’ß ’≥’´’∑’ø ’æ’•÷Ä’ª’´’∂ ’∞’°’ø’Ø’∏÷Ç’©’µ’∏÷Ç’∂’® -15.957373845367895 -72.43214053572748 ’™’∏’≤’∏’æ÷Ä’§’°’Ø’°’∂ ’§’∫÷Ä’∏÷Å ’∂’°’æ’°÷Ä’§’•’¨’∏÷Ç÷Å’∞’•’ø’∏÷Ç ’°’∑’≠’°’ø’•’¨ ’ß ’¢÷Ä’°’∂’§’•’∂’¢’∏÷Ç÷Ä’£’µ’°’∂ ’∞’°÷Å’´ ’≠’°’∂’∏÷Ç’©’∏÷Ç’¥ ’™’∏’≤’∏’æ÷Ä’§’°’Ø’°’∂ ’§’∫÷Ä’∏÷Å’∂ ’°’æ’°÷Ä’ø’•’¨’∏÷Ç÷Å ’∞’•’ø’∏ ’°’∑’≠’°’ø’•’¨ ’ß ’¢÷Ä’°’∂’§’•÷Ä’¢’∏÷Ç÷Ä’£’µ’°’∂ ’∞’°÷Å’´ ’≠’°’∂’∏÷Ç’©’∏÷Ç’¥ . pyctcdecode does beam search for decoding. Candidate sequences are selected by the negative log likelihood. Total score is composed of NLL assigned to token sequence by the model and NLL score from language model. Negative log likelihood of sequence is a sum of NLLs of individual elements. Therefore the score returned by decoder is not suited well for filtering the generated labels: longer sequences are expected to have lower score. To adjust the scores for filtering I&#39;m adopting an approach from Improved Noisy Student Training for Automatic Speech Recognition. $$ score = (s_{LM} - alpha*l - beta)/ ( sigma sqrt l) $$ . where $s_{LM}$ - fused LM score, $l$ - length, $ alpha, beta$ - tuned hyperparameters, $ sigma$ - standard deviation of adjusted score. . text_length = np.array([len(s) for s in decoded_preds.text]) lm_scores = np.array(decoded_preds.lm_score) . wer_metric.compute(predictions=decoded_preds.text, references=test_transcripts) . 0.28415300546448086 . cer_metric.compute(predictions=decoded_preds.text, references=test_transcripts) . 0.07228123419322205 . example_wer = np.array([wer_metric.compute(predictions=[pred], references=[ref]) for pred, ref in zip(decoded_preds.text, test_transcripts)]) example_wer.mean() . 0.28854536839611467 . It&#39;s easy to fit a line to the data. For instance one can scikit-learn library for this purpose. . from sklearn.linear_model import LinearRegression reg = LinearRegression() reg.fit(np.expand_dims(np.array(text_length), axis=1), np.array(decoded_preds.lm_score)) a = reg.coef_[0] b = reg.intercept_ print(f&quot;{a:.7f}, {b:.7f}&quot;) . -0.6229814, -18.6591776 . fig, ax = plt.subplots() ax.scatter(text_length, decoded_preds.lm_score) x = np.array([20, 100]) ax.plot(x, a*x+b, &quot;r&quot;) plt.show() . scores = (lm_scores - a*text_length - b)/ np.sqrt(text_length) std = scores.std() print(std) . 3.8947836733862813 . scores = scores/ scores.std() . Finally we can check how the filtering reflects on the label quality. . mask = scores &gt; .5 print(f&quot;Selected {mask.sum()}/{len(mask)} examples after filtering with average WER {example_wer[mask].mean():.5f}&quot;) . Selected 114/335 examples after filtering with average WER 0.06044 . The examples with score higher then 0.5 have average 6.044 WER which is much better compared to 28.4 for the whole dataset. Hopefully filtering weak labels will allow to select high quality labels as well. . 4. Generate pseudo-labeled data . For dataset generation I&#39;m using VoxLingua107 dataset. . . unsup_dataset = load_dataset(&quot;voxlingua107.py&quot;, lang_id.split(&quot;-&quot;)[0], split=&quot;train&quot;) len(unsup_dataset) . Reusing dataset vox_lingua107 (/workspace/cache/hf/datasets/vox_lingua107/hy/0.0.0/92f9b48de9b7250925c8d895423b1e5e809f341da44e36e62b481c87fe6a1ca4) . 24868 . from datasets import load_dataset, Dataset, Audio, load_from_disk . sampling_rate = 16_000 unsup_dataset = unsup_dataset.cast_column(&quot;audio&quot;, Audio(sampling_rate)) . unsup_dataset[0] . {&#39;path&#39;: &#39;/workspace/cache/hf/datasets/downloads/extracted/e2c69883717bc530ea985b078278eca672e297147dc25dbe8b1c2e751186f304/hy/PtZMJZrea3w__U__S10292.510-0301.900.wav&#39;, &#39;audio&#39;: {&#39;path&#39;: &#39;/workspace/cache/hf/datasets/downloads/extracted/e2c69883717bc530ea985b078278eca672e297147dc25dbe8b1c2e751186f304/hy/PtZMJZrea3w__U__S10292.510-0301.900.wav&#39;, &#39;array&#39;: array([-0.00149536, -0.00271606, -0.00183105, ..., -0.00839233, -0.0062561 , -0.00439453]), &#39;sampling_rate&#39;: 16000}} . unsup_dataset = unsup_dataset.map(lambda s: {&quot;duration&quot;: len(s[&quot;audio&quot;][&quot;array&quot;]) / sampling_rate}) . Loading cached processed dataset at /workspace/cache/hf/datasets/vox_lingua107/hy/0.0.0/92f9b48de9b7250925c8d895423b1e5e809f341da44e36e62b481c87fe6a1ca4/cache-7e876b6e58276cea.arrow . durs = np.array(unsup_dataset[&quot;duration&quot;]) durs.mean(), durs.std(), durs.min(), durs.max() . (9.97123931106241, 4.763985759128929, 1.89, 20.0) . filtered_dataset = unsup_dataset.filter(lambda x: (3. &lt; x &lt; 16.), input_columns=&quot;duration&quot;) len(filtered_dataset) . Loading cached processed dataset at /workspace/cache/hf/datasets/vox_lingua107/hy/0.0.0/92f9b48de9b7250925c8d895423b1e5e809f341da44e36e62b481c87fe6a1ca4/cache-96f89da5fecedc13.arrow . 20694 . filtered_dataset = filtered_dataset.sort(&quot;duration&quot;) . Loading cached sorted indices for dataset at /workspace/cache/hf/datasets/vox_lingua107/hy/0.0.0/92f9b48de9b7250925c8d895423b1e5e809f341da44e36e62b481c87fe6a1ca4/cache-b68a45dfe0d0dc6e.arrow . durs = np.array(filtered_dataset[&quot;duration&quot;]) durs.mean(), durs.std(), durs.min(), durs.max() . (8.68453510377404, 3.5985018135481797, 3.01, 15.99) . def preprocess(batch): audio = batch[&quot;audio&quot;] batch[&quot;input_values&quot;] = processor(audio[&quot;array&quot;], sampling_rate=audio[&quot;sampling_rate&quot;]).input_values[0] # batch[&quot;length&quot;] = len(batch[&quot;input_values&quot;]) return batch . processed_dataset = filtered_dataset.map(preprocess, remove_columns=filtered_dataset.column_names, num_proc=5) . . vl_logits = trainer.predict(processed_dataset).predictions vl_logits.shape . ***** Running Prediction ***** Num examples = 20694 Batch size = 32 . (20694, 799, 44) . np.save(&quot;voxlingua_hy_logits_1.npy&quot;, vl_logits) . vl_decoded_preds = processor_lm.batch_decode(vl_logits, beam_width=100, num_processes=8) . vl_decoded_preds.text[:10] . [&#39;’≥’°’∂’°’∫’°÷Ä’∞’´ ’Ø’•’Ω’∂ ’ß&#39;, &#39;’∑’°’¥’• ’£’∂’∏÷Ç’¥ ’ß’´’∂ ’•÷Ä÷á’°’∂&#39;, &#39;’´’∂’π ’æ’•÷Ä’°’¢’•÷Ä’æ’∏÷Ç’¥ ’ß ’°’¥÷Ñ’∏÷Ä ’¥’´’ª’°’¶’£’°’µ’´’∂ ’Ø’°’¶’¥’°’Ø’•÷Ä’∫’∏÷Ç’©’µ’°’∂’®&#39;, &#39;’∞’´’¥’∂’°’Ø’°’∂’∏÷Ç’¥ ’¥’°’µ’Ω’∂’•÷Ä’® ’£’∂’∏÷Ç’¥ ’•’∂ ’Ω’´’¨’´’Ø’∏’∂’µ’°’∂ ’∞’∏’æ’´’ø&#39;, &#39;’ø’°÷Ä’°’Æ’¥’°’∂ ’™’°’¥’°’∂’°’Ø’°’Ø’´÷Å ’∏÷Ç ’°÷Ä’§’µ’∏÷Ç’∂’°’æ’•’ø ’°’µ’ø’•÷Ä’®&#39;, &#39;’´’∂’π’∏÷Ç ’∂’°÷á&#39;, &#39;’º’∏’∫’ß÷Ä ÷Ñ’∏’π’°÷Ä’µ’°’∂’´ ÷É’°’Ω’ø’°’¢’°’∂’∂’•÷Ä’´ ’®’∂’ø÷Ä’°’Æ ’¥’°÷Ä’ø’°’æ’°÷Ä’∏÷Ç’©’µ’°’∂&#39;, &#39;’∞’°’∂’∂’°’£÷Ä’´÷Ñ’∏÷Ä’µ’°’∂ ÷á ÷Ä’£’¥’°’∂’∏÷Ç’Ø’µ’°’∂’°’∑’∏÷Å’°÷Ä÷Ñ’Ω’´’°’∂ ’°’µ’Ω÷Ö÷Ä&#39;, &#39;’ø’°’∑’•’¨ ’°’Ω’´ ’£’ß’°’≠’°’¨’∏÷Ç’¥&#39;, &#39;’°’µ’¨’°’∫’•’Ω ’°’µ’Ω ÷Ñ’°’¥’¢’•÷Ñ ’´÷Ä’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂ ’π’ß÷Ä ’§’°’º’∂’°&#39;] . vl_decoded_preds.lm_score[:10] . [-12.35923369838104, -30.3769261717412, -26.37022658856078, -51.1881242531656, -42.20113835486087, -7.537985312772793, -43.072356623004666, -68.51194965704599, -63.09046945295047, -38.81879677106827] . text_length = np.array([len(s) for s in vl_decoded_preds.text]) lm_scores = np.array(vl_decoded_preds.lm_score) . 5. Filter pseudo-labeled data . import matplotlib.pyplot as plt fig, ax = plt.subplots() ax.scatter(text_length[:1000], lm_scores[:1000]) x = np.array([20, 100]) ax.plot(x, a*x+b, &quot;r&quot;) . [&lt;matplotlib.lines.Line2D at 0x7fce33161a30&gt;] . vl_scores = (lm_scores - a*text_length - b)/ np.sqrt(text_length) / std . mask = vl_scores &gt; .4 print(f&quot;Selected {mask.sum()}/{len(mask)} examples after filtering&quot;) . Selected 553/20694 examples after filtering . len(common_voice_train) . 728 . idx = np.arange(len(mask))[mask] print(&quot;lm_score t|adj_score t| text&quot;) print(&quot;-&quot;*100) for i, _ in zip(idx, range(10)): print(f&quot;{lm_scores[i]:.2f} t t|{vl_scores[i]:.2f} t t|&quot;, vl_decoded_preds.text[i]) . lm_score |adj_score | text - -12.36 |1.04 | ’≥’°’∂’°’∫’°÷Ä’∞’´ ’Ø’•’Ω’∂ ’ß -26.37 |0.87 | ’´’∂’π ’æ’•÷Ä’°’¢’•÷Ä’æ’∏÷Ç’¥ ’ß ’°’¥÷Ñ’∏÷Ä ’¥’´’ª’°’¶’£’°’µ’´’∂ ’Ø’°’¶’¥’°’Ø’•÷Ä’∫’∏÷Ç’©’µ’°’∂’® -7.54 |1.43 | ’´’∂’π’∏÷Ç ’∂’°÷á -29.84 |0.54 | ’ø’°÷Ä’°’Æ’°’∑÷Ä’ª’°’∂’∏÷Ç’¥ ’´÷Ä’°’æ’´’≥’°’Ø’® ’≠’´’Ω’ø ’°’∂’∞’°’∂’£’´’Ω’ø -36.64 |0.46 | ’∫’°’ø’¥’∏÷Ç’©’µ’°’∂ ’®’∂’©’°÷Å÷Ñ’∏÷Ç’¥ ’∞’°’µ’°’Ω’ø’°’∂÷Å’´ ’£’∏÷Ä’Æ’°÷Ä’°÷Ä’∂’•÷Ä’® ’≥÷Ä’°’£ -23.88 |0.69 | ’¥’•÷Ä ’°’º’°’ª’°÷Ä’Ø’∂ ’ß ’¢’∏’¨’∏÷Ä ’∂’•÷Ä’§÷Ä’∏’≤’∂’•÷Ä’´’∂ -38.15 |0.40 | ’∞’∏÷Ç’¶’¥’∏÷Ç’∂÷Ñ’´ ’°÷Ä’ø’°’∞’°’µ’ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’® ’Ø’°’¥ ’∞’°’¶’æ’°’§’•’∫ ’¢’°’º’•÷Ä -36.44 |0.57 | ’∏’æ ÷Ö÷Ä’´’∂’°’Ø’°’∂ ’≥’°’∂’°’∫’°÷Ä’∏’æ ’æ’•÷Ä’°’§’°’º’∂’°’¨ ’º’∏÷Ç’Ω’°’Ω’ø’°’∂’´ ’§’°’∑’∂’∏÷Ç’©’µ’∏÷Ç’∂ -32.75 |0.64 | ’¥’°’Ω’´’∂ ’ß ’∏÷Ä’® ’∞’°’µ’°’Ω’ø’°’∂’´ ’¥’´’ª’°’¶’£’°’µ’´’∂ ’∫’°÷Ä’ø’°’æ’∏÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’∂ ÷á -24.39 |0.71 | ’∞’°’ø’Ø’°’∫’•’Ω ’¥’ø’°’∞’∏’£’´’π ’•’∂ ’°÷Ä’ø’°’£’°’≤’©’´ ’©’æ’•÷Ä’® . labeled_dataset = filtered_dataset.select(idx) . paths = labeled_dataset[&quot;path&quot;] . from datasets import Dataset, DatasetDict labeled_dataset = Dataset.from_dict({&quot;path&quot;:paths, &quot;audio&quot;:paths, &quot;sentence&quot;:[vl_decoded_preds.text[i] for i in idx]}) . labeled_dataset = labeled_dataset.cast_column(&quot;audio&quot;, Audio(sampling_rate)) . labeled_dataset.save_to_disk(&quot;/workspace/data/hy/vox_lingua_hy_labeled_1&quot;) . from datasets import . len(labeled_dataset) . 553 . common_voice_train = load_dataset(&quot;mozilla-foundation/common_voice_8_0&quot;, lang_id, split=&quot;train+validation&quot;, use_auth_token=True) common_voice_test = load_dataset(&quot;mozilla-foundation/common_voice_8_0&quot;, lang_id, split=&quot;test&quot;, use_auth_token=True) common_voice_train = common_voice_train.remove_columns([&quot;accent&quot;, &quot;age&quot;, &quot;gender&quot;, &quot;client_id&quot;, &quot;down_votes&quot;, &quot;locale&quot;, &quot;segment&quot;, &quot;up_votes&quot;]) common_voice_test = common_voice_test.remove_columns([&quot;accent&quot;, &quot;age&quot;, &quot;gender&quot;, &quot;client_id&quot;, &quot;down_votes&quot;, &quot;locale&quot;, &quot;segment&quot;, &quot;up_votes&quot;]) common_voice_train = common_voice_train.cast_column(&quot;audio&quot;, Audio(sampling_rate=16_000)) common_voice_test = common_voice_test.cast_column(&quot;audio&quot;, Audio(sampling_rate=16_000)) common_voice_train = common_voice_train.map(normalize_text) common_voice_test = common_voice_test.map(normalize_text) . train_dataset = concatenate_datasets([common_voice_train, labeled_dataset]) print(&quot;Composed dataset size: &quot;, len(train_dataset)) . 1281 . noizy_student_ds = DatasetDict({&quot;train&quot;:train_dataset, &quot;test&quot;:common_voice_test}) noizy_student_ds.save_to_disk(&quot;./data/hy/noizy_student_1_demo&quot;) . train_dataset = load_from_disk(&quot;/workspace/data/hy/noizy_student_1_demo/train&quot;) test_dataset = load_from_disk(&quot;/workspace/data/hy/noizy_student_1_demo/test&quot;) len(train_dataset), len(test_dataset) . You might also check out some of the selected samples to assess label quality manually. . import IPython.display as ipd import numpy as np import random i = int(random.choice(idx)) print(vl_decoded_preds.text[i]) print(f&quot;lm_score {lm_scores[i]:.2f}| score {vl_scores[i]:.2f}&quot;) ipd.Audio(data=filtered_dataset[i][&quot;audio&quot;][&quot;array&quot;], autoplay=True, rate=16000) . fordi vi netop har v√¶ret vidne til i europa lm_score -13.63| score 0.66 . Your browser does not support the audio element. 6. Re-train model using mixture of labeled and pseudo-labeled data . train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names) test_dataset = test_dataset.map(prepare_dataset, remove_columns=common_voice_test.column_names) . model = Wav2Vec2ForCTC.from_pretrained( &quot;facebook/wav2vec2-xls-r-300m&quot;, attention_dropout=0.0, hidden_dropout=0.1, feat_proj_dropout=0.0, mask_time_prob=0.75, mask_feature_prob=0.25, mask_feature_length=64, layerdrop=0.05, ctc_loss_reduction=&quot;mean&quot;, pad_token_id=processor.tokenizer.pad_token_id, vocab_size=len(processor.tokenizer), ) model.freeze_feature_encoder(); . num_training_steps = 1600 optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=8e-5, betas=(0.9, 0.98), eps=1e-8, weight_decay=0.) scheduler = get_tri_stage_schedule(optimizer, num_training_steps) . training_args = TrainingArguments( output_dir=model_dir, group_by_length=True, per_device_train_batch_size=32, per_device_eval_batch_size=32, gradient_accumulation_steps=4, dataloader_num_workers=8, evaluation_strategy=&quot;steps&quot;, max_steps=num_training_steps, gradient_checkpointing=True, fp16=True, save_steps=200, eval_steps=200, logging_steps=200, learning_rate=8e-5, save_total_limit=4, push_to_hub=False, run_name=&quot;xlsr-300m-hy-demo-2&quot;, report_to=&quot;wandb&quot;, load_best_model_at_end=True, ) . PyTorch: setting up devices . trainer = Trainer( model=model, data_collator=data_collator, args=training_args, compute_metrics=compute_metrics, train_dataset=train_dataset, eval_dataset=test_dataset, tokenizer=processor.feature_extractor, optimizers=(optimizer, scheduler) ) . max_steps is given, it will override any value given in num_train_epochs Using amp half precision backend . trainer.train() . The following columns in the training set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running training ***** Num examples = 1281 Num Epochs = 160 Instantaneous batch size per device = 32 Total train batch size (w. parallel, distributed &amp; accumulation) = 128 Gradient Accumulation steps = 4 Total optimization steps = 1600 Automatic Weights &amp; Biases logging enabled, to disable set os.environ[&#34;WANDB_DISABLED&#34;] = &#34;true&#34; wandb: wandb version 0.12.10 is available! To upgrade, please run: wandb: $ pip install wandb --upgrade . Syncing run xlsr-300m-hy-demo-2 to Weights &amp; Biases (docs). . [1021/1600 1:18:58 &lt; 44:52, 0.22 it/s, Epoch 101.98/160] Step Training Loss Validation Loss Wer Cer . 200 | 6.938900 | 3.195832 | 1.000000 | 1.000000 | . 400 | 3.250400 | 3.141035 | 1.000000 | 1.000000 | . 600 | 2.789400 | 1.334666 | 0.951210 | 0.312089 | . 800 | 1.714000 | 0.573159 | 0.674863 | 0.145928 | . 1000 | 1.330500 | 0.458515 | 0.570258 | 0.117451 | . &lt;/div&gt; &lt;/div&gt; The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-200 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-200/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-200/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-200/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-1000] due to args.save_total_limit The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-400 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-400/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-400/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-400/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-1200] due to args.save_total_limit The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-600 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-600/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-600/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-600/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-1400] due to args.save_total_limit The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-800 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-800/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-800/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-800/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-1600] due to args.save_total_limit The following columns in the evaluation set don&#39;t have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. ***** Running Evaluation ***** Num examples = 335 Batch size = 32 Saving model checkpoint to wav2vec2-xls-r-300m-hy-ns/checkpoint-1000 Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1000/config.json Model weights saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1000/pytorch_model.bin Configuration saved in wav2vec2-xls-r-300m-hy-ns/checkpoint-1000/preprocessor_config.json Deleting older checkpoint [wav2vec2-xls-r-300m-hy-ns/checkpoint-200] due to args.save_total_limit . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; preds = trainer.predict(common_voice_test) logits = preds.predictions . decoded_preds = processor_lm.batch_decode(logits, beam_width=100, num_processes=8) wer = wer_metric.compute(predictions=decoded_preds.text, references=test_transcripts) cer = cer_metric.compute(predictions=decoded_preds.text, references=test_transcripts) print(f&quot;{alpha} t| {beta} t| {wer:.4f} t| {cer:.4f}&quot;) . Concluding thoughts . The 1st iteration results in relative improvement 18.2% and 28.4% for WER and loss respectively. . . . Noisy student procedure can be repeated multiple times for further improvement of the results. A larger 1 billion parameter model trained for 4 iterations can be found here. This model has taken first place at Robust Speech Recognition Challenge hosted by HuggingFace and is (to my knowledge) the best-performing open-source model for Armenian language. I also release Robust Speech Recognition Challenge winning models for Ukrainian and Georgian. . References . XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale - Babu et al., 2021 | Common Voice: A Massively-Multilingual Speech Corpus - Adila et al., 2019 | VoxLingua107: a Dataset for Spoken Language Recognition - Valk &amp; Alum√§e, 2020 | Boosting Wav2Vec2 with n-grams in ü§ó Transformers - Patrick von Platen, 2022 | Improved Noisy Student Training for Automatic Speech Recognition - Park et al., 2020 | &lt;/div&gt; .",
            "url": "https://blog.abstrukt.co/asr/transformers/wav2vec2/self-training/2022/02/18/self-training-for-asr.html",
            "relUrl": "/asr/transformers/wav2vec2/self-training/2022/02/18/self-training-for-asr.html",
            "date": " ‚Ä¢ Feb 18, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Finetuning Transformers on GLUE benchmark",
            "content": "from transformers import AutoModelForSequenceClassification from fastai.text.all import * from fastai.callback.wandb import * from fasthugs.learner import TransLearner from fasthugs.data import TransformersTextBlock, TextGetter, get_splits from datasets import load_dataset, concatenate_datasets import wandb import gc . . Introduction . In this blogpost we will look at how to combine the power of HuggingFace with great flexibility of fastai. For this purpose we will finetune distilroberta-base on The General Language Understanding Evaluation(GLUE) benchmark. GLUE consists of 8 diverse sequence classification and one regression task. . I&#39;ll use fasthugs to make HuggingFace+fastai integration smooth. . Fun fact:GLUE benchmark was introduced in this paper in 2018 as tough to beat benchmark to chellange NLP systems and in just about a year new SuperGLUE benchmark was introduced because original GLUE has become too easy for the models. To give you a grasp on what we are dealing with, here is a brief summary of GLUE tasks: . Name Task description Size Metrics . cola Corpus of Linguistic Acceptability | Determine whether it is a grammatical sentence | 8.5k | matthews_corrcoef | . sst2 Stanford Sentiment Treebank | Predict the sentiment of a givensentence | 67k | accuracy | . mrpc Microsoft Research Paraphrase Corpus | Determine whether the sentences in the pair are semantically equivalent | 3.7k | f1/accuracy | . stsb Semantic Textual Similarity Benchmark | Determine similarity score for 2 sentences | 7k | pearsonr/spearmanr | . qqp Quora question pair | Determine if 2 questions are the same (paraphrase) | 364k | f1/accuracy | . mnli Mulit-Genre Natural Language Inference | Predict whether the premise entails, contradicts or is neutral to the hypothesis | 393k | accuracy | . qnli Stanford Question Answering Dataset | Determine whether the context sentence containsthe answer to the question | 105k | accuracy | . rte Recognize Textual Entailment | Determine whether one sentece entails another | 2.5k | accuracy | . wnli Winograd Schema Challenge | Predict if the sentence with the pronoun substituted is entailed by the original sentence | 634 | accuracy | . As you can see some datasets are really small here. And we&#39;ll look at how one can adress. . Setup . Let&#39;s define main settings for the run in one place: . ds_name = &#39;glue&#39; model_name = &quot;distilroberta-base&quot; max_len = 512 bs = 32 val_bs = bs*2 n_epoch = 4 lr = 2e-5 wd = 0. opt_func = Adam diff_lr_decay_factor = 0 . To make switching between datasets smooth I define couple of dictionaries containing per-task information. We need metrics, text fields to retrieve data and number of outputs for the model. . GLUE_TASKS = [&quot;cola&quot;, &quot;mnli&quot;, &quot;mrpc&quot;, &quot;qnli&quot;, &quot;qqp&quot;, &quot;rte&quot;, &quot;sst2&quot;, &quot;stsb&quot;, &quot;wnli&quot;] def validate_task(): assert task in GLUE_TASKS . glue_metrics = { &#39;cola&#39;:[MatthewsCorrCoef()], &#39;sst2&#39;:[accuracy], &#39;mrpc&#39;:[F1Score(), accuracy], &#39;stsb&#39;:[PearsonCorrCoef(), SpearmanCorrCoef()], &#39;qqp&#39;: [F1Score(), accuracy], &#39;mnli&#39;:[accuracy], &#39;qnli&#39;:[accuracy], &#39;rte&#39;: [accuracy], &#39;wnli&#39;:[accuracy], } glue_textfields = { &#39;cola&#39;:[&#39;sentence&#39;, None], &#39;sst2&#39;:[&#39;sentence&#39;, None], &#39;mrpc&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;stsb&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;qqp&#39;: [&#39;question1&#39;, &#39;question2&#39;], &#39;mnli&#39;:[&#39;premise&#39;, &#39;hypothesis&#39;], &#39;qnli&#39;:[&#39;question&#39;, &#39;sentence&#39;], &#39;rte&#39;: [&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;wnli&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], } glue_num_labels = {&#39;mnli&#39;:3, &#39;stsb&#39;:1} . . def layerwise_splitter(model): emb = L(model.base_model.embeddings) layers = L(model.base_model.encoder.layer.children()) clf = L(m for m in list(model.children())[1:] if params(m)) groups = emb + layers + clf return groups.map(params) . . Running a GLUE task . task = &#39;sst2&#39;; validate_task() ds = load_dataset(ds_name, task) . valid_ = &#39;validation-matched&#39; if task==&#39;mnli&#39; else &#39;validation&#39; len(ds[&#39;train&#39;]), len(ds[valid_]) . (67349, 872) . train_idx, valid_idx = get_splits(ds, valid=valid_) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[valid_]]) train_ds[0] . {&#39;idx&#39;: 0, &#39;label&#39;: 0, &#39;sentence&#39;: &#39;hide new secretions from the parental units &#39;} . Here I use number of characters a proxy for length of tokenized text to speed up dls creation. . lens = train_ds.map(lambda s: {&#39;len&#39;: sum([len(s[i]) for i in glue_textfields[task] if i])}, remove_columns=train_ds.column_names, num_proc=2, keep_in_memory=True) train_lens = lens.select(train_idx)[&#39;len&#39;] valid_lens = lens.select(valid_idx)[&#39;len&#39;] . blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock() if task==&#39;stsb&#39; else CategoryBlock()] dblock = DataBlock(blocks = blocks, get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . dl_kwargs=[{&#39;res&#39;:train_lens}, {&#39;val_res&#39;:valid_lens}] dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs, dl_kwargs=dl_kwargs) dls.show_batch(max_n=4) . text category . 0 ... spiced with humor (&#39;i speak fluent flatula,&#39;advises denlopp after a rather, er, bubbly exchange with an alien deckhand ) and witty updatings ( silver&#39;s parrot has been replaced with morph, a cute alien creature who mimics everyone and everything around ) | 1 | . 1 stopped thinking about how good it all was, and started doing nothing but reacting to it - feeling a part of its grand locations, thinking urgently as the protagonists struggled, feeling at the mercy of its inventiveness, gasping at its visual delights | 1 | . 2 there aren&#39;t too many films that can be as simultaneously funny, offbeat and heartwarming ( without a thick shmear of the goo, at least ), but `` elling &#39;&#39; manages to do all three quite well, making it one of the year&#39;s most enjoyable releases | 1 | . 3 hatfield and hicks make the oddest of couples, and in this sense the movie becomes a study of the gambles of the publishing world, offering a case study that exists apart from all the movie&#39;s political ramifications. | 1 | . Single run . The GLUE benchmark contains 8 tasks and it might be cumbersome to systematize the results. To make the analysis simpler and much more powerful I will be using Weights&amp;Biases tracking platform. And even better thanks to Morgan McGuire (@morg) we have an open W&amp;B project. You just need to log your runs under glue-benchmark project and set entity=&quot;fastai_community&quot; and your results will be added to the pull for further investigation of hyperparameters. The fastest way to start participating would be to fork this notebook as it is set up to run any of the GLUE tasks with minimal changes. There is a lot to try: gradual unfreezing strategy is reported not to be helpful when finetuning Transformer-based models (for example see a discussion here); differential learning rates are used in NLP [1, 2] but are not common practice, do we need to use weight decay, if yes - how much and where, what suggestions from LR-finder work best? These are only few of many open questions and there are so much more. And even more interesting one how do this scale with dataset and model size? . Deep Learning as of now is highly empirical field and experiments require both some engendering and compute. This post is aimed to fuel community effort towards finding empirical truth by joining small forces together. Even if you&#39;re new to NLP do not hesitate to participate and run couple of experiments while learning along the way! . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; if diff_lr_decay_factor: GROUP += f&quot;diff_lr_{diff_lr_decay_factor}&quot; NOTES = f&#39;finetuning {model_name} with {opt_func.__name__} lr={lr:.0e}&#39; TAGS =[model_name, ds_name, opt_func.__name__] . wandb.init(reinit=True, project=&quot;glue-benchmark&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func, splitter=layerwise_splitter) . if diff_lr_decay_factor != 0: k = len(layerwise_splitter(model)) lr = slice(lr*diff_lr_decay_factor**k,lr) metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(4, lr, wd=wd, cbs=cbs) . Could not gather input dimensions . epoch train_loss valid_loss accuracy time . 0 | 0.229984 | 0.264627 | 0.900229 | 02:02 | . 1 | 0.157474 | 0.251536 | 0.912844 | 02:02 | . 2 | 0.105107 | 0.252113 | 0.916284 | 02:03 | . 3 | 0.070137 | 0.278783 | 0.925459 | 02:03 | . Better model found at epoch 0 with accuracy value: 0.9002293348312378. Better model found at epoch 1 with accuracy value: 0.9128440618515015. Better model found at epoch 2 with accuracy value: 0.9162843823432922. Better model found at epoch 3 with accuracy value: 0.9254587292671204. . It&#39;s always useful to check your model predictions after training. fastai makes this very simple: . learn.show_results() . text category category_ . 0 the movie has an infectious exuberance that will engage anyone with a passing interest in the skate/surf culture, the l.a. beach scene and the imaginative ( and sometimes illegal ) ways kids can make a playground out of the refuse of adults. | 1 | 1 | . 1 what really makes it special is that it pulls us into its world, gives us a hero whose suffering and triumphs we can share, surrounds him with interesting characters and sends us out of the theater feeling we&#39;ve shared a great adventure. | 1 | 1 | . 2 this is a train wreck of an action film -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs. | 0 | 0 | . 3 it&#39;s one of those baseball pictures where the hero is stoic, the wife is patient, the kids are as cute as all get-out and the odds against success are long enough to intimidate, but short enough to make a dream seem possible. | 1 | 1 | . 4 though perry and hurley make inspiring efforts to breathe life into the disjointed, haphazard script by jay scherick and david ronn, neither the actors nor director reginald hudlin can make it more than fitfully entertaining. | 0 | 1 | . 5 may be far from the best of the series, but it&#39;s assured, wonderfully respectful of its past and thrilling enough to make it abundantly clear that this movie phenomenon has once again reinvented itself for a new generation. | 1 | 1 | . 6 despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults. | 0 | 0 | . 7 it&#39;s inoffensive, cheerful, built to inspire the young people, set to an unending soundtrack of beach party pop numbers and aside from its remarkable camerawork and awesome scenery, it&#39;s about as exciting as a sunburn. | 0 | 1 | . 8 but the power of these ( subjects ) is obscured by the majority of the film that shows a stationary camera on a subject that could be mistaken for giving a public oration, rather than contributing to a film&#39;s narrative. | 0 | 0 | . . Sweeps . Finding the perfect learning rate for a task isn&#39;t easy. Add weight decay, different optimizers, differential learning rates and various scheduler to the mix and search for the best hyperparameters becomes a really big task. For that reason there exist automated tools for hyperparameter search. Here we&#39;ll look at sweeps functionality provided by W&amp;B. It not only facilitates hyperparameter finetuning but also enables great visualization of the results, which might help for further analysis. Check out documentaion for more details. . def train(): with wandb.init() as run: cfg = run.config model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(task, 2)) metrics = glue_metrics[task] k = len(layerwise_splitter(model)) if cfg.diff_lr_decay_factor: lr = slice(cfg.lr*cfg.diff_lr_decay_factor**k,cfg.lr) learn = TransLearner(dls, model, metrics=metrics, opt_func=Adam, splitter=layerwise_splitter) learn.fit_one_cycle(n_epoch, cfg.lr, wd=cfg.wd, cbs=[WandbCallback(log_preds=False, log_model=False)]) del learn gc.collect() torch.cuda.empty_cache() torch.cuda.synchronize() . Here we&#39;ll do a grid search over combinations of learning rate, weight decay and differential learning rates. Differential learning rates is specified by decay factor $ gamma$: $lr$ for layer $l$ are are determined as ${lr_0}* gamma^{L-l}$, where L is total number of layers. . metrics = glue_metrics[task] metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ sweep_name = f&quot;glue-{task}-sweep&quot; sweep_config = { &quot;project&quot;:&quot;glue-benchmark&quot;, &quot;entity&quot;:&quot;fastai_cimmunity&quot;, &quot;name&quot;: sweep_name, &quot;method&quot;: &quot;grid&quot;, &quot;parameters&quot;: { &quot;lr&quot;: {&quot;values&quot;:[1e-5,2e-5,3e-5,5e-5, 1e-4]}, &quot;wd&quot;: {&quot;values&quot;:[0.,1e-2,5e-2]}, &quot;diff_lr_decay_factor&quot;:{&quot;values&quot;:[0., 0.9, 0.8, 0.7, 0.6]} }, &quot;metric&quot;:{&quot;goal&quot;: &quot;maximise&quot;, &quot;name&quot;: metric_to_monitor}, &quot;early_terminate&quot;: {&quot;type&quot;: &quot;hyperband&quot;, &quot;s&quot;: 2, &quot;eta&quot;: 3, &quot;max_iter&quot;: 40} } . sweep_id = wandb.sweep(sweep_config) wandb.agent(sweep_id, function=train) . As a result we get a nice chart which helps to relate hyperparameter combinations to model performance. . . The sweep can be explored interactively by this link https://wandb.ai/fastai_community/glue-benchmark/sweeps/hc8ytty4. . Another task example: MNLI . MNLI task is interesting for a couple of reasons. It has the largest training set in the benchmark, for the results of training for MNLI might be useful for smaller tasks as we will consider in the next section. Unlike most of the GLUE tasks, which ar formulated as binary classification problem, this one has three categories: entailment, neutral and contradiction. One can argue that solving such kind of problem should envolve more &quot;understanding&quot; of the meaning of text. . task = &#39;mnli&#39;; validate_task() ds = load_dataset(ds_name, task) train_idx, valid_idx = get_splits(ds, valid=&#39;validation_matched&#39;) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[&#39;validation_matched&#39;]]) . Each sample contains premise and hypothesis, the task is to determine whether the hypothesis entails, contradicts or is neutral to the premise. Let&#39;s check out an example: . train_ds[0] . {&#39;hypothesis&#39;: &#39;Product and geography are what make cream skimming work. &#39;, &#39;idx&#39;: 0, &#39;label&#39;: 1, &#39;premise&#39;: &#39;Conceptually cream skimming has two basic dimensions - product and geography.&#39;} . The data preparation and dataloaders construction do not differ much from those for previous task: . lens = train_ds.map(lambda s: {&#39;len&#39;: len(s[&#39;premise&#39;])+len(s[&#39;hypothesis&#39;])}, remove_columns=train_ds.column_names, num_proc=4, keep_in_memory=True) train_lens = lens.select(train_idx)[&#39;len&#39;] valid_lens = lens.select(valid_idx)[&#39;len&#39;] blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock() if task==&#39;stsb&#39; else CategoryBlock()] dblock = DataBlock(blocks = blocks, get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . dl_kwargs=[{&#39;res&#39;:train_lens}, {&#39;val_res&#39;:valid_lens}] dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs, dl_kwargs=dl_kwargs, num_workers=4) dls.show_batch(max_n=4) . text text_ category . 0 well uh that&#39;s kind of obvious i mean they&#39;re even carrying it to to where now uh that they advertise on TV you know if your if you uh you know have done this or if you need this uh uh we&#39;ll sue for you and you don&#39;t have to pay us unless you but then what they don&#39;t tell you is that if you if they win you give them at least a third of the of the thing that they win so i don&#39;t know it is uh it&#39;s getting to be more business now rather than uh actually uh dealing with the crime than with uh um the uh punishment they the the lawyers are just in it for the money i&#39;m i&#39;m convinced i know i i agree with you i think you&#39;re real you&#39;re very right that the politicians should i think they | I think that there should be an equal representation of backgrounds in our politicians. | 0 | . 1 um-hum still have a problem with uh you know i haven&#39;t come to an absolute conclusion on my opinion on this but and i know other Christians would disagree with me my husband and i are kind of not even in agreement on this but we don&#39;t fight over it or anything but you know how can you know the Bible says bless your enemies and bless those that curse you and it&#39;s like be gentle unto all men apt to teach patient kind so it&#39;s like how can you i don&#39;t know for me i don&#39;t know you know i can&#39;t say that i agree with Vietnam because how can you be gentle unto all men and and then shoot them | As a Christian I believe that Vietnam is a necessary war. | 2 | . 2 These 1) approving both changes in existing services and the establishment of new servicesthose are known as classification cases; 2) adjudicating complaints from anyone who believes the Postal Service is not providing rates or services as required by law; 3) issuing advisory opinions when the Postal Service proposes a substantially nationwide change in the nature of its services; and, 4) our mostly recently assigned task, providing Congress with annual reports about the costs and revenues of international mail. | The Postal Service undergoing a review across all of its activities. | 1 | . 3 After all, in this piece the car-home-and-fire salesman turned global strategist describes the Gulf War as if it were a model of Clausewitzian clarity concerning ultimate goals and acceptable means, forgetting in the process that at the end of that war, the Bush/Powell/Schwarzkopf axis internally disagreed about war issues that had never been articulated for the American Should the U.S. destroy the Iraqi military, invade Baghdad, or topple Hussein even after Iraq was repulsed from Kuwait? | Bush, Powell and Schwarzkopf were in full agreement about war issues. | 2 | . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; NOTES = f&#39;finetuning {model_name} with Adam lr={lr:.0e}&#39; TAGS =[model_name, ds_name, &#39;adam&#39;, task] wandb.init(reinit=True, project=&quot;glue-benchmark&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . Training procedure is also very similar: . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics) . metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(4, lr, wd=wd, cbs=cbs) . Could not gather input dimensions . epoch train_loss valid_loss accuracy time . 0 | 0.532420 | 0.497427 | 0.801936 | 22:54 | . 1 | 0.447823 | 0.431625 | 0.835660 | 23:02 | . 2 | 0.384313 | 0.431362 | 0.841161 | 22:58 | . 3 | 0.297241 | 0.459461 | 0.843709 | 23:11 | . Better model found at epoch 0 with accuracy value: 0.8019357919692993. Better model found at epoch 1 with accuracy value: 0.8356596827507019. Better model found at epoch 2 with accuracy value: 0.8411614894866943. Better model found at epoch 3 with accuracy value: 0.8437086343765259. . learn.show_results() . text text_ category category_ . 0 yes they would they just wouldn&#39;t be able to own the kind of automobiles that they think they deserve to own or the kind of homes that we think we deserve to own we might have to you know just be able to i think if we a generation went without debt then the next generation like if if our our generation my husband and i we&#39;re twenty eight if we lived our lives and didn&#39;t become you know indebted like you know our generation before us that um the budget would balance and that we became accustomed to living with what we could afford which we wouldn&#39;t be destitute i mean we wouldn&#39;t be living on the street by any means but just compared to how spoiled we are we would be in our own minds but i feel like the generation after us would oh man it it | Society would be perfect and there would be no more war if we could just rid ourselves of our debt. | 1 | 2 | . 1 and i look back on that and i bought shoes i went shopping i did not need that money i did not need it i didn&#39;t need it i shouldn&#39;t have even qualified to get it i didn&#39;t need it and it would have been a little rough i might have eaten some bologna instead of roast beef out of the deli but i did not need it and as i look back now now we&#39;re paying that back i told my son if you have to live in the ghetto to go to college do it but don&#39;t take out ten thousand dollars in loans don&#39;t do it and i don&#39;t i hope don&#39;t think he&#39;ll have to do that but i just so like we might if we didn&#39;t have those loans we could have saved in the last five years the money for that and i believe | My friends should look towards me as a model of saving money. | 1 | 1 | . 2 and i look back on that and i bought shoes i went shopping i did not need that money i did not need it i didn&#39;t need it i shouldn&#39;t have even qualified to get it i didn&#39;t need it and it would have been a little rough i might have eaten some bologna instead of roast beef out of the deli but i did not need it and as i look back now now we&#39;re paying that back i told my son if you have to live in the ghetto to go to college do it but don&#39;t take out ten thousand dollars in loans don&#39;t do it and i don&#39;t i hope don&#39;t think he&#39;ll have to do that but i just so like we might if we didn&#39;t have those loans we could have saved in the last five years the money for that and i believe | I regret taking out loans. | 0 | 1 | . 3 well the first thing for me is i wonder i see a couple of different ways of talking about what privacy is um if privacy is something that disturbs your private state i mean an invasion of privacy is something that disturbs your private state that&#39;s one thing and if privacy is something that comes into your private state and extracts information from it in other words finds something out about you that&#39;s another and the first kind of invasion of the first type of privacy seems invaded to me in very much everyday in this country but in the second type at least overtly uh where someone comes in and uh finds out information about you that should be private uh does not seem uh um obviously everyday | Talking about privacy is a complicated topic, there are a couple different ways of talking about it, for example privacy is something that disturbs your private state... | 0 | 1 | . 4 The rule prohibits the sale of nicotine-containing cigarettes and smokeless tobacco to individuals under the age of 18; requires manufacturers, distributors, and retailers to comply with various conditions regarding the sale and distribution of these products; requires retailers to verify a purchaser&#39;s age by photographic identification; prohibits all free samples; limits the distribution of these products through vending machines and self-service displays by permitting such methods of sale only in facilities where access by individuals under 18 is prohibited; limits the advertising and labeling to which children and adolescents are exposed; prohibits promotional, non-tobacco items such as hats and tee shirts; prohibits sponsorship of | This rule will make the sale of tobacco products to people under 18 years old legal in every state and Mexico. | 2 | 2 | . 5 yeah the the i mean people like that are crazy i did a study on it though when i was in high school it was one of these things we had to pick a topic to to investigate and at that time i don&#39;t think it&#39;s like that any more but at that time uh it was very unfair capital punishment was a lot more common and if you tended and it tended to be that if you were ignorant or if you were a foreigner or if you were black or any minority for that matter the chances your chances of of uh getting the death penalty were you know like hundreds of times greater than if you could just communicate well i mean you didn&#39;t have to be um you didn&#39;t even necessarily have to be white but if you could just communicate and you could come | It was something I performed research on during high school. | 0 | 0 | . 6 yeah because you look at the statistics now and i&#39;m sure it&#39;s in your your newspapers just like it is in ours that every major city now the increase of crime is is escalating i mean there are more look at the look at the people there are being shot now i mean every day there&#39;s there&#39;s dozens of dozens of people across the nation they just get blown away for no reason you know stray bullets or California they were going out there and they were shooting and they get these guys and they don&#39;t do anything with them so i kind of i kind of agree with you i&#39;m kind of you still in the in the uh prison system | &quot;Crime is escalating now in every major city, however there are plans in place now.&quot; | 1 | 1 | . 7 i know that you know the further we go from Adam the worse the food is for you but God still somehow makes us all be able to still live i think it&#39;s a miracle we&#39;re all still alive after so many generations well the last couple of processed foods you know i mean but i don&#39;t know i like to i like to my i like to be able to eat really healthy you know what am saying and i guess i&#39;m going to have to wait for the millennium i think though because i do don&#39;t think we&#39;re going to restore the earth to you know i think Jesus is the only one that can make this earth be restored to what it should be | It is miraculous God still provides for us to this day. | 0 | 0 | . 8 i know because i think i&#39;ve been reading i read this ten years ago that they were having these big uh um rallies and people would be in the streets flashing signs statehood yes and other people would statehood down the statehood it&#39;s it down there if you&#39;re um familiar with their politics they uh it&#39;s very uh i i don&#39;t know it&#39;s called Latino there they have loudspeakers on their cars and they run down the neighborhood saying vote for you know Pierre he&#39;s or uh Pedro uh Pedro he&#39;s the best it&#39;s it&#39;s really kind of comical | Ten years ago, they rallies on streets with flashing signs and loudspeakers advertising voting candidates. | 0 | 0 | . . MNLI task has another missmatched validation set. matched set contains in-domain data and the missmatched is a cross-domain. . valid_mm_dl = dls.test_dl(ds[&#39;validation_mismatched&#39;], with_labels=True) learn.validate(dl=valid_mm_dl) . Notice that there are similar datasets available (e.g. snli dataset). Those might be used to improve the performance. But for these post I&#39;ll limit the scope to GLUE data only and leave the experiments with extra data for upcoming posts. . Low resource tasks . Some daatsets are rather small, RTE has only 2.5k samples in the training set. This is not much at all for nontrivial language task like this one. But we can try to use a small trick to improve the results. The MNLI task is quite similar and has much more training data. Let&#39;s reuse model trained on it for improving RTE score. This trick is common practice and has been employed in original RoBERTa paper when reporting GLUE score. . task = &#39;rte&#39;; validate_task() ds = load_dataset(ds_name, task) valid_ = &#39;validation-matched&#39; if task==&#39;mnli&#39; else &#39;validation&#39; len(ds[&#39;train&#39;]), len(ds[valid_]) train_idx, valid_idx = get_splits(ds, valid=valid_) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[valid_]]) . train_ds[0] . {&#39;idx&#39;: 0, &#39;label&#39;: 1, &#39;sentence1&#39;: &#39;No Weapons of Mass Destruction Found in Iraq Yet.&#39;, &#39;sentence2&#39;: &#39;Weapons of Mass Destruction Found in Iraq.&#39;} . blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock() if task==&#39;stsb&#39; else CategoryBlock()] dblock = DataBlock(blocks = blocks, get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs) dls.show_batch(max_n=4) . text text_ category . 0 No Weapons of Mass Destruction Found in Iraq Yet. | Weapons of Mass Destruction Found in Iraq. | 1 | . 1 The most recent poll carried out by NOP market research in January revealed that 61% of Britons are opposed to joining the euro. | The introduction of the euro has been opposed. | 0 | . 2 The disappearance of York University chef Claudia Lawrence is now being treated as suspected murder, North Yorkshire Police said. However detectives said they had not found any proof that the 35-year-old, who went missing on 18 March, was dead. Her father Peter Lawrence made a direct appeal to his daughter to contact him five weeks after she disappeared. His plea came at a news conference held shortly after a ¬£10,000 reward was offered to help find Miss Lawrence. Crimestoppers said the sum they were offering was &quot;significantly higher&quot; than usual because of public interest in the case. | Claudia Lawrence is 35 years old. | 0 | . 3 A Continental Connection flight from Newark to Buffalo crashed into a house about four to six miles from Buffalo Niagara International Airport on Thursday night, killing 50 people, officials said. Continental Airlines Flight 3407 is a daily commuter flight from Newark Liberty International Airport in Newark, New Jersey to Buffalo, New York, operated under the Continental Connection brand by Virginia-based regional airline Colgan Air. | A daily commuter flight crashed in New York. | 0 | . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; if diff_lr_decay_factor: GROUP += f&quot;diff_lr_{diff_lr_decay_factor}&quot; NOTES = f&#39;finetuning {model_name} with {opt_func.__name__} lr={lr:.0e}&#39; TAGS =[model_name, ds_name, opt_func.__name__] wandb.init(reinit=True, project=&quot;fasthugs&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func) . try: learn.load(&#39;distilroberta-base-mnli&#39;, with_opt=False, strict=False) except RuntimeError as e: print(e) . Error(s) in loading state_dict for RobertaForSequenceClassification: size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]). size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]). . if diff_lr_decay_factor != 0: k = len(layerwise_splitter(model)) lr = slice(lr*diff_lr_decay_factor**k,lr) metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(10, lr, wd=wd, cbs=cbs, pct_start=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.569979 | 0.565890 | 0.693141 | 00:30 | . 1 | 0.511280 | 0.529077 | 0.736462 | 00:31 | . 2 | 0.409093 | 0.601690 | 0.743682 | 00:31 | . 3 | 0.265996 | 0.763166 | 0.736462 | 00:31 | . 4 | 0.171846 | 0.770063 | 0.754513 | 00:32 | . 5 | 0.098103 | 0.922156 | 0.768953 | 00:32 | . 6 | 0.067698 | 1.030401 | 0.761733 | 00:31 | . 7 | 0.048222 | 1.007513 | 0.772563 | 00:31 | . 8 | 0.034855 | 1.056370 | 0.765343 | 00:32 | . 9 | 0.021131 | 1.069907 | 0.761733 | 00:32 | . As one can see by using this simple trick we&#39;ve improved the result reported at HuggingFace model card by some 10%. Pretty nice, ha? . Just to be sure that improvement is due to using model finetuned on mnli let&#39;s do another run starting from vanilla distilroberta: . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func) . learn.fit_one_cycle(10, lr, wd=wd, cbs=cbs, pct_start=0.1) . Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;, &#39;roberta.pooler.dense.weight&#39;, &#39;roberta.pooler.dense.bias&#39;] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . epoch train_loss valid_loss accuracy time . 0 | 0.695126 | 0.691306 | 0.527076 | 00:31 | . 1 | 0.692349 | 0.692152 | 0.480144 | 00:31 | . 2 | 0.678994 | 0.641740 | 0.624549 | 00:31 | . 3 | 0.602276 | 0.600447 | 0.671480 | 00:31 | . 4 | 0.488653 | 0.662074 | 0.678700 | 00:31 | . 5 | 0.377430 | 0.683057 | 0.678700 | 00:31 | . 6 | 0.269494 | 0.967499 | 0.657040 | 00:31 | . 7 | 0.182777 | 1.016970 | 0.685921 | 00:32 | . 8 | 0.140067 | 1.038462 | 0.696751 | 00:31 | . 9 | 0.113930 | 1.068865 | 0.682310 | 00:32 | . The same is applicable for STSB taks, which has 7k training samples. Performance gain for STSB is not so prominent but it&#39;s still there. You can compare the results for cold and warm starts in this W&amp;B report. . Concluding thoughts . With this we have an simple easy to use framework for quick experimentation with LM finetuning. HuggingFace provides us with huge variety of state of the art Transformers and fastai facilitates configurable training loop with gret API. You are wellcomed to share your comments in dedicated fastai forums topic, try out fasthugs (I&#39;m happy to here your opinions and accept feature requests) and finally open this notebook on Colab, select your task and try to set new best for the model. .",
            "url": "https://blog.abstrukt.co/fastai/huggingface/transformers/2021/05/07/glue-benchmark.html",
            "relUrl": "/fastai/huggingface/transformers/2021/05/07/glue-benchmark.html",
            "date": " ‚Ä¢ May 7, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "A Transformer based Language Model from scratch",
            "content": "In this notebook i&#39;m going to construct transformer based language model from scratch starting with the simplest building blocks. This is inspired by Chapter 12 of Deep Learning for Coders book in which it&#39;s demonstrated how to create a Recurrent Neural Network. It provides a strong intuition of how RNNs relate to regular feed-forward neural nets and why certain design choices were made. Here we aim to aquire similar kind of intuition about Transfomer based architectures. . But as always we should start with the data to be modeled, &#39;cause without data any model makes no particular sense. . Data . Similar to authors of the book I&#39;ll use simple Human numbers dataset which is specifically designed to prototyping model fast and straightforward. For more details on the data one can refer to the aforemantioned book chapter which is also available for free as a notebook (isn&#39;t that awesome?!) . from fastai.text.all import * path = untar_data(URLs.HUMAN_NUMBERS) Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;train.txt&#39;),Path(&#39;valid.txt&#39;)] . The data consists of consecutive numbers from 1 to 9999 inclusive spelled as words. . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . text = &#39; . &#39;.join([l.strip() for l in lines]) tokens = text.split(&#39; &#39;) tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . vocab = L(*tokens).unique() vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . The task will be to predict subsequent token given preceding three. This kind of tasks when the goal is to predict next token from previous ones is called autoregresive language modeling. . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . bs = 64 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . x, y = dls.one_batch() x.shape, y.shape . (torch.Size([64, 3]), torch.Size([64])) . Dot product attention . . The core idea behind Transformers is Attention. Since the release of famous paper Attention is All You Need transformers has become most popular architecture for language modelling. . There are a lot of great resourses explaining transformers architecture. I&#39;ll list some of those I found useful and comprehensive: . The Annotated Transformer completes the original paper with code | Encoder-Decoder Model notebook by huggingface gives mathemetically grounded explanation of how transformer encoder-decoder models work | The Illustrated GPT-2 one of the great blogposts by Jay Alammar visualizing generative language modelling on exaple of GPT-2 | minGPT cool repo by A. Karpathy providing clear minimal implementation of GPT model | There exist multiple attention mechanisms. The particular one used in the original transformer paper is Scaled Dot Product attention. Given query vector for particular token we will compare it with a key vector for each token in a sequence and decide how much value vectors of those will effect resulting representetion of the token of interest. One way to view this from a linguistic prospective is: a key is a question each word respondes to, value is information that word represent and a query is related to what every word was looking to combine with. . Mathemetically we can compute attention for all q, k, v in a matrix form: . $$ textbf {Attention}(Q,K,V) = textbf {softmax}({QK^T over sqrt d_k})V $$ . Note that dot product $QK^T$ results in matrix of shape (seq_len x seq_len). Then it is devided by $ sqrt d_k$ to compensate the fact, that longer sequences will have larger dot product. $ textbf{softmax}$ is applied to rescale the attention matrix to be betwin 0 and 1. When multiplied by $V$ it produces a matrix of the same shape as $V$ (seq_len x dv). . So where those q, k, v come from. Well that&#39;s fairly straitforward queries are culculated from the embeddings of tokens we want to find representation for by simple linear projection. Keys and values are calculated from the embeddings of context tokens. In case of self attention all of them come from the original sequence. . class SelfAttention(Module): def __init__(self, d_in, d_qk, d_v=None): d_v = ifnone(d_v, d_qk) self.iq = nn.Linear(d_in, d_qk) self.ik = nn.Linear(d_in, d_qk) self.iv = nn.Linear(d_in, d_v) self.out = nn.Linear(d_v, d_in) self.scale = d_qk**-0.5 def forward(self, x): q, k, v = self.iq(x), self.ik(x), self.iv(x) q *= self.scale return self.out(F.softmax(q@k.transpose(-2,-1), -1)@v) . Even though self attention mechanism is extremely useful it posseses limited expressive power. Essentially we are computing weighted some of the input modified by single affine transformation, shared across the whole sequence. To add more computational power to the model we can introduce fully connected feedforward network on top of the SelfAttention layer. . Curious reader can find detailed formal analysis of the roles of SelfAttention and FeedForward layers in transformer architecture in this paper by C. Yun et al. In brief the authors state that SelfAttention layers compute precise contextual maps and FeedForward layers then assign the results of these contextual maps to the desired output values. . class FeedForward(Module): def __init__(self, d_in, d_ff): self.lin1 = nn.Linear(d_in, d_ff) self.lin2 = nn.Linear(d_ff, d_in) self.act = nn.ReLU() def forward(self, x): out = self.lin2(self.act(self.lin1(x))) return out . The output would be of shape (bs, seq_len, d) which then may be mapped to (bs, seq_len, vocab_sz) using linear layer. But we have only one target. To adress this issue we can simply do average pooling over seq_len dimention. . The resulting model is fairly simple: . class Model1(Module): def __init__(self, vocab_sz, d_model, d_qk, d_ff): self.emb = Embedding(vocab_sz, d_model) self.attn = SelfAttention(d_model, d_qk) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) x = x.mean(1) return self.out(x) . model = Model1(len(vocab), 64, 64, 128) out = model(x) out.shape . torch.Size([64, 30]) . learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.lr_find() . SuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.019054606556892395) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.023451 | 2.183568 | 0.416211 | 00:03 | . 1 | 1.563715 | 2.401872 | 0.361540 | 00:03 | . 2 | 1.540635 | 1.874314 | 0.452817 | 00:03 | . 3 | 1.594812 | 1.739459 | 0.456145 | 00:03 | . 4 | 1.614958 | 1.713703 | 0.468743 | 00:03 | . To evaluete the model performance we need to compare it to some baseline. Let&#39;s see what would be the accuracy if of the model which would always predict most common token. . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . As you can see, always predicting &quot;thousand&quot; which turn out to be the most common token in the dataset would result in ~15% accuracy. Our simple transformer does much better then that. It feels promising, so let&#39;s try to improve the architecture and check if we can get better results. . Multihead attention . A structured sequence may comprise multiple distinctive kinds of relationships. Our model is forced to learn only one way in which queries, keys and values are constructed from the original token embedding. To remove this limitation we can modify attention layer include multiple heads which would correspond to extracting different kinds of relationships between tokens. The MultiHeadAttention layer consits of several heads each of those is similar to SelfAttention layer we made before. To keep computational cost of the multi-head layer we set $d_k = d_v = d_{model}/n_h$, where $n_h$ is number of heads. . class SelfAttention(Module): def __init__(self, d_in, d_qk, d_v=None): d_v = ifnone(d_v, d_qk) self.iq = nn.Linear(d_in, d_qk) self.ik = nn.Linear(d_in, d_qk) self.iv = nn.Linear(d_in, d_v) self.scale = d_qk**-0.5 def forward(self, x): q, k, v = self.iq(x), self.ik(x), self.iv(x) return F.softmax(q@k.transpose(-2,-1)*self.scale, -1)@v . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads, d_qk=None, d_v=None): d_qk = ifnone(d_qk, d_model//n_heads) d_v = ifnone(d_v, d_qk) self.heads = nn.ModuleList([SelfAttention(d_model, d_qk) for _ in range(n_heads)]) self.out = nn.Linear(d_v*n_heads, d_model) def forward(self, x): out = [m(x) for m in self.heads] return self.out(torch.cat(out, -1)) . inp = torch.randn(8, 10, 64) mha = MultiHeadAttention(64, 8) out = mha(inp) out.shape . torch.Size([8, 10, 64]) . class Model2(Module): def __init__(self, vocab_sz, d_model=64, n_heads=4, d_ff=64*4): self.emb = nn.Embedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) x = x.mean(1) return self.out(x) . learn = Learner(dls, Model2(len(vocab)), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 5e-4) . epoch train_loss valid_loss accuracy time . 0 | 2.177783 | 2.223564 | 0.332303 | 00:04 | . 1 | 1.629889 | 1.867587 | 0.445210 | 00:04 | . 2 | 1.607201 | 1.738342 | 0.464464 | 00:04 | . 3 | 1.606301 | 1.711135 | 0.467316 | 00:04 | . 4 | 1.592446 | 1.708671 | 0.467554 | 00:04 | . MultiHead Attention Refactor . Python for loops are slow, therefore it is better to refactor the MultiHeadAttention module to compute Q, K, V for all heads in batch. . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads): assert d_model%n_heads == 0 self.n_heads = n_heads #d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.out = nn.Linear(d_model, d_model, bias=False) self.scale = d_model//n_heads def forward(self, x): bs, seq_len, d = x.size() # (bs,sl,d) -&gt; (bs,sl,nh,dh) -&gt; (bs,nh,sl,dh) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q*= self.scale att = F.softmax(q@k.transpose(-2,-1), -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return self.out(out) . learn = Learner(dls, Model2(len(vocab)), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.945255 | 2.148961 | 0.362729 | 00:03 | . 1 | 1.576766 | 2.055920 | 0.426432 | 00:03 | . 2 | 1.599834 | 1.934431 | 0.443784 | 00:03 | . 3 | 1.624774 | 1.742481 | 0.460185 | 00:03 | . 4 | 1.615320 | 1.773680 | 0.452817 | 00:03 | . Note that some speedup is observed even on such a tiny dataset and small model. . More signal . Similarly to the RNN case considered in the book, we can take the next step and create more signal for the model to learn from. To adapt to the modified objective we need to make couple of steps. First let&#39;s rearrange data to proper input-target pairs for the new task. . Arranging data . Unlike RNN the tranformer is not a stateful model. This means it treats each sequence indepently and can only attend within fixed length context. This limitation was addressed by authors of Transformer-XL paper where adding a segment-level recurrence mechanism and a novel positional encoding scheme were proposed to enable capturing long-term dependencies. I will not go into details of TransformerXL architecture here. As we shell see stateless transformer can also learn a lot about the structure of our data. . One thing to note in this case is that we don&#39;t need to maintain the structure of the data outside of the sequences, so we can shuffle the sequences randomly in the dataloader. . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=bs, drop_last=True, shuffle=True) xb, yb = dls.one_batch() xb.shape, yb.shape . (torch.Size([64, 16]), torch.Size([64, 16])) . [L(vocab[o] for o in s) for s in seqs[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . Positional encoding . Before we did average pooling over seq_len dimension. Our model didn&#39;t care about the order of the tokens at all. But actually order of the tokens in a sentence matter a lot. In our case one hundred two and two hundred one are pretty different and hundred one two doesn&#39;t make sense. . To encorporate positional information into the model authors of the transformer architecture proposed to use positional encodings in addition to regular token embeddings. Positional encodings may be learned, but it&#39;s also possible to use hardcoded encodings. For instance encodings may be composed of sin and cos. In this way each position in a sequence will get unique vector associated with it. . class PositionalEncoding(Module): def __init__(self, d): self.register_buffer(&#39;freq&#39;, 1/(10000 ** (torch.arange(0., d, 2.)/d))) self.scale = d**0.5 def forward(self, x): device = x.device pos_enc = torch.cat([torch.sin(torch.outer(torch.arange(x.size(1), device=device), self.freq)), torch.cos(torch.outer(torch.arange(x.size(1), device=device), self.freq))], axis=-1) return x*self.scale + pos_enc . x = torch.zeros(1, 16, 64) encs = PositionalEncoding(64)(x) plt.matshow(encs.squeeze()) plt.xlabel(&#39;Embedding size&#39;) plt.ylabel(&#39;Sequence length&#39;) plt.show() . . class TransformerEmbedding(Module): def __init__(self, emb_sz, d_model): self.emb = nn.Embedding(emb_sz, d_model) self.pos_enc = PositionalEncoding(d_model) def forward(self, x): return self.pos_enc(self.emb(x)) . class Model3(Module): def __init__(self, vocab_sz, d_model=64, n_heads=4, d_ff=64*4): self.emb = TransformerEmbedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) return self.out(x) . model = Model3(len(vocab)) out = model(xb) out.shape . torch.Size([64, 16, 30]) . def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . learn = Learner(dls, Model3(len(vocab)), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.059098 | 1.966106 | 0.244059 | 00:01 | . 1 | 0.996265 | 0.151726 | 0.956055 | 00:01 | . 2 | 0.427699 | 0.078305 | 0.977865 | 00:01 | . 3 | 0.207902 | 0.066726 | 0.976481 | 00:01 | . 4 | 0.116920 | 0.074462 | 0.974935 | 00:01 | . Wow! That&#39;s a great accuracy! So the problem is solved and we only needed one attention layer and 2 layer deep feed-forward block? Don&#39;t you feel somewhat skeptical about this result? . Well, you should be! Think about what we did here: the goal was to predict a target sequence, say [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;] from an input [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;]. These two sequences intersect on all positions except the first and the last one. So models needs to learn simply to copy input tokens starting from the second one to the outputs. In our case this will result in 15 correct predictions of total 16 positions, that&#39;s almost 94% accuracy. This makes the task very simple but not very useful to learn. To train proper autoregressive language model, as we did with RNNs, a concept of masking is to be introduced. . Causal Masking . So we want to allow the model for each token to attend only to itself and those prior to it. To acomplish this we can set all the values of attention matrix above the main diagonal to $- infty$. After softmax this values will effectively turn to 0 thus disabling attention to the &quot;future&quot;. . def get_subsequent_mask(x): sz = x.size(1) mask = (torch.triu(torch.ones(sz, sz, device=x.device)) == 1).transpose(0, 1) mask = mask.float().masked_fill(mask == 0, float(&#39;-inf&#39;)).masked_fill(mask == 1, float(0.0)) return mask . inp = torch.randn(8, 10, 64) mask = get_subsequent_mask(inp) plt.matshow(mask); . q, k = torch.rand(1,10,32), torch.randn(1,10,32) att_ = F.softmax((q@k.permute(0,2,1)+mask), -1) plt.matshow(att_[0].detach()); . We should also modify the attention layer to accept mask: . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads): assert d_model%n_heads == 0 self.n_heads = n_heads d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.scale = d_qk**-0.5 self.out = nn.Linear(d_model, d_model, bias=False) def forward(self, x, mask=None): bs, seq_len, d = x.size() mask = ifnone(mask, 0) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q*= self.scale att = F.softmax(q@k.transpose(-2,-1) + mask, -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return self.out(out) . class Model4(Module): def __init__(self, vocab_sz, d_model=64, n_heads=8, d_ff=64*4): self.emb = TransformerEmbedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) mask = get_subsequent_mask(x) x = self.ff(self.attn(x, mask)) return self.out(x) . learn = Learner(dls, Model4(len(vocab)), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.399806 | 2.321602 | 0.262533 | 00:01 | . 1 | 1.804210 | 2.251197 | 0.251709 | 00:01 | . 2 | 1.559652 | 2.320621 | 0.282878 | 00:01 | . 3 | 1.426687 | 2.365385 | 0.281006 | 00:01 | . 4 | 1.355069 | 2.430914 | 0.301025 | 00:01 | . Now we get somewhat lower accuracy, which is expected given that the task has become more difficult. Also training loss is significantly lower than validation loss, which means the model is overfitting. Let&#39;s see if the same approaches as was applied to RNNs can help. . Multilayer transformer . To solve a more difficult task we ussualy need a deeper model. For convenience let&#39;s make a TransformerLayer which will combine self-attention and feed-forward blocks. . class TransformerLayer(Module): def __init__(self, d_model, n_heads=8, d_ff=None, causal=True): d_ff = ifnone(d_ff, 4*d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.causal = causal def forward(self, x, mask=None): if self.causal: mask = get_subsequent_mask(x) return self.ff(self.attn(x, mask)) . class Model5(Module): def __init__(self, vocab_sz, d_model=64, n_layer=4, n_heads=8): self.emb = TransformerEmbedding(vocab_sz, d_model) self.encoder = nn.Sequential(*[TransformerLayer(d_model, n_heads) for _ in range(n_layer)]) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.encoder(x) return self.out(x) . learn = Learner(dls, Model5(len(vocab), n_layer=4), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.896548 | 2.794600 | 0.151611 | 00:03 | . 1 | 2.790987 | 2.813897 | 0.151774 | 00:03 | . 2 | 2.769421 | 2.791088 | 0.151774 | 00:04 | . 3 | 2.756661 | 2.790368 | 0.151367 | 00:04 | . 4 | 2.748009 | 2.799597 | 0.150960 | 00:04 | . That&#39;s not good! 4 layer deep Transformer strugles to learn anything. But there are good news, this problem has been already resolved in the original transformer. . Residual connections and Regularization . If you are familiar with ResNets the proposed solution will not surprise you much. The idea is simple yet very effective. Instead of returning modified output $f(x)$ each transformer sublayer will return $x + f(x)$. This allows the original input to propagate freely through the model. So the model learns not an entirely new representation of $x$ but how to modify $x$ to add some useful information to the original representation. . As we modify layers to include the residual connections let&#39;s also add some regularization by inserting Dropout layers. . class TransformerEmbedding(Module): def __init__(self, emb_sz, d_model, p=0.1): self.emb = Embedding(emb_sz, d_model) nn.init.trunc_normal_(self.emb.weight, std=d_model**-0.5) self.pos_enc = PositionalEncoding(d_model) self.drop = nn.Dropout(p) def forward(self, x): return self.drop(self.pos_enc(self.emb(x))) . Another modification is to add layer normalization which is intended to improve learning dynamics of the network by reparametrising data statistics and is generally used in transformer based architectures. . class FeedForward(Module): def __init__(self, d_model, d_ff, p=0.2): self.lin1 = nn.Linear(d_model, d_ff) self.lin2 = nn.Linear(d_ff, d_model) self.act = nn.ReLU() self.norm = nn.LayerNorm(d_model) self.drop = nn.Dropout(p) def forward(self, x): x = self.norm(x) out = self.act(self.lin1(x)) out = self.lin2(out) return x + self.drop(out) . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads, p=0.1): assert d_model%n_heads == 0 self.n_heads = n_heads d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.scale = d_qk**0.5 self.out = nn.Linear(d_model, d_model, bias=False) self.norm = nn.LayerNorm(d_model) self.drop = nn.Dropout(p) def forward(self, x, mask=None): bs, seq_len, d = x.size() mask = ifnone(mask, 0) x = self.norm(x) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) att = F.softmax(q@k.transpose(-2,-1)/self.scale + mask, -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return x + self.drop(self.out(out)) . class TransformerLayer(Module): def __init__(self, d_model, n_heads=8, d_ff=None, causal=True, p_att=0.1, p_ff=0.1): d_ff = ifnone(d_ff, 4*d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff, p=p_ff) self.causal = causal self._init() def forward(self, x, mask=None): if self.causal: mask = get_subsequent_mask(x) return self.ff(self.attn(x, mask)) def _init(self): for p in self.parameters(): if p.dim()&gt;1: nn.init.xavier_uniform_(p) . class Model6(Module): def __init__(self, vocab_sz, d_model=64, n_layer=4, n_heads=8, p_emb=0.1, p_att=0.1, p_ff=0.2, tie_weights=True): self.emb = TransformerEmbedding(vocab_sz, d_model, p=p_emb) self.encoder = nn.Sequential(*[TransformerLayer(d_model, n_heads, p_att=p_att, p_ff=p_ff) for _ in range(n_layer)], nn.LayerNorm(d_model)) self.out = nn.Linear(d_model, vocab_sz) if tie_weights: self.out.weight = self.emb.emb.weight def forward(self, x): x = self.emb(x) x = self.encoder(x) return self.out(x) . learn = Learner(dls, Model6(len(vocab), n_layer=2), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(8, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.635322 | 2.077494 | 0.193929 | 00:02 | . 1 | 1.751456 | 1.042653 | 0.667806 | 00:02 | . 2 | 1.120504 | 0.743249 | 0.767822 | 00:02 | . 3 | 0.825433 | 0.797066 | 0.733643 | 00:02 | . 4 | 0.687212 | 0.747418 | 0.751058 | 00:02 | . 5 | 0.615355 | 0.815827 | 0.747233 | 00:02 | . 6 | 0.576437 | 0.809159 | 0.751302 | 00:02 | . 7 | 0.557015 | 0.823612 | 0.744466 | 00:02 | . Bonus - Generation example . Learning to predict numbers is great, but let&#39;s try something more entertaining. We can train a language model to generate texts. For example let&#39;s try to generate some text in style of Lewis Carroll. For this we&#39;ll fit a language model on &quot;Alice in Wonderland&quot; and &quot;Through the looking glass&quot;. . def parse_txt(fns): txts = [] for fn in fns: with open(fn) as f: tmp = &#39;&#39; for line in f.readlines(): line = line.strip(&#39; n&#39;) if line: tmp += &#39; &#39; + line elif tmp: txts.append(tmp.strip()) tmp = &#39;&#39; return txts . . texts = parse_txt([path/&#39;11-0.txt&#39;, path/&#39;12-0.txt&#39;]) . len(texts) . 1779 . texts[0:2] . [&#39; ufeffCHAPTER I. Down the Rabbit-Hole&#39;, &#39;Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, ‚Äúand what is the use of a book,‚Äù thought Alice ‚Äúwithout pictures or conversations?‚Äù&#39;] . class CharTokenizer(Transform): &quot;Simple charecter level tokenizer&quot; def __init__(self, vocab=None): self.vocab = ifnone(vocab, [&#39;&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;] + list(string.printable)) self.c2i = defaultdict(int, [(c,i) for i, c in enumerate(self.vocab)]) def encodes(self, s, add_bos=False, add_eos=False): strt = [self.c2i[&#39;xxbos&#39;]] if add_bos else [] end = [self.c2i[&#39;xxeos&#39;]] if add_eos else [] return LMTensorText(strt + [self.c2i[c] for c in s] + end) def decodes(self, s, remove_special=False): return TitledStr(&#39;&#39;.join([self.decode_one(i) for i in s])) def decode_one(self, i): if i == 2: return &#39; n&#39; elif i == 1: return &#39;&#39; else: return self.vocab[i] @property def vocab_sz(self): return len(self.vocab) . . tok = CharTokenizer() . def add_bos_eos(x:list, bos_id=1, eos_id=2): return [bos_id] + x + [eos_id] . nums = [add_bos_eos(tok(t.lower()).tolist()) for t in texts] . len(nums) . 1779 . all_nums = [] for n in nums: all_nums.extend(n) . all_nums[:15] . [1, 0, 15, 20, 13, 28, 32, 17, 30, 97, 21, 78, 97, 16, 27] . print(tok.decode(all_nums[:100])) . chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on . sl = 512 seqs = L((tensor(all_nums[i:i+sl]), tensor(all_nums[i+1:i+sl+1])) for i in range(0,len(all_nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], device=&#39;cuda&#39;, bs=8, drop_last=True, shuffle=True) xb, yb = dls.one_batch() xb.shape, yb.shape . (torch.Size([8, 512]), torch.Size([8, 512])) . model = Model6(tok.vocab_sz, 512, 6, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() learn.lr_find() . SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.6309573650360107) . learn.fit_one_cycle(50, 5e-4, cbs=EarlyStoppingCallback(patience=5)) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.207689 | 3.014779 | 0.187012 | 20.384583 | 00:06 | . 1 | 2.957462 | 2.648416 | 0.258105 | 14.131640 | 00:06 | . 2 | 2.645952 | 2.427977 | 0.287435 | 11.335924 | 00:07 | . 3 | 2.499318 | 2.395460 | 0.292887 | 10.973244 | 00:07 | . 4 | 2.436253 | 2.394616 | 0.288965 | 10.963985 | 00:07 | . 5 | 2.402833 | 2.364455 | 0.295703 | 10.638234 | 00:07 | . 6 | 2.387243 | 2.367871 | 0.290381 | 10.674637 | 00:07 | . 7 | 2.375951 | 2.384834 | 0.285010 | 10.857258 | 00:07 | . 8 | 2.372440 | 2.380138 | 0.276270 | 10.806398 | 00:07 | . 9 | 2.355436 | 2.329239 | 0.305892 | 10.270120 | 00:07 | . 10 | 2.314718 | 2.250078 | 0.331901 | 9.488480 | 00:07 | . 11 | 2.231658 | 2.096768 | 0.385319 | 8.139816 | 00:07 | . 12 | 2.145322 | 1.989301 | 0.413477 | 7.310425 | 00:07 | . 13 | 1.998541 | 1.862125 | 0.444971 | 6.437402 | 00:07 | . 14 | 1.880289 | 1.772380 | 0.465283 | 5.884840 | 00:07 | . 15 | 1.777548 | 1.735309 | 0.482080 | 5.670681 | 00:07 | . 16 | 1.692429 | 1.649408 | 0.501937 | 5.203897 | 00:07 | . 17 | 1.616076 | 1.616357 | 0.513688 | 5.034715 | 00:07 | . 18 | 1.548247 | 1.586346 | 0.522575 | 4.885864 | 00:07 | . 19 | 1.484927 | 1.550523 | 0.532633 | 4.713936 | 00:07 | . 20 | 1.430124 | 1.512773 | 0.543213 | 4.539303 | 00:07 | . 21 | 1.375972 | 1.500666 | 0.545199 | 4.484674 | 00:07 | . 22 | 1.324876 | 1.491262 | 0.552637 | 4.442699 | 00:07 | . 23 | 1.276637 | 1.469852 | 0.557975 | 4.348590 | 00:07 | . 24 | 1.229700 | 1.477631 | 0.560189 | 4.382551 | 00:07 | . 25 | 1.186633 | 1.459370 | 0.562956 | 4.303250 | 00:07 | . 26 | 1.139820 | 1.467342 | 0.564225 | 4.337692 | 00:07 | . 27 | 1.092420 | 1.476885 | 0.566960 | 4.379283 | 00:07 | . 28 | 1.050866 | 1.486061 | 0.567350 | 4.419652 | 00:07 | . 29 | 1.006091 | 1.505293 | 0.568506 | 4.505473 | 00:07 | . 30 | 0.957875 | 1.528569 | 0.567106 | 4.611572 | 00:07 | . No improvement since epoch 25: early stopping . . Text generation . Text generation is a big topic on it&#39;s own. One can refer to great posts by Patrick von Platen from HuggingFace and Lilian Weng for more details on various approaches. Here I will use nucleus sampling. This method rallies on sampling from candidates compounding certain value of probability mass. Intuitively this approach should work for character level generation: when there is only one grammatically correct option for continuation we always want to select it, but when starting a new word some diversity in outputs is desirable. . def expand_dim1(x): if len(x.shape) == 1: return x[None, :] else: return x def top_p_filter(logits, top_p=0.9): sorted_logits, sorted_indices = torch.sort(logits, descending=True) cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) sorted_indices_to_remove = cum_probs &gt; top_p sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove) logits[indices_to_remove] = float(&#39;-inf&#39;) return logits @torch.no_grad() def generate(model, inp, max_len=50, temperature=1., top_k = 20, top_p = 0.9, early_stopping=False, #need eos_idx to work eos_idx=None): model.to(inp.device) model.eval() thresh = top_p inp = expand_dim1(inp) b, t = inp.shape out = inp for _ in range(max_len): x = out logits = model(x)[:, -1, :] filtered_logits = top_p_filter(logits) probs = F.softmax(filtered_logits / temperature, dim=-1) sample = torch.multinomial(probs, 1) out = torch.cat((out, sample), dim=-1) if early_stopping and (sample == eos_idx).all(): break return out . . out = generate(learn.model, tok(&#39;Alice said &#39;), max_len=200, early_stopping=True, eos_idx=tok.c2i[&#39;xxeos&#39;]) . print(tok.decode(out[0])) . Alice said in a minute turn, only purind his with it migut at in musible. i cant elp out my why yested it to like thought: i know i did it wish indeed it hope? . Our relatively simple model learned to generate mostly grammatically plausible text, but it&#39;s not entirely coherent. But it would be too much to ask from the model to learn language from scratch by &quot;reading&quot; only two novels (however great those novels are). To get more from the model let&#39;s feed it larger corpus of data. . Pretraining on larger dataset . For this purpose I will use a sample from bookcorpus dataset. . dataset = load_dataset(&quot;bookcorpus&quot;, split=&#39;train&#39;) . Downloading and preparing dataset bookcorpus/plain_text (download: 1.10 GiB, generated: 4.52 GiB, post-processed: Unknown size, total: 5.62 GiB) to /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/af844be26c089fb64810e9f2cd841954fd8bd596d6ddd26326e4c70e2b8c96fc... Dataset bookcorpus downloaded and prepared to /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/af844be26c089fb64810e9f2cd841954fd8bd596d6ddd26326e4c70e2b8c96fc. Subsequent calls will reuse this data. . df = pd.DataFrame(dataset[:10_000_000]) df.head() . text . 0 the half-ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved . | . 1 isbn : 1492913731 isbn-13 : 978-1492913733 for my family , who encouraged me to never stop fighting for my dreams chapter 1 summer vacations supposed to be fun , right ? | . 2 i wish i had a better answer to that question . | . 3 starlings , new york is not the place youd expect much to happen . | . 4 its a small quiet town , the kind where everyone knows your name . | . df[&#39;len&#39;] = df[&#39;text&#39;].str.len() . cut = int(len(df)*0.8) splits = range_of(df)[:cut], range_of(df[cut:]) tfms = Pipeline([ColReader(&#39;text&#39;), tok]) dsets = Datasets(df, tfms=tfms, dl_type=LMDataLoader, splits=splits) . @patch def create_item(self:LMDataLoader, seq): if seq&gt;=self.n: raise IndexError sl = self.last_len if seq//self.bs==self.n_batches-1 else self.seq_len st = (seq%self.bs)*self.bl + (seq//self.bs)*self.seq_len txt = self.chunks[st : st+sl+1] return LMTensorText(txt[:-1]),txt[1:] . . %%time dl_kwargs = [{&#39;lens&#39;:df[&#39;len&#39;].values[splits[0]]}, {&#39;val_lens&#39;:df[&#39;len&#39;].values[splits[1]]}] dls = dsets.dataloaders(bs=32, seq_len=512, dl_kwargs=dl_kwargs, shuffle_train=True, num_workers=2) . CPU times: user 13min 29s, sys: 13 s, total: 13min 42s Wall time: 13min 35s . dls.show_batch(max_n=2) . text text_ . 0 im sorry .`` ah , this is katrice , the rowan queen , coming toward us . &#39;&#39;i said ill tell you !but she still did n&#39;t understand why he was a slave here .the sprites had come to believe by now that they were a forgotten people .thats what happened .well have to take him with us for a ways and then let him go .nick stifled the fear in him that was trying to take over .crouching down behind the stone balusters , with every nerve tingling , valeria glared down at the stealthy figure .there did seem to be shado | m sorry .`` ah , this is katrice , the rowan queen , coming toward us . &#39;&#39;i said ill tell you !but she still did n&#39;t understand why he was a slave here .the sprites had come to believe by now that they were a forgotten people .thats what happened .well have to take him with us for a ways and then let him go .nick stifled the fear in him that was trying to take over .crouching down behind the stone balusters , with every nerve tingling , valeria glared down at the stealthy figure .there did seem to be shadow | . 1 habitants were genuine , hard working , proud and tough .sergeant colon &#39;s view of the world was certainly changing .&#39;we did n&#39;t want any part of it , &#39; the grandfather continued , heedless of his company , &#39;but maybe that &#39;s just how the rhega are destined to die ... not by our own hands , our own fights .she looked enquiring , but no one took any notice of her .more land would n&#39;t save his people-they needed something else .the other got off a single shot of his .45 caliber pistol before he was clawed acr | abitants were genuine , hard working , proud and tough .sergeant colon &#39;s view of the world was certainly changing .&#39;we did n&#39;t want any part of it , &#39; the grandfather continued , heedless of his company , &#39;but maybe that &#39;s just how the rhega are destined to die ... not by our own hands , our own fights .she looked enquiring , but no one took any notice of her .more land would n&#39;t save his people-they needed something else .the other got off a single shot of his .45 caliber pistol before he was clawed acro | . model = Model6(tok.vocab_sz, 512, 8, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() learn.lr_find() . SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.5248074531555176) . learn = learn.load(path/&#39;char_bookcorpus_10m&#39;) . &lt;fastai.learner.Learner at 0x7fa5a807e0f0&gt; . learn.fit_one_cycle(1, 1e-4) . epoch train_loss valid_loss accuracy perplexity time . 0 | 1.065701 | 1.064358 | 0.667121 | 2.898976 | 4:23:52 | . learn.save(path/&#39;char_bookcorpus_10m&#39;) . Path(&#39;/content/drive/MyDrive/char_model/char_bookcorpus_10m.pth&#39;) . Finetune on Carrolls&#39; books . Finally we can finetune the pretrained bookcorpus model on Carroll&#39;s books. This will determine the style of generated text. . sl = 512 seqs = L((tensor(all_nums[i:i+sl]), tensor(all_nums[i+1:i+sl+1])) for i in range(0,len(all_nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], device=&#39;cuda&#39;, bs=16, drop_last=True, shuffle=True) . model = Model6(tok.vocab_sz, 512, 8, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() . learn = learn.load(path/&#39;char_bookcorpus_10m&#39;) learn.lr_find() . SuggestedLRs(lr_min=0.03019951581954956, lr_steep=0.25118863582611084) . learn.fit_one_cycle(10, 1e-4) . epoch train_loss valid_loss accuracy perplexity time . 0 | 1.744969 | 1.472124 | 0.618443 | 4.358481 | 00:08 | . 1 | 1.442924 | 1.128739 | 0.658796 | 3.091756 | 00:08 | . 2 | 1.248169 | 1.068535 | 0.670881 | 2.911111 | 00:08 | . 3 | 1.134621 | 1.048366 | 0.675764 | 2.852986 | 00:08 | . 4 | 1.058302 | 1.044972 | 0.678554 | 2.843319 | 00:08 | . 5 | 1.004934 | 1.036241 | 0.680333 | 2.818603 | 00:08 | . 6 | 0.966509 | 1.042076 | 0.679461 | 2.835096 | 00:08 | . 7 | 0.938987 | 1.040590 | 0.680507 | 2.830886 | 00:08 | . 8 | 0.920266 | 1.035050 | 0.681728 | 2.815248 | 00:08 | . 9 | 0.907935 | 1.037062 | 0.681257 | 2.820917 | 00:08 | . As you see pretraining model on large corpus followed by finetuning helped to reduce validation loss from arount 1.53 to 1.037 and improve accuracy in predicting next character to 68% (compared to 56.7% before). Let&#39;s see how it effects sampled text: . out = generate(learn.model, tok(&#39;Alice said &#39;), max_len=200, early_stopping=True, eos_idx=tok.c2i[&#39;xxeos&#39;]) . print(tok.decode(out[0])) . . Alice said what you want is, why, if you cant see a little rather from people to bed their moment, when birds began drinking from behind, and offering the cart to say something, and dripping off a strange mou .",
            "url": "https://blog.abstrukt.co/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html",
            "relUrl": "/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html",
            "date": " ‚Ä¢ Jan 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I‚Äôm independent AI Researcher and ML Engineer. . Other articles I had pleasure to co-author: . Our Reproducibility Challenge Experience with Morgan, Hallvar and our fastai community team . Fine tuning CLIP with Remote Sensing (Satellite) images and captions with Sujit Pal and TWIML community team . You can check out my open source projects at github . This website is powered by fastpages. .",
          "url": "https://blog.abstrukt.co/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://blog.abstrukt.co/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}