{
  
    
        "post0": {
            "title": "Finetuning Transformers on GLUE benchmark",
            "content": ". from transformers import AutoModelForSequenceClassification from fastai.text.all import * from fastai.callback.wandb import * from fasthugs.learner import TransLearner from fasthugs.data import TransformersTextBlock, TextGetter, get_splits from datasets import load_dataset, concatenate_datasets import wandb import gc . Introduction . In this blogpost will look at how to conbine the power of HuggingFace with great flexibility of fastai. For this purpose we will look at finetuning on GLUE benchmark tasks. Fun fact: it was introduced in this paper 2018 as tough to beat benchmark to chellange NLP systems and in just about a year new SuperGLUE benchmark was introduced because original GLUE has become too easy for the models. . To give you a grasp on what are we dealing with, here is a brief summary of GLUE tasks: . Name Task description Size Metrics . cola Corpus of Linguistic Acceptability | Determine whether it is a grammatical sentence | 8.5k | matthews_corrcoef | . sst2 Stanford Sentiment Treebank | Predict the sentiment of a givensentence | 67k | accuracy | . mrpc Microsoft Research Paraphrase Corpus | Determine whether the sentences in the pair are semantically equivalent | 3.7k | f1/accuracy | . stsb Semantic Textual Similarity Benchmark | Determine similarity score for 2 sentences | 7k | pearsonr/spearmanr | . qqp Quora question pair | Determine if 2 questions are the same (paraphrase) | 364k | f1/accuracy | . mnli Mulit-Genre Natural Language Inference | Predict whether the premise entails, contradicts or is neutral to the hypothesis | 393k | accuracy | . qnli Stanford Question Answering Dataset | Determine whether the context sentence containsthe answer to the question | 105k | accuracy | . rte Recognize Textual Entailment | Determine whether one sentece entails another | 2.5k | accuracy | . wnli Winograd Schema Challenge | Predict if the sentence with the pronoun substituted is entailed by the original sentence | 634 | accuracy | . As you can see some datasets are really small here. And we&#39;ll look at how one can adress. . Setup . Let&#39;s define main settings for the run in one place: . ds_name = &#39;glue&#39; model_name = &quot;distilroberta-base&quot; max_len = 512 bs = 32 val_bs = bs*2 n_epoch = 4 lr = 2e-5 wd = 0. opt_func = Adam diff_lr_decay_factor = 0 . To make switching between datasets smooth I&#39;ll define couple of dictionaries containing per-task information. We&#39;ll need metrics, text fields to retrieve data and number of outputs for the model. . [&#39;/&#39;.join([m.name if isinstance(m, Metric) else m.__name__ for m in ms]) for ms in glue_metrics.values()] . [&#39;matthews_corrcoef&#39;, &#39;accuracy&#39;, &#39;f1_score/accuracy&#39;, &#39;pearsonr/spearmanr&#39;, &#39;f1_score/accuracy&#39;, &#39;accuracy&#39;, &#39;accuracy&#39;, &#39;accuracy&#39;, &#39;accuracy&#39;] . GLUE_TASKS = [&quot;cola&quot;, &quot;mnli&quot;, &quot;mrpc&quot;, &quot;qnli&quot;, &quot;qqp&quot;, &quot;rte&quot;, &quot;sst2&quot;, &quot;stsb&quot;, &quot;wnli&quot;] def validate_task(): assert task in GLUE_TASKS . glue_metrics = { &#39;cola&#39;:[MatthewsCorrCoef()], &#39;sst2&#39;:[accuracy], &#39;mrpc&#39;:[F1Score(), accuracy], &#39;stsb&#39;:[PearsonCorrCoef(), SpearmanCorrCoef()], &#39;qqp&#39; :[F1Score(), accuracy], &#39;mnli&#39;:[accuracy], &#39;qnli&#39;:[accuracy], &#39;rte&#39; :[accuracy], &#39;wnli&#39;:[accuracy], } glue_textfields = { &#39;cola&#39;:[&#39;sentence&#39;, None], &#39;sst2&#39;:[&#39;sentence&#39;, None], &#39;mrpc&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;stsb&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;qqp&#39; :[&#39;question1&#39;, &#39;question2&#39;], &#39;mnli&#39;:[&#39;premise&#39;, &#39;hypothesis&#39;], &#39;qnli&#39;:[&#39;question&#39;, &#39;sentence&#39;], &#39;rte&#39; :[&#39;sentence1&#39;, &#39;sentence2&#39;], &#39;wnli&#39;:[&#39;sentence1&#39;, &#39;sentence2&#39;], } glue_num_labels = {&#39;mnli&#39;:3, &#39;stsb&#39;:1} . def layerwise_splitter(model): emb = L(model.base_model.embeddings) layers = L(model.base_model.encoder.layer.children()) clf = L(m for m in list(model.children())[1:] if params(m)) groups = emb + layers + clf return groups.map(params) . Running a GLUE task . task = &#39;sst2&#39; validate_task() . ds = load_dataset(ds_name, task) . Reusing dataset glue (/root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad) . valid_ = &#39;validation-matched&#39; if task==&#39;mnli&#39; else &#39;validation&#39; len(ds[&#39;train&#39;]), len(ds[valid_]) . (67349, 872) . train_idx, valid_idx = get_splits(ds, valid=valid_) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[valid_]]) . train_ds[0] . {&#39;idx&#39;: 0, &#39;label&#39;: 0, &#39;sentence&#39;: &#39;hide new secretions from the parental units &#39;} . Here I use number of characters a proxy for length of tokenized text to speed up dls creation. . lens = train_ds.map(lambda s: {&#39;len&#39;: sum([len(s[i]) for i in glue_textfields[task] if i])}, remove_columns=train_ds.column_names, num_proc=2, keep_in_memory=True) train_lens = lens.select(train_idx)[&#39;len&#39;] valid_lens = lens.select(valid_idx)[&#39;len&#39;] . . dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), CategoryBlock()], get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . %%time dl_kwargs=[{&#39;res&#39;:train_lens}, {&#39;val_res&#39;:valid_lens}] dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs, dl_kwargs=dl_kwargs) . CPU times: user 16.9 s, sys: 992 ms, total: 17.9 s Wall time: 17.8 s . dls.show_batch(max_n=4) . text category . 0 ... spiced with humor (&#39;i speak fluent flatula,&#39;advises denlopp after a rather, er, bubbly exchange with an alien deckhand ) and witty updatings ( silver&#39;s parrot has been replaced with morph, a cute alien creature who mimics everyone and everything around ) | 1 | . 1 stopped thinking about how good it all was, and started doing nothing but reacting to it - feeling a part of its grand locations, thinking urgently as the protagonists struggled, feeling at the mercy of its inventiveness, gasping at its visual delights. | 1 | . 2 ozpetek offers an aids subtext, skims over the realities of gay sex, and presents yet another tired old vision of the gay community as an all-inclusive world where uptight, middle class bores like antonia can feel good about themselves. | 0 | . 3 &#39;s... worth the extra effort to see an artist, still committed to growth in his ninth decade, change while remaining true to his principles with a film whose very subject is, quite pointedly, about the peril of such efforts. | 1 | . Single run . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; if diff_lr_decay_factor: GROUP += f&quot;diff_lr_{diff_lr_decay_factor}&quot; NOTES = f&#39;finetuning {model_name} with {opt_func.__name__} lr={lr:.0e}&#39; TAGS =[model_name, ds_name, opt_func.__name__] . wandb.init(reinit=True, project=&quot;fasthugs&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func, splitter=layerwise_splitter) . if diff_lr_decay_factor != 0: k = len(layerwise_splitter(model)) lr = slice(lr*diff_lr_decay_factor**k,lr) metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(4, lr, wd=wd, cbs=cbs) . Could not gather input dimensions . epoch train_loss valid_loss accuracy time . 0 | 0.234400 | 0.234248 | 0.915138 | 03:09 | . 1 | 0.173527 | 0.247280 | 0.917431 | 03:11 | . 2 | 0.097726 | 0.246916 | 0.924312 | 03:10 | . 3 | 0.078707 | 0.263630 | 0.925459 | 03:11 | . Better model found at epoch 0 with accuracy value: 0.9151375889778137. Better model found at epoch 1 with accuracy value: 0.9174311757087708. Better model found at epoch 2 with accuracy value: 0.9243119359016418. Better model found at epoch 3 with accuracy value: 0.9254587292671204. . learn.show_results() . text category category_ . 0 the movie has an infectious exuberance that will engage anyone with a passing interest in the skate/surf culture, the l.a. beach scene and the imaginative ( and sometimes illegal ) ways kids can make a playground out of the refuse of adults. | 1 | 1 | . 1 what really makes it special is that it pulls us into its world, gives us a hero whose suffering and triumphs we can share, surrounds him with interesting characters and sends us out of the theater feeling we&#39;ve shared a great adventure. | 1 | 1 | . 2 this is a train wreck of an action film -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs. | 0 | 0 | . 3 it&#39;s one of those baseball pictures where the hero is stoic, the wife is patient, the kids are as cute as all get-out and the odds against success are long enough to intimidate, but short enough to make a dream seem possible. | 1 | 1 | . 4 though perry and hurley make inspiring efforts to breathe life into the disjointed, haphazard script by jay scherick and david ronn, neither the actors nor director reginald hudlin can make it more than fitfully entertaining. | 0 | 1 | . 5 may be far from the best of the series, but it&#39;s assured, wonderfully respectful of its past and thrilling enough to make it abundantly clear that this movie phenomenon has once again reinvented itself for a new generation. | 1 | 1 | . 6 despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults. | 0 | 0 | . 7 it&#39;s inoffensive, cheerful, built to inspire the young people, set to an unending soundtrack of beach party pop numbers and aside from its remarkable camerawork and awesome scenery, it&#39;s about as exciting as a sunburn. | 0 | 1 | . 8 but the power of these ( subjects ) is obscured by the majority of the film that shows a stationary camera on a subject that could be mistaken for giving a public oration, rather than contributing to a film&#39;s narrative. | 0 | 0 | . Sweeps . Finding the perfect learning rate for a task isn&#39;t easy. Add weight decay, different optimizers, differential learning rates and various scheduler to the mix and search for best hyperparameters becomes a really big task. For that reason there exist automated tools for hyperparameter search. Here we&#39;ll look at sweeps functionality provided by W&amp;B. It not only facilitates hyperparameter finetuning but also enables great visualization of the results, which might help for further analysis. . wandb.login() . Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable wandb: Currently logged in as: fastai_community (use `wandb login --relogin` to force relogin) . True . def train(): with wandb.init() as run: cfg = run.config model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(task, 2)) metrics = glue_metrics[task] k = len(layerwise_splitter(model)) if cfg.diff_lr_decay_factor: lr = slice(cfg.lr*cfg.diff_lr_decay_factor**k,cfg.lr) learn = TransLearner(dls, model, metrics=metrics, opt_func=Adam, splitter=layerwise_splitter) learn.fit_one_cycle(n_epoch, cfg.lr, wd=cfg.wd, cbs=[WandbCallback(log_preds=False, log_model=False)]) del learn gc.collect() torch.cuda.empty_cache() torch.cuda.synchronize() . metrics = glue_metrics[task] metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ sweep_name = f&quot;glue-{task}-sweep&quot; sweep_config = { &quot;name&quot;: sweep_name, &quot;method&quot;: &quot;random&quot;, &quot;parameters&quot;: { &quot;lr&quot;: {&quot;values&quot;:[1e-5,2e-5,3e-5,5e-5, 1e-4, 3e-4]}, &quot;wd&quot;:{&quot;values&quot;:[0.,1e-2,5e-2]}, &quot;diff_lr_decay_factor&quot;:{&quot;values&quot;:[0., 0.9, 0.8, 0.7, 0.6]} }, &quot;metric&quot;:{&quot;goal&quot;: &quot;maximise&quot;, &quot;name&quot;: metric_to_monitor}, &quot;early_terminate&quot;: {&quot;type&quot;: &quot;hyperband&quot;, &quot;s&quot;: 2, &quot;eta&quot;: 3, &quot;max_iter&quot;: 40} } . sweep_id = wandb.sweep(sweep_config, project=&#39;glue-benchmark&#39;, entity=&quot;fastai_community&quot;) . Create sweep with ID: dwio5fl2 Sweep URL: https://wandb.ai/fastai_community/uncategorized/sweeps/dwio5fl2 . wandb.agent(sweep_id, function=train) . wandb.finish() . Another task example: MultiNLI . task = &#39;mnli&#39; validate_task() . ds = load_dataset(ds_name, task) . Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad) . train_idx, valid_idx = get_splits(ds, valid=&#39;validation_matched&#39;) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[&#39;validation_matched&#39;]]) . train_ds[0] . {&#39;hypothesis&#39;: &#39;Product and geography are what make cream skimming work. &#39;, &#39;idx&#39;: 0, &#39;label&#39;: 1, &#39;premise&#39;: &#39;Conceptually cream skimming has two basic dimensions - product and geography.&#39;} . lens = train_ds.map(lambda s: {&#39;len&#39;: len(s[&#39;premise&#39;])+len(s[&#39;hypothesis&#39;])}, remove_columns=train_ds.column_names, num_proc=4, keep_in_memory=True) train_lens = lens.select(train_idx)[&#39;len&#39;] valid_lens = lens.select(valid_idx)[&#39;len&#39;] . . dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), CategoryBlock()], get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . %%time dl_kwargs=[{&#39;res&#39;:train_lens}, {&#39;val_res&#39;:valid_lens}] dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs, dl_kwargs=dl_kwargs, num_workers=4) . CPU times: user 1min 55s, sys: 2.15 s, total: 1min 57s Wall time: 1min 57s . dls.show_batch(max_n=4) . text text_ category . 0 well uh that&#39;s kind of obvious i mean they&#39;re even carrying it to to where now uh that they advertise on TV you know if your if you uh you know have done this or if you need this uh uh we&#39;ll sue for you and you don&#39;t have to pay us unless you but then what they don&#39;t tell you is that if you if they win you give them at least a third of the of the thing that they win so i don&#39;t know it is uh it&#39;s getting to be more business now rather than uh actually uh dealing with the crime than with uh um the uh punishment they the the lawyers are just in it for the money i&#39;m i&#39;m convinced i know i i agree with you i think you&#39;re real you&#39;re very right that the politicians should i think they | I think that there should be an equal representation of backgrounds in our politicians. | 0 | . 1 that&#39;s funny yeah and that is a good short term thing though that little things like that that overall though i just think we&#39;re just going to i don&#39;t know see i know i guess i&#39;m kind of leery of this topic because i know that Bush is real for the new world order the one world government and alleviating all you know national debt between all of the nations but i see that to be a potential power problem later with um who&#39;s going to be in charge with this new world order and i you know i&#39;m uncomfortable with that much power being in one place but i know we already have a new money system we already have new bills printed for the US Treasury already has our new bills printed for new currency and i mean i&#39;ve seen them and so i know that the long-term | I hope all power is concentrated into one country. | 2 | . 2 that&#39;s right um we were down in Dallas right after Christmas and on the way back we stopped in Louisiana to visit my brother and we were driving my husband&#39;s Toyota pick up truck well we made a quick little stop when we got to Baton Rouge and he came came back out and the car the truck wouldn&#39;t stop i mean it wouldn&#39;t start so gave it a somebody came along and helped give it a little push and the next morning they took it to the garage and it was just a small private garage and he said it was the starter motor proba bly and he was going to take it off and either repair it or replace it or whatever and we got a call in the middle of the morning and he said i&#39;ve got good news and bad news uh the starter motor | He said the starter motor was probably fine and that it must be something else. | 2 | . 3 not really yeah really well that&#39;s pretty wild we yeah we used it for fleas we had fleas in our yard real bad last year and we did that um i just i&#39;m not basically i like to mow the lawn believe it or not but i sometimes have problems starting the mower so a lot of times i don&#39;t get out and do it but my husband basically does most of it and he does the you know edging and all that kind of thing and we&#39;re renting and so we don&#39;t really put a lot of money into the uh you know this like this lawn could probably stand a couple of loads of dirt and some Saint Augustine we just we have winter rye out back and we have i don&#39;t even know what it is out front but um we this is the first house we&#39;ve | We don&#39;t worry much about the lawn because we rent. | 0 | . Tracking with W&amp;B . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; NOTES = f&#39;finetuning {model_name} with Adam lr={lr:.0e}&#39; TAGS =[model_name, ds_name, &#39;adam&#39;] . wandb.init(reinit=True, project=&quot;glue-benchmark&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . Training . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics) . metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(8, lr, wd=wd, cbs=cbs) . Could not gather input dimensions . . 0.00% [0/8 00:00&lt;00:00] epoch train_loss valid_loss accuracy time . . 16.23% [1992/12271 04:53&lt;25:16 0.9426] &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.show_results() . valid_mm_dl = dls.test_dl(ds[&#39;validation_mismatched&#39;], with_labels=True) learn.validate(dl=valid_mm_dl) . Low resource tasks . Notice that rte task has only 2.5k samples in the training set. This is not much at all for untrivial language task like this one. But we can try to use a small trick for that. The MNLI task is quite similar and has much more training data. Let&#39;s reuse model trained on it for improving RTE score. This trick is common practice and has been employed in original RoBERTa paper whe reporting GLUE score. . task = &#39;rte&#39;; validate_task() ds = load_dataset(ds_name, task) valid_ = &#39;validation-matched&#39; if task==&#39;mnli&#39; else &#39;validation&#39; len(ds[&#39;train&#39;]), len(ds[valid_]) train_idx, valid_idx = get_splits(ds, valid=valid_) train_ds = concatenate_datasets([ds[&#39;train&#39;], ds[valid_]]) . Reusing dataset glue (/root/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad) . train_ds[0] . {&#39;idx&#39;: 0, &#39;label&#39;: 1, &#39;sentence1&#39;: &#39;No Weapons of Mass Destruction Found in Iraq Yet.&#39;, &#39;sentence2&#39;: &#39;Weapons of Mass Destruction Found in Iraq.&#39;} . dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), CategoryBlock()], get_x=TextGetter(*glue_textfields[task]), get_y=ItemGetter(&#39;label&#39;), splitter=IndexSplitter(valid_idx)) . dls = dblock.dataloaders(train_ds, bs=bs, val_bs=val_bs) . dls.show_batch(max_n=4) . text text_ category . 0 No Weapons of Mass Destruction Found in Iraq Yet. | Weapons of Mass Destruction Found in Iraq. | 1 | . 1 The most recent poll carried out by NOP market research in January revealed that 61% of Britons are opposed to joining the euro. | The introduction of the euro has been opposed. | 0 | . 2 The disappearance of York University chef Claudia Lawrence is now being treated as suspected murder, North Yorkshire Police said. However detectives said they had not found any proof that the 35-year-old, who went missing on 18 March, was dead. Her father Peter Lawrence made a direct appeal to his daughter to contact him five weeks after she disappeared. His plea came at a news conference held shortly after a £10,000 reward was offered to help find Miss Lawrence. Crimestoppers said the sum they were offering was &quot;significantly higher&quot; than usual because of public interest in the case. | Claudia Lawrence is 35 years old. | 0 | . 3 A Continental Connection flight from Newark to Buffalo crashed into a house about four to six miles from Buffalo Niagara International Airport on Thursday night, killing 50 people, officials said. Continental Airlines Flight 3407 is a daily commuter flight from Newark Liberty International Airport in Newark, New Jersey to Buffalo, New York, operated under the Continental Connection brand by Virginia-based regional airline Colgan Air. | A daily commuter flight crashed in New York. | 0 | . WANDB_NAME = f&#39;{ds_name}-{task}-{model_name}&#39; GROUP = f&#39;{ds_name}-{task}-{model_name}-{lr:.0e}&#39; if diff_lr_decay_factor: GROUP += f&quot;diff_lr_{diff_lr_decay_factor}&quot; NOTES = f&#39;finetuning {model_name} with {opt_func.__name__} lr={lr:.0e}&#39; TAGS =[model_name, ds_name, opt_func.__name__] . wandb.init(reinit=True, project=&quot;fasthugs&quot;, entity=&quot;fastai_community&quot;, name=WANDB_NAME, group=GROUP, notes=NOTES, tags=TAGS); . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func) . try: learn.load(&#39;distilroberta-base-mnli&#39;, with_opt=False, strict=False) except RuntimeError as e: print(e) . Error(s) in loading state_dict for RobertaForSequenceClassification: size mismatch for classifier.out_proj.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]). size mismatch for classifier.out_proj.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]). . if diff_lr_decay_factor != 0: k = len(layerwise_splitter(model)) lr = slice(lr*diff_lr_decay_factor**k,lr) metric_to_monitor = metrics[0].name if isinstance(metrics[0], Metric) else metrics[0].__name__ cbs = [WandbCallback(log_preds=False, log_model=False), SaveModelCallback(monitor=metric_to_monitor, fname=f&#39;{model_name}-{task}&#39;)] learn.fit_one_cycle(10, lr, wd=wd, cbs=cbs, pct_start=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.569979 | 0.565890 | 0.693141 | 00:30 | . 1 | 0.511280 | 0.529077 | 0.736462 | 00:31 | . 2 | 0.409093 | 0.601690 | 0.743682 | 00:31 | . 3 | 0.265996 | 0.763166 | 0.736462 | 00:31 | . 4 | 0.171846 | 0.770063 | 0.754513 | 00:32 | . 5 | 0.098103 | 0.922156 | 0.768953 | 00:32 | . 6 | 0.067698 | 1.030401 | 0.761733 | 00:31 | . 7 | 0.048222 | 1.007513 | 0.772563 | 00:31 | . 8 | 0.034855 | 1.056370 | 0.765343 | 00:32 | . 9 | 0.021131 | 1.069907 | 0.761733 | 00:32 | . model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=glue_num_labels.get(&#39;task&#39;, 2)) metrics = glue_metrics[task] learn = TransLearner(dls, model, metrics=metrics, opt_func=opt_func) . learn.fit_one_cycle(10, lr, wd=wd, cbs=cbs, pct_start=0.1) . Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;, &#39;roberta.pooler.dense.weight&#39;, &#39;roberta.pooler.dense.bias&#39;] - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: [&#39;classifier.dense.weight&#39;, &#39;classifier.dense.bias&#39;, &#39;classifier.out_proj.weight&#39;, &#39;classifier.out_proj.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . epoch train_loss valid_loss accuracy time . 0 | 0.695126 | 0.691306 | 0.527076 | 00:31 | . 1 | 0.692349 | 0.692152 | 0.480144 | 00:31 | . 2 | 0.678994 | 0.641740 | 0.624549 | 00:31 | . 3 | 0.602276 | 0.600447 | 0.671480 | 00:31 | . 4 | 0.488653 | 0.662074 | 0.678700 | 00:31 | . 5 | 0.377430 | 0.683057 | 0.678700 | 00:31 | . 6 | 0.269494 | 0.967499 | 0.657040 | 00:31 | . 7 | 0.182777 | 1.016970 | 0.685921 | 00:32 | . 8 | 0.140067 | 1.038462 | 0.696751 | 00:31 | . 9 | 0.113930 | 1.068865 | 0.682310 | 00:32 | . The same holds for STSB taks, which has 7k training samples. You can compare the results for cold and warm starts in this report. . &lt;/div&gt; .",
            "url": "https://arampacha.github.io/thoughtsamples/2021/05/07/glue_benchmark.html",
            "relUrl": "/2021/05/07/glue_benchmark.html",
            "date": " • May 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A Transformer based Language Model from scratch",
            "content": "In this notebook i&#39;m going to construct transformer based language model from scratch starting with the simplest building blocks. This is inspired by Chapter 12 of Deep Learning for Coders book in which it&#39;s demonstrated how to create a Recurrent Neural Network. It provides a strong intuition of how RNNs relate to regular feed-forward neural nets and why certain design choices were made. Here we aim to aquire similar kind of intuition about Transfomer based architectures. . But as always we should start with the data to be modeled, &#39;cause without data any model makes no particular sense. . Data . Similar to authors of the book I&#39;ll use simple Human numbers dataset which is specifically designed to prototyping model fast and straightforward. For more details on the data one can refer to the aforemantioned book chapter which is also available for free as a notebook (isn&#39;t that awesome?!) . from fastai.text.all import * path = untar_data(URLs.HUMAN_NUMBERS) Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;train.txt&#39;),Path(&#39;valid.txt&#39;)] . The data consists of consecutive numbers from 1 to 9999 inclusive spelled as words. . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . text = &#39; . &#39;.join([l.strip() for l in lines]) tokens = text.split(&#39; &#39;) tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . vocab = L(*tokens).unique() vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . The task will be to predict subsequent token given preceding three. This kind of tasks when the goal is to predict next token from previous ones is called autoregresive language modeling. . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . bs = 64 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . x, y = dls.one_batch() x.shape, y.shape . (torch.Size([64, 3]), torch.Size([64])) . Dot product attention . . The core idea behind Transformers is Attention. Since the release of famous paper Attention is All You Need transformers has become most popular architecture for language modelling. . There are a lot of great resourses explaining transformers architecture. I&#39;ll list some of those I found useful and comprehensive: . The Annotated Transformer completes the original paper with code | Encoder-Decoder Model notebook by huggingface gives mathemetically grounded explanation of how transformer encoder-decoder models work | The Illustrated GPT-2 one of the great blogposts by Jay Alammar visualizing generative language modelling on exaple of GPT-2 | minGPT cool repo by A. Karpathy providing clear minimal implementation of GPT model | There exist multiple attention mechanisms. The particular one used in the original transformer paper is Scaled Dot Product attention. Given query vector for particular token we will compare it with a key vector for each token in a sequence and decide how much value vectors of those will effect resulting representetion of the token of interest. One way to view this from a linguistic prospective is: a key is a question each word respondes to, value is information that word represent and a query is related to what every word was looking to combine with. . Mathemetically we can compute attention for all q, k, v in a matrix form: . $$ textbf {Attention}(Q,K,V) = textbf {softmax}({QK^T over sqrt d_k})V $$ . Note that dot product $QK^T$ results in matrix of shape (seq_len x seq_len). Then it is devided by $ sqrt d_k$ to compensate the fact, that longer sequences will have larger dot product. $ textbf{softmax}$ is applied to rescale the attention matrix to be betwin 0 and 1. When multiplied by $V$ it produces a matrix of the same shape as $V$ (seq_len x dv). . So where those q, k, v come from. Well that&#39;s fairly straitforward queries are culculated from the embeddings of tokens we want to find representation for by simple linear projection. Keys and values are calculated from the embeddings of context tokens. In case of self attention all of them come from the original sequence. . class SelfAttention(Module): def __init__(self, d_in, d_qk, d_v=None): d_v = ifnone(d_v, d_qk) self.iq = nn.Linear(d_in, d_qk) self.ik = nn.Linear(d_in, d_qk) self.iv = nn.Linear(d_in, d_v) self.out = nn.Linear(d_v, d_in) self.scale = d_qk**-0.5 def forward(self, x): q, k, v = self.iq(x), self.ik(x), self.iv(x) q *= self.scale return self.out(F.softmax(q@k.transpose(-2,-1), -1)@v) . Even though self attention mechanism is extremely useful it posseses limited expressive power. Essentially we are computing weighted some of the input modified by single affine transformation, shared across the whole sequence. To add more computational power to the model we can introduce fully connected feedforward network on top of the SelfAttention layer. . Curious reader can find detailed formal analysis of the roles of SelfAttention and FeedForward layers in transformer architecture in this paper by C. Yun et al. In brief the authors state that SelfAttention layers compute precise contextual maps and FeedForward layers then assign the results of these contextual maps to the desired output values. . class FeedForward(Module): def __init__(self, d_in, d_ff): self.lin1 = nn.Linear(d_in, d_ff) self.lin2 = nn.Linear(d_ff, d_in) self.act = nn.ReLU() def forward(self, x): out = self.lin2(self.act(self.lin1(x))) return out . The output would be of shape (bs, seq_len, d) which then may be mapped to (bs, seq_len, vocab_sz) using linear layer. But we have only one target. To adress this issue we can simply do average pooling over seq_len dimention. . The resulting model is fairly simple: . class Model1(Module): def __init__(self, vocab_sz, d_model, d_qk, d_ff): self.emb = Embedding(vocab_sz, d_model) self.attn = SelfAttention(d_model, d_qk) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) x = x.mean(1) return self.out(x) . model = Model1(len(vocab), 64, 64, 128) out = model(x) out.shape . torch.Size([64, 30]) . learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.lr_find() . SuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.019054606556892395) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.023451 | 2.183568 | 0.416211 | 00:03 | . 1 | 1.563715 | 2.401872 | 0.361540 | 00:03 | . 2 | 1.540635 | 1.874314 | 0.452817 | 00:03 | . 3 | 1.594812 | 1.739459 | 0.456145 | 00:03 | . 4 | 1.614958 | 1.713703 | 0.468743 | 00:03 | . To evaluete the model performance we need to compare it to some baseline. Let&#39;s see what would be the accuracy if of the model which would always predict most common token. . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . As you can see, always predicting &quot;thousand&quot; which turn out to be the most common token in the dataset would result in ~15% accuracy. Our simple transformer does much better then that. It feels promising, so let&#39;s try to improve the architecture and check if we can get better results. . Multihead attention . A structured sequence may comprise multiple distinctive kinds of relationships. Our model is forced to learn only one way in which queries, keys and values are constructed from the original token embedding. To remove this limitation we can modify attention layer include multiple heads which would correspond to extracting different kinds of relationships between tokens. The MultiHeadAttention layer consits of several heads each of those is similar to SelfAttention layer we made before. To keep computational cost of the multi-head layer we set $d_k = d_v = d_{model}/n_h$, where $n_h$ is number of heads. . class SelfAttention(Module): def __init__(self, d_in, d_qk, d_v=None): d_v = ifnone(d_v, d_qk) self.iq = nn.Linear(d_in, d_qk) self.ik = nn.Linear(d_in, d_qk) self.iv = nn.Linear(d_in, d_v) self.scale = d_qk**-0.5 def forward(self, x): q, k, v = self.iq(x), self.ik(x), self.iv(x) return F.softmax(q@k.transpose(-2,-1)*self.scale, -1)@v . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads, d_qk=None, d_v=None): d_qk = ifnone(d_qk, d_model//n_heads) d_v = ifnone(d_v, d_qk) self.heads = nn.ModuleList([SelfAttention(d_model, d_qk) for _ in range(n_heads)]) self.out = nn.Linear(d_v*n_heads, d_model) def forward(self, x): out = [m(x) for m in self.heads] return self.out(torch.cat(out, -1)) . inp = torch.randn(8, 10, 64) mha = MultiHeadAttention(64, 8) out = mha(inp) out.shape . torch.Size([8, 10, 64]) . class Model2(Module): def __init__(self, vocab_sz, d_model=64, n_heads=4, d_ff=64*4): self.emb = nn.Embedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) x = x.mean(1) return self.out(x) . learn = Learner(dls, Model2(len(vocab)), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 5e-4) . epoch train_loss valid_loss accuracy time . 0 | 2.177783 | 2.223564 | 0.332303 | 00:04 | . 1 | 1.629889 | 1.867587 | 0.445210 | 00:04 | . 2 | 1.607201 | 1.738342 | 0.464464 | 00:04 | . 3 | 1.606301 | 1.711135 | 0.467316 | 00:04 | . 4 | 1.592446 | 1.708671 | 0.467554 | 00:04 | . MultiHead Attention Refactor . Python for loops are slow, therefore it is better to refactor the MultiHeadAttention module to compute Q, K, V for all heads in batch. . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads): assert d_model%n_heads == 0 self.n_heads = n_heads #d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.out = nn.Linear(d_model, d_model, bias=False) self.scale = d_model//n_heads def forward(self, x): bs, seq_len, d = x.size() # (bs,sl,d) -&gt; (bs,sl,nh,dh) -&gt; (bs,nh,sl,dh) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q*= self.scale att = F.softmax(q@k.transpose(-2,-1), -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return self.out(out) . learn = Learner(dls, Model2(len(vocab)), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.945255 | 2.148961 | 0.362729 | 00:03 | . 1 | 1.576766 | 2.055920 | 0.426432 | 00:03 | . 2 | 1.599834 | 1.934431 | 0.443784 | 00:03 | . 3 | 1.624774 | 1.742481 | 0.460185 | 00:03 | . 4 | 1.615320 | 1.773680 | 0.452817 | 00:03 | . Note that some speedup is observed even on such a tiny dataset and small model. . More signal . Similarly to the RNN case considered in the book, we can take the next step and create more signal for the model to learn from. To adapt to the modified objective we need to make couple of steps. First let&#39;s rearrange data to proper input-target pairs for the new task. . Arranging data . Unlike RNN the tranformer is not a stateful model. This means it treats each sequence indepently and can only attend within fixed length context. This limitation was adressed by authors of Transformer-XL paper where adding a segment-level recurrence mechanism and a novel positional encoding scheme were proposed to enable capturing long-term dependencies. I will not go into details of TransformerXL architecture here. As will shell see stateless transformer can also learn a lot about the structure of our data. . One thing to note in this case is that we don&#39;t need to maintain the structure of the data outside of the sequences, so we can shuffle the sequences randomly in the dataloader. . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=bs, drop_last=True, shuffle=True) xb, yb = dls.one_batch() xb.shape, yb.shape . (torch.Size([64, 16]), torch.Size([64, 16])) . [L(vocab[o] for o in s) for s in seqs[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . Positional encoding . Before we did average pooling over seq_len dimention. Our model didn&#39;t care about the order of the tokens at all. But actually order of the tokens in a sentence matter a lot. In our case one hundred two and two hundred one are pretty different and hundred one two doesn&#39;t make sense. . To encorporate positional information into the model authors of the transformer architecture proposed to use positional encodings in addition to regular token embeddings. Positional encodings may be learned, but it&#39;s also possible to use hardcoded encodings. For instance encodings may be composed of sin and cos. In this way each position in a sequence will get unique vector associated with it. . class PositionalEncoding(Module): def __init__(self, d): self.register_buffer(&#39;freq&#39;, 1/(10000 ** (torch.arange(0., d, 2.)/d))) self.scale = d**0.5 def forward(self, x): device = x.device pos_enc = torch.cat([torch.sin(torch.outer(torch.arange(x.size(1), device=device), self.freq)), torch.cos(torch.outer(torch.arange(x.size(1), device=device), self.freq))], axis=-1) return x*self.scale + pos_enc . x = torch.zeros(1, 16, 64) encs = PositionalEncoding(64)(x) plt.matshow(encs.squeeze()) plt.xlabel(&#39;Embedding size&#39;) plt.ylabel(&#39;Sequence length&#39;) plt.show() . . class TransformerEmbedding(Module): def __init__(self, emb_sz, d_model): self.emb = nn.Embedding(emb_sz, d_model) self.pos_enc = PositionalEncoding(d_model) def forward(self, x): return self.pos_enc(self.emb(x)) . class Model3(Module): def __init__(self, vocab_sz, d_model=64, n_heads=4, d_ff=64*4): self.emb = TransformerEmbedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.ff(self.attn(x)) return self.out(x) . model = Model3(len(vocab)) out = model(xb) out.shape . torch.Size([64, 16, 30]) . def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . learn = Learner(dls, Model3(len(vocab)), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.059098 | 1.966106 | 0.244059 | 00:01 | . 1 | 0.996265 | 0.151726 | 0.956055 | 00:01 | . 2 | 0.427699 | 0.078305 | 0.977865 | 00:01 | . 3 | 0.207902 | 0.066726 | 0.976481 | 00:01 | . 4 | 0.116920 | 0.074462 | 0.974935 | 00:01 | . Wow! Thats a great accuracy! So the problem is solved and we only needed one attention layer and 2 layer deep feed-forward block? Don&#39;t you feel somewhat skeptical about this result? . Well, you should be! Think about what we did here: the goal was to predict a target sequence, say [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;] from an input [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;]. These two sequences intersect on all positions except the first and the last one. So models needs to learn simply to copy input tokens starting from the second one to the outputs. In our case this will result in 15 correct predictions of total 16 positions, thats almost 94% accuracy. This makes the task very simple but not very useful to learn. To train proper autoregressive language model, as we did with RNNs, a concept of masking is to be introduced. . Causal Masking . So we want to allow the model for each token to attend only to itself and those prior to it. To acomplish this we can set all the values of attention matrix above the main diagonal to $- infty$. After softmax this values will effectively turn to 0 thus disabling attention to the &quot;future&quot;. . def get_subsequent_mask(x): sz = x.size(1) mask = (torch.triu(torch.ones(sz, sz, device=x.device)) == 1).transpose(0, 1) mask = mask.float().masked_fill(mask == 0, float(&#39;-inf&#39;)).masked_fill(mask == 1, float(0.0)) return mask . inp = torch.randn(8, 10, 64) mask = get_subsequent_mask(inp) plt.matshow(mask); . q, k = torch.rand(1,10,32), torch.randn(1,10,32) att_ = F.softmax((q@k.permute(0,2,1)+mask), -1) plt.matshow(att_[0].detach()); . We should also modify the attention layer to except mask: . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads): assert d_model%n_heads == 0 self.n_heads = n_heads d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.scale = d_qk**-0.5 self.out = nn.Linear(d_model, d_model, bias=False) def forward(self, x, mask=None): bs, seq_len, d = x.size() mask = ifnone(mask, 0) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q*= self.scale att = F.softmax(q@k.transpose(-2,-1) + mask, -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return self.out(out) . class Model4(Module): def __init__(self, vocab_sz, d_model=64, n_heads=8, d_ff=64*4): self.emb = TransformerEmbedding(vocab_sz, d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) mask = get_subsequent_mask(x) x = self.ff(self.attn(x, mask)) return self.out(x) . learn = Learner(dls, Model4(len(vocab)), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.399806 | 2.321602 | 0.262533 | 00:01 | . 1 | 1.804210 | 2.251197 | 0.251709 | 00:01 | . 2 | 1.559652 | 2.320621 | 0.282878 | 00:01 | . 3 | 1.426687 | 2.365385 | 0.281006 | 00:01 | . 4 | 1.355069 | 2.430914 | 0.301025 | 00:01 | . Now we get somewhat lower accuracy, which is expected given that the task has become more difficult. Also training loss is significantly lower than validation loss, which means the model is overfitting. Let&#39;s see if the same approaches as was taken to RNNs can help. . Multilayer transformer . To solve a more difficult task we ussualy need a deeper model. For convenience let&#39;s make a TransformerLayer which will combine self-attention and feed-forward blocks. . class TransformerLayer(Module): def __init__(self, d_model, n_heads=8, d_ff=None, causal=True): d_ff = ifnone(d_ff, 4*d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff) self.causal = causal def forward(self, x, mask=None): if self.causal: mask = get_subsequent_mask(x) return self.ff(self.attn(x, mask)) . class Model5(Module): def __init__(self, vocab_sz, d_model=64, n_layer=4, n_heads=8): self.emb = TransformerEmbedding(vocab_sz, d_model) self.encoder = nn.Sequential(*[TransformerLayer(d_model, n_heads) for _ in range(n_layer)]) self.out = nn.Linear(d_model, vocab_sz) def forward(self, x): x = self.emb(x) x = self.encoder(x) return self.out(x) . learn = Learner(dls, Model5(len(vocab), n_layer=4), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.896548 | 2.794600 | 0.151611 | 00:03 | . 1 | 2.790987 | 2.813897 | 0.151774 | 00:03 | . 2 | 2.769421 | 2.791088 | 0.151774 | 00:04 | . 3 | 2.756661 | 2.790368 | 0.151367 | 00:04 | . 4 | 2.748009 | 2.799597 | 0.150960 | 00:04 | . That&#39;s not good! 4 layer deep Transformer strugles to learn anything. But there are good news, this problem has been already resolved in the original transformer. . Residual connections and Regularization . If you are familiar with ResNets the proposed solution will not surprise you much. The idea is simple yet very effective. Instead of returning modified output $f(x)$ each transformer sublayer will return $x + f(x)$. Thishow to produce allows the original input to propagate freely through the model. So the model learns not an entirely new representation of $x$ but how to modify $x$ to add some useful information to the original representation. . As we modify layers to include the residual connections let&#39;s also add some regularization by inserting Dropout layers. . class TransformerEmbedding(Module): def __init__(self, emb_sz, d_model, p=0.1): self.emb = Embedding(emb_sz, d_model) nn.init.trunc_normal_(self.emb.weight, std=d_model**-0.5) self.pos_enc = PositionalEncoding(d_model) self.drop = nn.Dropout(p) def forward(self, x): return self.drop(self.pos_enc(self.emb(x))) . Another modification is to add layer normalization which is intended to improve learning dynamics of the network by reparametrising data statistics and is generally used in transformer based architectures. . class FeedForward(Module): def __init__(self, d_model, d_ff, p=0.2): self.lin1 = nn.Linear(d_model, d_ff) self.lin2 = nn.Linear(d_ff, d_model) self.act = nn.ReLU() self.norm = nn.LayerNorm(d_model) self.drop = nn.Dropout(p) def forward(self, x): x = self.norm(x) out = self.act(self.lin1(x)) out = self.lin2(out) return x + self.drop(out) . class MultiHeadAttention(Module): def __init__(self, d_model, n_heads, p=0.1): assert d_model%n_heads == 0 self.n_heads = n_heads d_qk, d_v = d_model//n_heads, d_model//n_heads self.iq = nn.Linear(d_model, d_model, bias=False) self.ik = nn.Linear(d_model, d_model, bias=False) self.iv = nn.Linear(d_model, d_model, bias=False) self.scale = d_qk**0.5 self.out = nn.Linear(d_model, d_model, bias=False) self.norm = nn.LayerNorm(d_model) self.drop = nn.Dropout(p) def forward(self, x, mask=None): bs, seq_len, d = x.size() mask = ifnone(mask, 0) x = self.norm(x) k = self.ik(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) q = self.iq(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) v = self.iv(x).view(bs, seq_len, self.n_heads, d//self.n_heads).transpose(1, 2) att = F.softmax(q@k.transpose(-2,-1)/self.scale + mask, -1) out = att @ v # (bs, nh, sl, sl) x (bs, nh, sl, dh) -&gt; (bs, nh, sl, dh) out = out.transpose(1, 2).contiguous().view(bs, seq_len, d) # back to original shape return x + self.drop(self.out(out)) . class TransformerLayer(Module): def __init__(self, d_model, n_heads=8, d_ff=None, causal=True, p_att=0.1, p_ff=0.1): d_ff = ifnone(d_ff, 4*d_model) self.attn = MultiHeadAttention(d_model, n_heads) self.ff = FeedForward(d_model, d_ff, p=p_ff) self.causal = causal self._init() def forward(self, x, mask=None): if self.causal: mask = get_subsequent_mask(x) return self.ff(self.attn(x, mask)) def _init(self): for p in self.parameters(): if p.dim()&gt;1: nn.init.xavier_uniform_(p) . class Model6(Module): def __init__(self, vocab_sz, d_model=64, n_layer=4, n_heads=8, p_emb=0.1, p_att=0.1, p_ff=0.2, tie_weights=True): self.emb = TransformerEmbedding(vocab_sz, d_model, p=p_emb) self.encoder = nn.Sequential(*[TransformerLayer(d_model, n_heads, p_att=p_att, p_ff=p_ff) for _ in range(n_layer)], nn.LayerNorm(d_model)) self.out = nn.Linear(d_model, vocab_sz) if tie_weights: self.out.weight = self.emb.emb.weight def forward(self, x): x = self.emb(x) x = self.encoder(x) return self.out(x) . learn = Learner(dls, Model6(len(vocab), n_layer=2), loss_func=loss_func, metrics=accuracy) learn.fit_one_cycle(8, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 2.635322 | 2.077494 | 0.193929 | 00:02 | . 1 | 1.751456 | 1.042653 | 0.667806 | 00:02 | . 2 | 1.120504 | 0.743249 | 0.767822 | 00:02 | . 3 | 0.825433 | 0.797066 | 0.733643 | 00:02 | . 4 | 0.687212 | 0.747418 | 0.751058 | 00:02 | . 5 | 0.615355 | 0.815827 | 0.747233 | 00:02 | . 6 | 0.576437 | 0.809159 | 0.751302 | 00:02 | . 7 | 0.557015 | 0.823612 | 0.744466 | 00:02 | . Bonus - Generation example . Learning to predict numbers is great, but let&#39;s try something more fun. We can train a langyage model generate some texts. For example let&#39;s try to generate some text in style of Lewis Carroll. For this we&#39;ll fit a language model on &quot;Alice in Wonderland&quot; and &quot;Through the looking glass&quot;. . def parse_txt(fns): txts = [] for fn in fns: with open(fn) as f: tmp = &#39;&#39; for line in f.readlines(): line = line.strip(&#39; n&#39;) if line: tmp += &#39; &#39; + line elif tmp: txts.append(tmp.strip()) tmp = &#39;&#39; return txts . . texts = parse_txt([path/&#39;11-0.txt&#39;, path/&#39;12-0.txt&#39;]) . len(texts) . 1779 . texts[0:2] . [&#39; ufeffCHAPTER I. Down the Rabbit-Hole&#39;, &#39;Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, “and what is the use of a book,” thought Alice “without pictures or conversations?”&#39;] . class CharTokenizer(Transform): &quot;Simple charecter level tokenizer&quot; def __init__(self, vocab=None): self.vocab = ifnone(vocab, [&#39;&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;] + list(string.printable)) self.c2i = defaultdict(int, [(c,i) for i, c in enumerate(self.vocab)]) def encodes(self, s, add_bos=False, add_eos=False): strt = [self.c2i[&#39;xxbos&#39;]] if add_bos else [] end = [self.c2i[&#39;xxeos&#39;]] if add_eos else [] return LMTensorText(strt + [self.c2i[c] for c in s] + end) def decodes(self, s, remove_special=False): return TitledStr(&#39;&#39;.join([self.decode_one(i) for i in s])) def decode_one(self, i): if i == 2: return &#39; n&#39; elif i == 1: return &#39;&#39; else: return self.vocab[i] @property def vocab_sz(self): return len(self.vocab) . . tok = CharTokenizer() . def add_bos_eos(x:list, bos_id=1, eos_id=2): return [bos_id] + x + [eos_id] . nums = [add_bos_eos(tok(t.lower()).tolist()) for t in texts] . len(nums) . 1779 . all_nums = [] for n in nums: all_nums.extend(n) . all_nums[:15] . [1, 0, 15, 20, 13, 28, 32, 17, 30, 97, 21, 78, 97, 16, 27] . print(tok.decode(all_nums[:100])) . chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on . sl = 512 seqs = L((tensor(all_nums[i:i+sl]), tensor(all_nums[i+1:i+sl+1])) for i in range(0,len(all_nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], device=&#39;cuda&#39;, bs=8, drop_last=True, shuffle=True) xb, yb = dls.one_batch() xb.shape, yb.shape . (torch.Size([8, 512]), torch.Size([8, 512])) . model = Model6(tok.vocab_sz, 512, 6, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() learn.lr_find() . SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.6309573650360107) . learn.fit_one_cycle(50, 5e-4, cbs=EarlyStoppingCallback(patience=5)) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.207689 | 3.014779 | 0.187012 | 20.384583 | 00:06 | . 1 | 2.957462 | 2.648416 | 0.258105 | 14.131640 | 00:06 | . 2 | 2.645952 | 2.427977 | 0.287435 | 11.335924 | 00:07 | . 3 | 2.499318 | 2.395460 | 0.292887 | 10.973244 | 00:07 | . 4 | 2.436253 | 2.394616 | 0.288965 | 10.963985 | 00:07 | . 5 | 2.402833 | 2.364455 | 0.295703 | 10.638234 | 00:07 | . 6 | 2.387243 | 2.367871 | 0.290381 | 10.674637 | 00:07 | . 7 | 2.375951 | 2.384834 | 0.285010 | 10.857258 | 00:07 | . 8 | 2.372440 | 2.380138 | 0.276270 | 10.806398 | 00:07 | . 9 | 2.355436 | 2.329239 | 0.305892 | 10.270120 | 00:07 | . 10 | 2.314718 | 2.250078 | 0.331901 | 9.488480 | 00:07 | . 11 | 2.231658 | 2.096768 | 0.385319 | 8.139816 | 00:07 | . 12 | 2.145322 | 1.989301 | 0.413477 | 7.310425 | 00:07 | . 13 | 1.998541 | 1.862125 | 0.444971 | 6.437402 | 00:07 | . 14 | 1.880289 | 1.772380 | 0.465283 | 5.884840 | 00:07 | . 15 | 1.777548 | 1.735309 | 0.482080 | 5.670681 | 00:07 | . 16 | 1.692429 | 1.649408 | 0.501937 | 5.203897 | 00:07 | . 17 | 1.616076 | 1.616357 | 0.513688 | 5.034715 | 00:07 | . 18 | 1.548247 | 1.586346 | 0.522575 | 4.885864 | 00:07 | . 19 | 1.484927 | 1.550523 | 0.532633 | 4.713936 | 00:07 | . 20 | 1.430124 | 1.512773 | 0.543213 | 4.539303 | 00:07 | . 21 | 1.375972 | 1.500666 | 0.545199 | 4.484674 | 00:07 | . 22 | 1.324876 | 1.491262 | 0.552637 | 4.442699 | 00:07 | . 23 | 1.276637 | 1.469852 | 0.557975 | 4.348590 | 00:07 | . 24 | 1.229700 | 1.477631 | 0.560189 | 4.382551 | 00:07 | . 25 | 1.186633 | 1.459370 | 0.562956 | 4.303250 | 00:07 | . 26 | 1.139820 | 1.467342 | 0.564225 | 4.337692 | 00:07 | . 27 | 1.092420 | 1.476885 | 0.566960 | 4.379283 | 00:07 | . 28 | 1.050866 | 1.486061 | 0.567350 | 4.419652 | 00:07 | . 29 | 1.006091 | 1.505293 | 0.568506 | 4.505473 | 00:07 | . 30 | 0.957875 | 1.528569 | 0.567106 | 4.611572 | 00:07 | . No improvement since epoch 25: early stopping . Text generation . def expand_dim1(x): if len(x.shape) == 1: return x[None, :] else: return x def top_p_filter(logits, top_p=0.9): sorted_logits, sorted_indices = torch.sort(logits, descending=True) cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) sorted_indices_to_remove = cum_probs &gt; top_p sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove) logits[indices_to_remove] = float(&#39;-inf&#39;) return logits @torch.no_grad() def generate(model, inp, max_len=50, temperature=1., top_k = 20, top_p = 0.9, early_stopping=False, #need eos_idx to work eos_idx=None): model.to(inp.device) #TODO test for potential problems model.eval() thresh = top_p inp = expand_dim1(inp) b, t = inp.shape out = inp for _ in range(max_len): x = out logits = model(x)[:, -1, :] filtered_logits = top_p_filter(logits) probs = F.softmax(filtered_logits / temperature, dim=-1) sample = torch.multinomial(probs, 1) out = torch.cat((out, sample), dim=-1) if early_stopping and (sample == eos_idx).all(): break return out . . out = generate(learn.model, tok(&#39;Alice said &#39;), max_len=200, early_stopping=True, eos_idx=tok.c2i[&#39;xxeos&#39;]) . print(tok.decode(out[0])) . Alice said in a minute turn, only purind his with it migut at in musible. i cant elp out my why yested it to like thought: i know i did it wish indeed it hope? . Pretraining on larger dataset . dataset = load_dataset(&quot;bookcorpus&quot;, split=&#39;train&#39;) . Downloading and preparing dataset bookcorpus/plain_text (download: 1.10 GiB, generated: 4.52 GiB, post-processed: Unknown size, total: 5.62 GiB) to /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/af844be26c089fb64810e9f2cd841954fd8bd596d6ddd26326e4c70e2b8c96fc... Dataset bookcorpus downloaded and prepared to /root/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/af844be26c089fb64810e9f2cd841954fd8bd596d6ddd26326e4c70e2b8c96fc. Subsequent calls will reuse this data. . df = pd.DataFrame(dataset[:10_000_000]) df.head() . text . 0 the half-ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved . | . 1 isbn : 1492913731 isbn-13 : 978-1492913733 for my family , who encouraged me to never stop fighting for my dreams chapter 1 summer vacations supposed to be fun , right ? | . 2 i wish i had a better answer to that question . | . 3 starlings , new york is not the place youd expect much to happen . | . 4 its a small quiet town , the kind where everyone knows your name . | . df[&#39;len&#39;] = df[&#39;text&#39;].str.len() . cut = int(len(df)*0.8) splits = range_of(df)[:cut], range_of(df[cut:]) tfms = Pipeline([ColReader(&#39;text&#39;), tok]) dsets = Datasets(df, tfms=tfms, dl_type=LMDataLoader, splits=splits) . @patch def create_item(self:LMDataLoader, seq): if seq&gt;=self.n: raise IndexError sl = self.last_len if seq//self.bs==self.n_batches-1 else self.seq_len st = (seq%self.bs)*self.bl + (seq//self.bs)*self.seq_len txt = self.chunks[st : st+sl+1] return LMTensorText(txt[:-1]),txt[1:] . %%time dl_kwargs = [{&#39;lens&#39;:df[&#39;len&#39;].values[splits[0]]}, {&#39;val_lens&#39;:df[&#39;len&#39;].values[splits[1]]}] dls = dsets.dataloaders(bs=32, seq_len=512, dl_kwargs=dl_kwargs, shuffle_train=True, num_workers=2) . CPU times: user 13min 29s, sys: 13 s, total: 13min 42s Wall time: 13min 35s . dls.show_batch(max_n=2) . text text_ . 0 im sorry .`` ah , this is katrice , the rowan queen , coming toward us . &#39;&#39;i said ill tell you !but she still did n&#39;t understand why he was a slave here .the sprites had come to believe by now that they were a forgotten people .thats what happened .well have to take him with us for a ways and then let him go .nick stifled the fear in him that was trying to take over .crouching down behind the stone balusters , with every nerve tingling , valeria glared down at the stealthy figure .there did seem to be shado | m sorry .`` ah , this is katrice , the rowan queen , coming toward us . &#39;&#39;i said ill tell you !but she still did n&#39;t understand why he was a slave here .the sprites had come to believe by now that they were a forgotten people .thats what happened .well have to take him with us for a ways and then let him go .nick stifled the fear in him that was trying to take over .crouching down behind the stone balusters , with every nerve tingling , valeria glared down at the stealthy figure .there did seem to be shadow | . 1 habitants were genuine , hard working , proud and tough .sergeant colon &#39;s view of the world was certainly changing .&#39;we did n&#39;t want any part of it , &#39; the grandfather continued , heedless of his company , &#39;but maybe that &#39;s just how the rhega are destined to die ... not by our own hands , our own fights .she looked enquiring , but no one took any notice of her .more land would n&#39;t save his people-they needed something else .the other got off a single shot of his .45 caliber pistol before he was clawed acr | abitants were genuine , hard working , proud and tough .sergeant colon &#39;s view of the world was certainly changing .&#39;we did n&#39;t want any part of it , &#39; the grandfather continued , heedless of his company , &#39;but maybe that &#39;s just how the rhega are destined to die ... not by our own hands , our own fights .she looked enquiring , but no one took any notice of her .more land would n&#39;t save his people-they needed something else .the other got off a single shot of his .45 caliber pistol before he was clawed acro | . model = Model6(tok.vocab_sz, 512, 8, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() learn.lr_find() . SuggestedLRs(lr_min=0.06309573650360108, lr_steep=0.5248074531555176) . learn = learn.load(path/&#39;char_bookcorpus_10m&#39;) . &lt;fastai.learner.Learner at 0x7fa5a807e0f0&gt; . learn.fit_one_cycle(1, 1e-4) . epoch train_loss valid_loss accuracy perplexity time . 0 | 1.065701 | 1.064358 | 0.667121 | 2.898976 | 4:23:52 | . learn.save(path/&#39;char_bookcorpus_10m&#39;) . Path(&#39;/content/drive/MyDrive/char_model/char_bookcorpus_10m.pth&#39;) . Finetune on Carrolls&#39; books . sl = 512 seqs = L((tensor(all_nums[i:i+sl]), tensor(all_nums[i+1:i+sl+1])) for i in range(0,len(all_nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], device=&#39;cuda&#39;, bs=16, drop_last=True, shuffle=True) . model = Model6(tok.vocab_sz, 512, 8, p_emb=0.1, p_ff=0.1, tie_weights=True) learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=[accuracy, perplexity]).to_native_fp16() . learn = learn.load(path/&#39;char_bookcorpus_10m&#39;) learn.lr_find() . SuggestedLRs(lr_min=0.03019951581954956, lr_steep=0.25118863582611084) . learn.fit_one_cycle(10, 1e-4) . epoch train_loss valid_loss accuracy perplexity time . 0 | 1.744969 | 1.472124 | 0.618443 | 4.358481 | 00:08 | . 1 | 1.442924 | 1.128739 | 0.658796 | 3.091756 | 00:08 | . 2 | 1.248169 | 1.068535 | 0.670881 | 2.911111 | 00:08 | . 3 | 1.134621 | 1.048366 | 0.675764 | 2.852986 | 00:08 | . 4 | 1.058302 | 1.044972 | 0.678554 | 2.843319 | 00:08 | . 5 | 1.004934 | 1.036241 | 0.680333 | 2.818603 | 00:08 | . 6 | 0.966509 | 1.042076 | 0.679461 | 2.835096 | 00:08 | . 7 | 0.938987 | 1.040590 | 0.680507 | 2.830886 | 00:08 | . 8 | 0.920266 | 1.035050 | 0.681728 | 2.815248 | 00:08 | . 9 | 0.907935 | 1.037062 | 0.681257 | 2.820917 | 00:08 | . As you see pretraining model on large corpus followed by finetuning helped to reduce validation loss from arount 1.53 to 1.037 and improve accuracy in predicting next character to 68% (compared to 56.7% before). Let&#39;s see how it effects sampled text: . out = generate(learn.model, tok(&#39;Alice said &#39;), max_len=200, early_stopping=True, eos_idx=tok.c2i[&#39;xxeos&#39;]) . print(tok.decode(out[0])) . . Alice said what you want is, why, if you cant see a little rather from people to bed their moment, when birds began drinking from behind, and offering the cart to say something, and dripping off a strange mou .",
            "url": "https://arampacha.github.io/thoughtsamples/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html",
            "relUrl": "/fastai/pytorch/2021/01/02/transformer-lm-from-scratch.html",
            "date": " • Jan 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://arampacha.github.io/thoughtsamples/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://arampacha.github.io/thoughtsamples/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}